<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hive笔记</title>
    <url>/2019/04/19/hive%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<!-- <img src="49.jpg"> -->
<a id="more"></a>
<p>架构在Hadoop之上，提供简单的sql查询功能，可以<strong><em>将sql语句转换为MapReduce任务进行运行(增删改查)</em></strong>。<br>所有的增删改查操作都是应用在hdfs上的。Hive 中所有的数据都存储在 HDFS 中，Hive 中包含以下数据模型：Table，External Table，Partition，Bucket。<br>hive是一个数据仓库工具，作用是可以将结构化的数据文件映射为一张数据库表，并提供简单查询功能，可以将sql语句转化为Mapreduce任务进行，是在Hadoop上的数据库基础架构。<br><strong>Hive 不是一个关系数据库/实时查询和行级更新的语言.</strong></p>
<p>Hadoop是一个开源框架来存储和处理大型数据在分布式环境中。它包含两个模块，一个是MapReduce，另外一个是Hadoop分布式文件系统（HDFS）:</p>
<ul>
<li><strong>MapReduce</strong>：它是一种并行编程模型在大型集群普通硬件可用于处理大型结构化，半结构化和非结构化数据。</li>
<li><strong>HDFS</strong>：Hadoop分布式文件系统是Hadoop的框架的一部分，用于存储和处理数据集。它提供了一个容错文件系统在普通硬件上运行。</li>
</ul>
<p>hive的安装需要安装MySQL,因为hive 默认的数据库是Derby数据库，其与MySQL数据库比较存在缺陷，比如不可以执行两个并发的Hive CLI。<br><strong><em>Hive 将元数据存储在数据库中，如 mysql、derby。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。</em></strong></p>
<p>Hive只是一个客户端，在安装时，我们可以在Hadoop集群中，选择一台安装Hive。Hive没有集群的概念，但是可以搭建Server/Client端。<br>MapReduce任务(job)的启动需要消耗较长时间，所以Hive的查询延时比较严重。在传统数据库中秒级的任务，在Hive仍需要更长时间。Hive适用不需要实时响应查询的数据仓库程序，不需要记录级别的增删改<br>Hive不支持事务。提交查询和返回结果可能有很大的延时，此时选用NoSQL数据库，Hbase等。</p>
<h3 id="Hive框架的作用"><a href="#Hive框架的作用" class="headerlink" title="Hive框架的作用"></a>Hive框架的作用</h3><p>（1）可以让不懂java的数据分析人员使用hadoop进行数据分析；<br>（2）MapReduce开发非常繁琐复杂，使用hive可以提高效率。<br>（3）统一的元数据管理，可与impala/spark共享元数据。</p>
<p>hive模型图<br>driver：hive查询的sql都会先提交到driver这里。而driver又由compiler、optimizer、Executor组成。compiler将类sql查询语句进行解析、并且从元数据库取元数据解析优化，成mr job，提交到hadoop集群执行。driver里面有个优化器optimizer。<br>它的作用是：</p>
<ul>
<li>1、去掉不必要的列和分区，优化查询。</li>
<li>2、将多 multiple join 合并为一个 multi-way join；</li>
<li>3、对join、group-by 和自定义的 map-reduce 操作重新进行划分；<br><img src="48.png" alt=""></li>
</ul>
<h3 id="hive在hdfs上的文件结构"><a href="#hive在hdfs上的文件结构" class="headerlink" title="hive在hdfs上的文件结构"></a>hive在hdfs上的文件结构</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">　　    数据仓库的位置                数据库目录           表目录          表的数据文件</span><br><span class="line">　　&#x2F;user&#x2F;hive&#x2F;warehouse             &#x2F;test.db             &#x2F;row_table       &#x2F;hive_test.txt</span><br></pre></td></tr></table></figure>
<p>Hive的数据都是存储在HDFS上的，默认有一个根目录，在hive-site.xml中，由参数hive.metastore.warehouse.dir指定。<br>default是默认的数据库：指的就是这个/user/hive/warehouse路径，因此表就直接在这个目录下<br>参考：<a href="https://www.cnblogs.com/xningge/p/8439970.html" target="_blank" rel="noopener">https://www.cnblogs.com/xningge/p/8439970.html</a></p>
<h3 id="queuename"><a href="#queuename" class="headerlink" title="queuename"></a>queuename</h3><p>hadoop相关,作业提交到的队列，默认是<code>default</code>。通过<code>set mapreduce.job.queuename</code>可以查看当前定义队列名。<br>队列是跟用户对应的，哪个用户要执行，需要指定哪个队列。<br><code>mapred.job.queue.name</code> 一样，这个是老版本v1的，上面是新版本v2的。<br>拥有不同优先级的各种队列只是让Hadoop可以轻松决定处理器可用时下一步该做什么，或者它可以使用多少。</p>
<p>更多参考：<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-configurations-mapreduce/" target="_blank" rel="noopener">http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-configurations-mapreduce/</a></p>
<h3 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h3><p>Hive中表和分区的所有元数据都存储在Hive的元存储（Metastore）中。<br>元数据使用JPOX（Java Persistent Objects）对象关系映射解决方案进行持久化，所以任何被JPOX支持的存储都可以被Hive使用。<br>大多数商业关系型数据库和许多开源的数据存储都被支持，所以就可以被Hive使用存储元数据。Hive支持三种不同的元存储服务器，分别为：内嵌式元存储、本地元存储、远程元存储，每种存储方式使用不同的配置参数，</p>
<p>内嵌式元存储：主要用于单元测试，在该模式下每次只有一个进程可以连接到元存储，Derby是内嵌式元存储的默认数据库。<br>本地模式：每个Hive客户端都会打开到数据存储的连接并在该连接上请求SQL查询。<br>远程模式：所有的Hive客户端都将打开一个到元数据服务器的连接，该服务器依次查询元数据。</p>
<p>参考：<a href="https://blog.csdn.net/skywalker_only/article/details/26219619（三种元数据存储方式）" target="_blank" rel="noopener">https://blog.csdn.net/skywalker_only/article/details/26219619（三种元数据存储方式）</a><br><a href="http://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/cdh5ig_hive_metastore_configure.html" target="_blank" rel="noopener">http://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/cdh5ig_hive_metastore_configure.html</a></p>
<h3 id="引擎"><a href="#引擎" class="headerlink" title="引擎"></a>引擎</h3><p>hive执行引擎 mr/tez/spark</p>
<h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>hive引入partition和bucket的概念，中文翻译分别为分区和桶，这两个概念都是把数据划分成块，分区是粗粒度的划分，桶是细粒度的划分，这样做为了可以让查询发生在小范围的数据上以提高效率。<br>在Hive Select查询中一般会扫描整个表内容，会消耗很多时间做没必要的工作。有时候只需要扫描表中关心的一部分数据，因此建表时引入了partition概念。<br>分区表指的是在创建表时指定的partition的分区空间。如果需要创建有分区的表，需要在create表的时候调用可选参数partitioned by。<br>一个表可以拥有一个或者多个分区，每个分区以文件夹的形式单独存在表文件夹的目录下。<br>分区是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在，但是该字段不存放实际的数据内容，仅仅是分区的表示。</p>
<h3 id="创建单分区-多分区"><a href="#创建单分区-多分区" class="headerlink" title="创建单分区/多分区"></a>创建单分区/多分区</h3><p>关于分区维度的选择，我们应该尽量选取那些<strong><em>有限且少量的数值集</em></strong>作为分区，例如国家、省份就是一个良好的分区，而城市就可能不适合进行分区。<br>分区建表分为2种，一种是单分区，也就是说在表文件夹目录下只有一级文件夹目录。另外一种是多分区，表文件夹下出现多文件夹嵌套模式。<br>a、单分区建表语句：<code>create table day_table (id int, content string) partitioned by (dt string);</code>单分区表，按天分区，在表结构中存在id，content，dt三列。<br>b、双分区建表语句：<code>create table day_hour_table (id int, content string) partitioned by (dt string, hour string);</code>双分区表，按天和小时分区，在表结构中新增加了dt和hour两列。多个分区意味着多级目录。<br>分区是数据表中的一个列名，但是这个列并不占有表的实际存储空间。它作为一个虚拟列而存在。</p>
<h3 id="查看-增加-删除分区"><a href="#查看-增加-删除分区" class="headerlink" title="查看/增加/删除分区"></a>查看/增加/删除分区</h3><p><code>show partitions table_name [partition(...)]</code> 查看表所有的分区；加上可选<code>[partition(...)]</code>可以查看指定分区是否存在。<br><code>alter table xxx add [if not exist] partition (dt=&#39;2018-05-22&#39;)</code>  对分区名是dt的表增加分区<br><code>alter table table_name drop partition (dt=&#39;2018-05-22&#39;)</code> 删除分区<br><code>alter table table_name partition(dt=&#39;...&#39;) set localtion &#39;...&#39;</code> 修改分区地址（不会修改/删除旧的分区数据）<br><code>alter table table_name drop [if exist] partition (dt=&#39;...&#39;)</code> 删除分区。如果是内部表，还会删除数据。<br><strong><em>当外部表是分区表时，只有建立对应的分区，才能查到数据. 删除内部表的分区会删除相应的数据。</em></strong></p>
<h3 id="Buckets-桶"><a href="#Buckets-桶" class="headerlink" title="Buckets 桶"></a>Buckets 桶</h3><p>假设我们有一张地域姓名表并按城市分区。那么很有可能，北京分区的人数会远远大于其他分区，该分区的数据I/O吞吐效率将成为查询的瓶颈。如果我们对表中的姓名做分桶，将姓名按哈希值分发到桶中，每个桶将分配到大致均匀的人数。<br>分桶解决的是数据倾斜的问题。</p>
<p>Hive采用<strong><em>对列值哈希，然后除以桶的个数求余</em></strong>的方式决定该条记录存放在哪个桶当中。<br>对指定列计算 hash，根据 hash 值切分数据，目的是为了并行，每一个 Bucket 对应一个文件。将 user 列分散至 32 个 bucket，首先对 user 列的值计算 hash，对应 hash 值为 0 的 HDFS 目录为：/warehouse/app/dt=20100801/ctry=US/part-00000；hash 值为 20 的 HDFS 目录为：/warehouse/app/dt=20100801/ctry=US/part-00020</p>
<h4 id="创建桶表"><a href="#创建桶表" class="headerlink" title="创建桶表"></a>创建桶表</h4><p><code>create table table_name() clustered by(col_0) into bucket_num buckets;</code><br>创建表，按照col_0分桶，有bucket_num个桶。<br><code>set hive.enforce.bucketing = true;</code> 强制桶的个数和表定义相同，否则实际桶的个数和reducer一样。</p>
<h4 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h4><p>由文件导入数据时，需要一种中间表，详细看下面‘文件导入数据’小节。</p>
<h4 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h4><p><code>select * from table_name tablesample(bucket x out of y on col_0);</code><br>x:从第x桶开始抽取数据。<br>y:是总桶数的因数或倍数。 从x开始，分隔y个桶取数。<br>col_0:分桶的列。<br>eg：共有4个桶，y=2，x=2，会取第2、4个桶的数据；按照col_0分桶，会取col_0列的哈希值除以4余数是1、3的列。</p>
<p>总结自：<a href="https://www.cnblogs.com/MrFee/p/hive_bucket.html" target="_blank" rel="noopener">https://www.cnblogs.com/MrFee/p/hive_bucket.html</a><br><a href="https://blog.csdn.net/m0_37534613/article/details/55258928" target="_blank" rel="noopener">https://blog.csdn.net/m0_37534613/article/details/55258928</a></p>
<h3 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h3><p><img src="1.jpeg" alt=""></p>
<ul>
<li>PARTITIONED 表示的是分区，不同的分区会以文件夹的形式存在，在查询的时候指定分区查询将会大大加快查询的时间。</li>
<li>CLUSTERED表示的是按照某列聚类，例如在插入数据中有两项“张三，数学”和“张三，英语”，若是CLUSTERED BY name，则只会有一项，“张三，(数学，英语)”，这个机制也是为了加快查询的操作。</li>
<li>STORED是指定排序的形式，是降序还是升序。</li>
<li>BUCKETS是指定了分桶的信息，这在后面会单独列出来，在这里还不会涉及到。</li>
<li>ROW FORMAT是指定了行格式字段，如行、列的分隔符，<code>ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; LINES TERMINATED BY &#39;\n&#39;</code></li>
<li>STORED AS是指定文件的存储格式。Hive中基本提供两种文件格式：SEQUENCEFILE和TEXTFILE，序列文件是一种压缩的格式，通常可以提供更高的性能，默认是TEXTFILE。</li>
<li>LOCATION指的是在HDFS上存储的位置。</li>
</ul>
<p><code>create [external] table table_name1 like table_name2 [location hdfs_path]</code> 创建一个和表2结构一样的表</p>
<h3 id="内部-表"><a href="#内部-表" class="headerlink" title="(内部)表"></a>(内部)表</h3><p>表其实就是hdfs目录<br>Hive中的表和关系型数据库中的表在概念上很类似，每个表在HDFS中都有相应的目录用来存储表的数据，这个目录可以通过${HIVE_HOME}/conf/hive-site.xml配置文件中的hive.metastore.warehouse.dir属性来配置，这个属性默认的值是/user/hive/warehouse（这个目录在HDFS上），我们可以根据实际的情况来修改这个配置。<br>如果我有一个表wyp在cl库中，那么在HDFS中会创建/user/hive/warehouse/cl.db/wyp目录（这里假定hive.metastore.warehouse.dir配置为/user/hive/warehouse）；wyp表所有的数据都存放在这个目录中。这个例外是外部表。</p>
<p>参考：<a href="https://www.jianshu.com/p/dd97e0b2d2cf" target="_blank" rel="noopener">https://www.jianshu.com/p/dd97e0b2d2cf</a></p>
<h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><p>数据源不在我们这里，由别人提供，或者其他工具提供。<br>指向已经在 HDFS 中存在的数据，可以创建 Partition。它和 Table 在元数据的组织上是相同的，而实际数据的存储则有较大的差异。</p>
<h3 id="内部表-外部表-区别"><a href="#内部表-外部表-区别" class="headerlink" title="(内部表)/外部表 区别"></a>(内部表)/外部表 区别</h3><p>外部表在建表时多了‘EXTERNAL’： CREATE EXTERNAL TABLE<br>（1）、在导入数据到外部表，数据并没有移动到自己的数据仓库目录下，也就是说外部表中的数据并不是由它自己来管理的！而表则不一样；<br>（2）、在删除表的时候，Hive将会把属于表的元数据和数据全部删掉；而删除外部表的时候，Hive仅仅删除外部表的元数据，数据是不会删除的！<br>（3）、表有创建过程和数据加载过程（这两个过程可以在同一个语句中完成），在加载数据的过程中，实际数据会被移动到数据仓库目录中；之后对数据对访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除。<br>外部表只有一个过程，加载数据和创建表同时完成（CREATE EXTERNAL TABLE ……LOCATION），实际数据是存储在 LOCATION 后面指定的 HDFS 路径中，并不会移动到数据仓库目录中。</p>
<p>外部表适用场景：源表，需要定期将外部数据映射到表中。<br>使用场景例子：<br>每天将收集到的网站日志定期流入HDFS文本文件，一天一个目录；<br>在Hive中建立外部表作为源表，通过添加分区的方式，将每天HDFS上的原始日志映射到外部表的天分区中；<br>在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p>
<h3 id="关于Strict-Mode"><a href="#关于Strict-Mode" class="headerlink" title="关于Strict Mode"></a>关于Strict Mode</h3><p> Hive中的严格模式可以防止用户发出（可以有问题）的查询无意中造成不良的影响。 将hive.mapred.mode设置成strict可以禁止三种类型的查询：<br> 1）、在一个分区表上，如果没有在WHERE条件中指明具体的分区，那么这是不允许的，换句话说，不允许在分区表上全表扫描。这种限制的原因是分区表通常会持非常大的数据集并且可能数据增长迅速，对这样的一个大表做全表扫描会消耗大量资源，必须要再WHERE过滤条件中具体指明分区才可以执行成功的查询。<br> 2）、第二种是禁止执行有ORDER BY的排序要求但没有LIMIT语句的HiveQL查询。因为ORDER BY全局查询会导致有一个单一的reducer对所有的查询结果排序，如果对大数据集做排序，这将导致不可预期的执行时间，必须要加上limit条件才可以执行成功的查询。<br> 3）、第三种是禁止产生笛卡尔集(full Cartesian product)。在JION接连查询中没有ON连接key而通过WHERE条件语句会产生笛卡尔集，需要改为JOIN…ON语句。  </p>
<h3 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h3><p>所有通过<code>alter</code>，修改的只是表的元数据，表里存的数据并不会改变。</p>
<h4 id="改变location"><a href="#改变location" class="headerlink" title="改变location"></a>改变location</h4><p>通过修改表DDL：<code>alter table t_m_cc set location &#39;hdfs://heracles/user/video-mvc/hive/warehouse/t_m_cc&#39;</code><br>直接修改hive 的meta info: <code>update DBS set DB_LOCATION_URI = replace(DB_LOCATION_URI,&quot;oldpath&quot;,&quot;newpath&quot;)</code><br>                        <code>update SDS  set location =replace(location,&#39;oldpath,&#39;newpath&#39;)</code></p>
<h4 id="修改表名"><a href="#修改表名" class="headerlink" title="修改表名"></a>修改表名</h4><p><code>alter table old_name rename to new_name</code> 改表名</p>
<h4 id="修改列"><a href="#修改列" class="headerlink" title="修改列"></a>修改列</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter table table_name change column old_field_name new_field_name field_type</span><br><span class="line">comment &#39;...&#39;</span><br><span class="line">after field_name;</span><br></pre></td></tr></table></figure>
<p>修改列名、注释的位置；如果要挪到第一个位置，只需要用<code>first</code>代替<code>after field_name</code>。<br><strong><em>注意前面提到的<code>alter</code>语句只修改表元数据，数据没有任何变化，所以列的位置变化时，数据不会跟着变化</em></strong>。<br>但其实不推荐增加列，因为会有很多的问题。<br><strong><em>增加新的列后，可能发现新增加的列插入的数据都是null。</em></strong>不管是用<code>insert into</code>还是<code>insert overwrite</code>。<br>解决办法（1）：删除对应的分区<code>alter table table_name drop partition(dt=...)</code>，然后可以准确插入数据。<br>解决办法（2）：修改元数据，但一般没有权限，<a href="https://blog.csdn.net/lxpbs8851/article/details/17118841" target="_blank" rel="noopener">参考</a>，没有验证过。</p>
<p><code>alter table table_name add columns(..., ...)</code>添加新的（多个）字段。<br><code>alter table table replace columns (..., ..., ...);</code>删除/替换列</p>
<h4 id="修改表属性"><a href="#修改表属性" class="headerlink" title="修改表属性"></a>修改表属性</h4><p><code>alter table table_name set tblproperties(...)</code> 可以增加新的表属性，或者修改已经存在的属性，但是无法删除属性</p>
<h3 id="MSCK修复分区"><a href="#MSCK修复分区" class="headerlink" title="MSCK修复分区"></a>MSCK修复分区</h3><p><code>MSCK REPAIR TABLE table_name;</code><br>Hive会检测如果HDFS目录下存在但表的metastore中不存在的partition元信息，更新到metastore中。<br>代替手动通过<code>alter table add partition</code>方式增加Hive分区的方式。</p>
<h3 id="查看命令"><a href="#查看命令" class="headerlink" title="查看命令"></a>查看命令</h3><p><code>describe [extended/formatted] table_name</code> 查看表信息，类似<code>desc</code>  可选的<code>[extended]</code>可以看到更详细的信息，<code>formatted</code>看更多信息，可读性强<br><code>describe database [extended/formatted] database1</code> 查看库信息，可以看到库地址<br><code>drop database database1 cascade/restrict</code>  库不为空时，一般不允许直接删除，<code>cascade</code>保证可以删除，默认是restrict<br><code>show tables in data_base</code> 在别的库里查询库<code>data_base</code>所有的表</p>
<h3 id="读时模式"><a href="#读时模式" class="headerlink" title="读时模式"></a>读时模式</h3><p>hive是“读时模式”，对于存储文件的完整性、数据的格式是否和表匹配性等方面都没有支配能力。<br>只有在读数据时才会尽量的把hdfs的文件和表字段进行匹配。<br>我遇到的一个典型例子：hdfs文件里数据是3.5，hive表对应字段类型是<code>decimal</code>，这样导致读出来的数是4.（decimal没有指定小数精度时，默认是没有小数位）</p>
<h3 id="自定义表的存储格式"><a href="#自定义表的存储格式" class="headerlink" title="自定义表的存储格式"></a>自定义表的存储格式</h3><p><code>inputformat</code>对象将输入流分割成记录；<code>outputformat</code>对象将记录格式化为输出流（如查询的输出结果）；一个SerDe在读数据时将记录解析列，在写数据时将列编码成记录。<br>SerDe决定了记录是如何分解成字段的（反序列化过程），以及字段是如何写入到存储中的（序列化过程）。</p>
<h3 id="SerDe-Library、InputFormat、outputFormat"><a href="#SerDe-Library、InputFormat、outputFormat" class="headerlink" title="SerDe Library、InputFormat、outputFormat"></a>SerDe Library、InputFormat、outputFormat</h3><p>由一个错误引出：<code>Failed with exception java.io.IOException:java.lang.ClassCastException: org.apache.hadoop.hive.ql.io.orc.OrcStruct cannot be cast to org.apache.hadoop.io.BinaryComparable</code><br>问题复现：<br>建一个外部表，建表语句如下，一般inputformat、outputformat写成下面这样的，都是通过<code>show create table table_name</code>得到建表语句：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE test_orc(</span><br><span class="line">id string,</span><br><span class="line">content string</span><br><span class="line">)</span><br><span class="line">PARTITIONED BY (</span><br><span class="line">dt string )</span><br><span class="line">ROW FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED BY &#39;\u0001&#39;</span><br><span class="line">STORED AS INPUTFORMAT</span><br><span class="line">&#39;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat&#39;</span><br><span class="line">OUTPUTFORMAT</span><br><span class="line">&#39;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat&#39;;</span><br></pre></td></tr></table></figure>
<p>之后使用 <code>select</code>查询该表时报上面的错。</p>
<p>原因分析：<br>orc格式的表通过show create table得到的建表语句直接建外部表，查数据时会报强转失败的错。<br>因为这个建表语句显式指定了STORED AS INPUTFORMAT/OUTPUTFORMAT，但是没有定义serde，serde使用了默认值 。<br>通过<code>describe formatted test_orc</code>看到<code>SerDe Library</code>的类型和<code>inputformat/outputformat</code>没有对应。<br>Your SerDe library is LazySimpleSerde and your Input Format and Output Format are ORC. Totally not gonna work!：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">| <span class="comment"># Storage Information |</span></span><br><span class="line">| SerDe Library: | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |</span><br><span class="line">| InputFormat:   | org.apache.hadoop.hive.ql.io.orc.OrcInputFormat    |</span><br><span class="line">| OutputFormat:  | org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat   |</span><br></pre></td></tr></table></figure>
<p>解决办法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE test_orc(</span><br><span class="line">id string,</span><br><span class="line">content string</span><br><span class="line">)</span><br><span class="line">PARTITIONED BY (</span><br><span class="line">dt string )</span><br><span class="line">ROW FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED BY &#39;\u0001&#39;</span><br><span class="line">STORED AS orc;</span><br></pre></td></tr></table></figure>
<p>之后<code>describe formatted test_orc</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Storage Information	 	 </span></span><br><span class="line">SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde	 </span><br><span class="line">InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat	 </span><br><span class="line">OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat</span><br></pre></td></tr></table></figure>
<p>三者关系详解：</p>
<p>Hive中，默认使用的是TextInputFormat，一行表示一条记录。在每条记录(一行中)，默认使用^A分割各个字段。<br>在有些时候，我们往往面对多行，结构化的文档，并需要将其导入Hive处理。<br>有很多可选的办法来存储数据。如把数据放在一个地方，hive通过<code>external</code>包裹它；<br>如直接放在<code>hive warehouse</code>用表<code>table</code>来管理。可以指定<code>inputformat</code>和<code>outputformat</code>指定表的存储。<br>我们在客户端使用sql操作表，但是底层数据可能是 text file or sequence file or hbase table or some other data structure.</p>
<p>Hive官方说法：<br>SerDe is a short name for “Serializer and Deserializer.”<br>Hive uses SerDe (and !FileFormat) to read and write table rows.<br>HDFS files –&gt; InputFileFormat –&gt; &lt;key, value&gt; –&gt; Deserializer –&gt; Row object<br>Row object –&gt; Serializer –&gt; &lt;key, value&gt; –&gt; OutputFileFormat –&gt; HDFS files</p>
<p><strong><em>总结一下</em></strong>，当面临一个HDFS上的文件时，Hive将如下处理（以读为例）：<br>(1) 调用InputFormat，将文件切成不同的文档。每篇文档即一行(Row)。<br>(2) 调用SerDe的Deserializer，将一行(Row)，切分为各个字段。<br>(3)SerDe：序列化、反序列化。hive读写表数据(不是文件)。可以理解成一行row和多个字段field转变的过程。<br>(4)InputFormat、outputFormat：hdfs文件到表数据的转化。将文件切成不同的文档row；把row组合成底层的文件。</p>
<p>其他：<code>serdeproperties</code>可以传递参数给serde。</p>
<p>这三个参数都可以重写，详细看下面第一个链接。</p>
<p>参考：<a href="https://www.coder4.com/archives/4031" target="_blank" rel="noopener">https://www.coder4.com/archives/4031</a><br><a href="https://stackoverflow.com/questions/42416236/what-is-the-difference-between-inputformat-outputformat-stored-as-in-hive" target="_blank" rel="noopener">https://stackoverflow.com/questions/42416236/what-is-the-difference-between-inputformat-outputformat-stored-as-in-hive</a></p>
<h3 id="hive的默认数据分隔符-A"><a href="#hive的默认数据分隔符-A" class="headerlink" title="hive的默认数据分隔符^A"></a>hive的默认数据分隔符^A</h3><p>hive的默认数据分隔符是\001,也就是^A ，属于不可见字符。</p>
<p>最简单的方法就是用sed（<strong><em>注意这个^A是按CTRL+V+A打出来的，或者按下crtl+v然后再按下crtl+a就会出来/tmp/out目录(\001)</em></strong>，直接输入的^A是不行的。）<br><strong><em>也不能通过复制粘贴的方式。前一个地方用的CTRL+V+A，复制粘贴后就失效，要重新CTRL+V+A。</em></strong><br>例：sed -i ‘s/^A/|/g’ 000000_0</p>
<p>来自网络：<br>在python中可以使用line.split(‘\x01’)来进行切分，也可以使用line.split(‘\001’)，注意其中是单引号<br>在java中可以使用split(“\u0001”)来进行切分</p>
<h3 id="hive默认记录、字段分割符"><a href="#hive默认记录、字段分割符" class="headerlink" title="hive默认记录、字段分割符"></a>hive默认记录、字段分割符</h3><table>
<thead>
<tr>
<th align="center">分隔符</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">\n</td>
<td align="left">换行符。默认记录分隔符，一行一个记录。</td>
</tr>
<tr>
<td align="center">^A</td>
<td align="left">用于分割字段。可用八进制\001表示(看上节)</td>
</tr>
<tr>
<td align="center">^B</td>
<td align="left">用与分割array、struct、map的key-value对。可用八进制\002表示</td>
</tr>
<tr>
<td align="center">^C</td>
<td align="left">用于分割map的key、value对。可用八进制\003表示</td>
</tr>
</tbody></table>
<p>ROW FORMAT DELIMITED必须写在其他字段前，除了stored as。</p>
<h3 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h3><p>array、map、struct三种。好处是处理p/t级数据时，减少寻址，快。坏处是增大数据冗余等。</p>
<h3 id="时间类型"><a href="#时间类型" class="headerlink" title="时间类型"></a>时间类型</h3><p><code>Timestamps</code>类型可以是</p>
<ul>
<li>（1）以秒为单位的整数；</li>
<li>（2）带精度的浮点数，最大精确到小数点后9位，纳秒级；</li>
<li>（3）java.sql.Timestamp格式的字符串 YYYY-MM-DD hh:mm:ss.fffffffff</li>
</ul>
<p><code>Date</code> 只支持YYYY-MM-DD格式的日期，其余写法都是错误的，如需带上时分秒，需使用timestamp。</p>
<h3 id="计算时间的月份差"><a href="#计算时间的月份差" class="headerlink" title="计算时间的月份差"></a>计算时间的月份差</h3><p><code>select floor(months_between(&#39;2018-07-01&#39;,&#39;2018-02-04&#39;)) from default.dual</code><br>返回值为: 4<br>时间格式必须是<code>yyyy-mm-dd</code>，如果是<code>yyyymmdd</code>需要转换<br><code>floor</code>是取整函数</p>
<h3 id="yyyy-mm-dd、yyyymmdd互转"><a href="#yyyy-mm-dd、yyyymmdd互转" class="headerlink" title="yyyy-mm-dd、yyyymmdd互转"></a>yyyy-mm-dd、yyyymmdd互转</h3><p>方法1: from_unixtime+ unix_timestamp</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--20171205转成2017-12-05</span><br><span class="line">select from_unixtime(unix_timestamp(&#39;20171205&#39;,&#39;yyyymmdd&#39;),&#39;yyyy-mm-dd&#39;) from dual;</span><br><span class="line"></span><br><span class="line">--2017-12-05转成20171205</span><br><span class="line">select from_unixtime(unix_timestamp(&#39;2017-12-05&#39;,&#39;yyyy-mm-dd&#39;),&#39;yyyymmdd&#39;) from dual;</span><br></pre></td></tr></table></figure>
<p>方法2: substr + concat</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--20171205转成2017-12-05</span><br><span class="line">select concat(substr(&#39;20171205&#39;,1,4),&#39;-&#39;,substr(&#39;20171205&#39;,5,2),&#39;-&#39;,substr(&#39;20171205&#39;,7,2)) from dual;</span><br><span class="line"></span><br><span class="line">--2017-12-05转成20171205</span><br><span class="line">select concat(substr(&#39;2017-12-05&#39;,1,4),substr(&#39;2017-12-05&#39;,6,2),substr(&#39;2017-12-05&#39;,9,2)) from dual;</span><br></pre></td></tr></table></figure>
<h3 id="执行外部命令"><a href="#执行外部命令" class="headerlink" title="执行外部命令"></a>执行外部命令</h3><p>hadoop命令：<br>把命令行里的hadoop去掉。<br>如直接执行<code>dfs -ls ...;</code>此种方法相叫hadoop的命令更为高效，hadoop是新开一个jvm线程执行，前者在当前线程执行。</p>
<p>其他命令，以!开始，以;结束，不能用管道、文件补全、用户交互等操作。<br>如<code>!echo &#39;li&#39;;</code></p>
<h3 id="if"><a href="#if" class="headerlink" title="if"></a>if</h3><p>If 函数语法: if(boolean testCondition, T valueTrue, T valueFalseOrNull)<br>返回值: T<br>说明:  当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull<br>举例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select if(1&#x3D;2,100,200) from dual;</span><br><span class="line">200</span><br></pre></td></tr></table></figure>

<h3 id="【is-null】-【-null】？、【is-not-null】-【-lt-gt-null】？"><a href="#【is-null】-【-null】？、【is-not-null】-【-lt-gt-null】？" class="headerlink" title="【is null】 = 【 = null】？、【is not null】 = 【 &lt;&gt; null】？"></a>【is null】 = 【 = null】？、【is not null】 = 【 &lt;&gt; null】？</h3><p>hive 里（包括IF函数与Where条件里）判断是否为NULL要用 is null或 is not null ，不能使用 &lt;&gt; null 或 = null（虽然不报错）<br>null在hive底层默认是用’\N’来存储的，可以通过alter table test SET SERDEPROPERTIES(‘serialization.null.format’ = ‘a’);来修改。</p>
<h3 id="lt-gt-区别"><a href="#lt-gt-区别" class="headerlink" title="&lt;&gt; != 区别"></a>&lt;&gt; != 区别</h3><p>语法: A &lt;&gt; B<br>操作类型: 所有基本类型<br>描述: 如果表达式A为NULL，或者表达式B为NULL，返回NULL，因此比较时要特别注意字段为null的情况（如果有一边等于null，结果就是false）；<br>如果表达式A与表达式B不相等，则为TRUE；否则为FALSE</p>
<p>hive中，当两边数据类型不对等时，比较的时候会出现问题。</p>
<h3 id="insert"><a href="#insert" class="headerlink" title="insert"></a>insert</h3><p>1.insert into是增加数据<br>2.insert overwrite是删除原有数据然后在新增数据，如果有分区那么只会删除指定分区数据，其他分区数据不受影响</p>
<h3 id="rand"><a href="#rand" class="headerlink" title="rand"></a>rand</h3><p>语法: rand(),rand(int seed)<br>返回值: double<br>说明:返回一个0到1范围内的随机数。如果指定种子seed，则会等到一个稳定的随机数序列</p>
<h3 id="cast"><a href="#cast" class="headerlink" title="cast"></a>cast</h3><p>作用：转换<br>格式 cast(col as type)</p>
<h3 id="binary-string-binary"><a href="#binary-string-binary" class="headerlink" title="binary(string|binary)"></a>binary(string|binary)</h3><p>将输入的值转换成二进制  </p>
<h3 id="base64-binary-bin"><a href="#base64-binary-bin" class="headerlink" title="base64(binary bin)"></a>base64(binary bin)</h3><p>将二进制bin转换成64位的字符串</p>
<h3 id="find-in-set查找函数"><a href="#find-in-set查找函数" class="headerlink" title="find_in_set查找函数"></a>find_in_set查找函数</h3><p>集合查找函数: find_in_set<br>语法: find_in_set(string str, string strList)<br>返回值: int<br>说明: 返回str在strlist第一次出现的位置，strlist是用逗号分割的字符串。如果没有找该str字符，则返回0<br>例子：<code>select find_in_set(&#39;de&#39;,&#39;ef,ab,de&#39;);</code> 返回3</p>
<h3 id="decimal"><a href="#decimal" class="headerlink" title="decimal"></a>decimal</h3><p>DECIMAL Hive 0.11.0引入，Hive 0.13.0开始，用户可以使用DECIMAL(precision, scale) 语法在创建表时来定义Decimal数据类型的precision和scale。<br>如果未指定precision，则默认为10。如果未指定scale，它将默认为0（无小数位）。<br><strong><em>曾遇到这样的问题，创建的外部表没有指定精度，外部表指定的内部表有指定精度，从外部表查数据时仍然截断了小数部分。</em></strong></p>
<h3 id="export-LC-ALL-en-US-UTF-8"><a href="#export-LC-ALL-en-US-UTF-8" class="headerlink" title="export LC_ALL=en_US.UTF-8"></a>export LC_ALL=en_US.UTF-8</h3><p><code>export LC_ALL=en_US.UTF-8</code> 解决hive客户端调用脚本中文问题</p>
<p><a href="https://perlgeek.de/en/article/set-up-a-clean-utf8-environment" target="_blank" rel="noopener">https://perlgeek.de/en/article/set-up-a-clean-utf8-environment</a></p>
<h3 id="SIZE"><a href="#SIZE" class="headerlink" title="SIZE"></a>SIZE</h3><p>数组长度。<br>注意的是，如果和split一起用<code>size(split(str, &#39;operate&#39;))</code>，如果str为‘’或者null时，返回的结果是1；因为split返回的是有一个空串的数组。</p>
<h3 id="get-json-object"><a href="#get-json-object" class="headerlink" title="get_json_object"></a>get_json_object</h3><p><code>get_json_object(json_string,’$.str’)</code> 得到json字符串json_string的$.str节点的值，$指根节点。</p>
<h3 id="REGEXP-RLIKE-LIKE"><a href="#REGEXP-RLIKE-LIKE" class="headerlink" title="REGEXP/RLIKE/LIKE"></a>REGEXP/RLIKE/LIKE</h3><p>语法: A REGEXP B<br>操作类型: strings<br>描述: 功能与RLIKE相同</p>
<p>LIKE:不是正则，而是通配符。这个通配符可以看一下SQL的标准，例如%代表任意多个字符。<br>RLIKE:是正则，正则的写法与java一样。功能与REGEXP相同.</p>
<h3 id="REGEXP-EXTRACT"><a href="#REGEXP-EXTRACT" class="headerlink" title="REGEXP_EXTRACT"></a>REGEXP_EXTRACT</h3><p>regexp_extract(string subject, string pattern, int index)<br>通过下标返回正则表达式指定的部分。正则<code>\</code>需要转义<code>\\</code>,例如’\w’需要使用’\w’<br>index指的是：返回所有匹配的第N个.<br>参考：<a href="http://www.cnblogs.com/judylucky/p/3713774.html" target="_blank" rel="noopener">http://www.cnblogs.com/judylucky/p/3713774.html</a></p>
<h3 id="NVL"><a href="#NVL" class="headerlink" title="NVL"></a>NVL</h3><p><code>NVL( str, replace_with)</code><br>str为NULL, 则NVL函数返replace_with值，否则返str值</p>
<h3 id="CONCAT-str1-str2-…"><a href="#CONCAT-str1-str2-…" class="headerlink" title="CONCAT(str1, str2,…)"></a>CONCAT(str1, str2,…)</h3><p>连接字符串，如果参数中有null，返回结果也会是null，因此可以结合上面的方面使用。</p>
<h3 id="CONCAT-WS-separator-str1-str2-…"><a href="#CONCAT-WS-separator-str1-str2-…" class="headerlink" title="CONCAT_WS(separator, str1, str2,…)"></a>CONCAT_WS(separator, str1, str2,…)</h3><p>它是一个特殊形式的 CONCAT()。第一个参数是剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间<br>这个函数会跳过分隔符参数后的任何 NULL 和空字符串，但是<strong>跳过空字符串后还是会有多余的分隔符存在</strong>（非常鸡肋啊）。</p>
<h3 id="COLLECT-SET-列转行"><a href="#COLLECT-SET-列转行" class="headerlink" title="COLLECT_SET() 列转行"></a>COLLECT_SET() 列转行</h3><p>是 Hive 内置的一个聚合函数, 它返回一个<strong><em>消除了重复元素</em></strong>的对象集合, 其返回值类型是 array 。<br>把group by值一样的分组由列变成行，即变成数组，可以用下标访问。<br>collect_set()方法把group by一样的组里的数据组成一个数组。数组从0开始，如果直接select数组，是[item1, …, itemn]的格式。<br>如<code>select collect_set(uname) unames ....group by uid</code>，把同一个uid的uname组成数组， 通过别名unames[ind]访问数据。<br><code>concat_ws(&#39;,&#39;,collect_set(cast(col_0 as string)))</code> 两个一起使用把列变成由逗号分割的行。</p>
<h3 id="COLLECT-LIST-行转列去重"><a href="#COLLECT-LIST-行转列去重" class="headerlink" title="COLLECT_LIST() 行转列去重"></a>COLLECT_LIST() 行转列去重</h3><p><code>collect_list(id)</code> 列出该字段所有的值，列出来<strong><em>不去重</em></strong></p>
<h3 id="EXPLODE-行转列"><a href="#EXPLODE-行转列" class="headerlink" title="EXPLODE 行转列"></a>EXPLODE 行转列</h3><p>上面的collect_set是把列变成行，explode是把行变成列。<br><code>explode(array)</code> 把数组里的数据变成列形式。经常与split一起用。<br>例如，在wordcount中有<code>explode(split(line, &#39; &#39;))</code> 或<code>explode(split(line, &#39;\\s&#39;))</code></p>
<h3 id="LATERAL-VIEW-一行变多行"><a href="#LATERAL-VIEW-一行变多行" class="headerlink" title="LATERAL VIEW 一行变多行"></a>LATERAL VIEW 一行变多行</h3><p>lateral view用于和split、explode等UDTF一起使用的，能将一行数据拆分成多行数据，在此基础上可以对拆分的数据进行聚合。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT myCol1, myCol2, col3 FROM baseTable</span><br><span class="line">LATERAL VIEW explode(col1) myTable1 AS myCol1</span><br><span class="line">LATERAL VIEW explode(col2) myTable2 AS myCol2;</span><br></pre></td></tr></table></figure>
<p>示例：<br><strong><em>执行过程是先执行from到 as cloumn的列过程，再执行select 和where后边的语句；</em></strong><br>sql如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select datenu,des,type from tb_split</span><br><span class="line">lateral view explode(split(des,&quot;&#x2F;&#x2F;|&quot;)) tb1 as des</span><br><span class="line">lateral view explode(split(type,&quot;&#x2F;&#x2F;|&quot;)) tb2 as type</span><br></pre></td></tr></table></figure>
<p>数据如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">20141018  aa|bb 7|9</span><br><span class="line">20141019  cc|dd  6|1|8</span><br></pre></td></tr></table></figure>
<p>希望的结果是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">20141018  aa 7</span><br><span class="line">20141018  aa 9</span><br><span class="line">20141018  bb 7</span><br><span class="line">20141018  bb 9</span><br><span class="line">20141019  cc  6</span><br><span class="line">20141019  cc  1</span><br><span class="line">20141019  cc  8</span><br><span class="line">20141019  dd  6</span><br><span class="line">20141019  dd  1</span><br><span class="line">20141019  dd  8</span><br></pre></td></tr></table></figure>
<h3 id="json数组行转列"><a href="#json数组行转列" class="headerlink" title="json数组行转列"></a>json数组行转列</h3><p>方法1：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> get_json_object(<span class="keyword">col</span>, <span class="string">'$.bssid'</span>)</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">  <span class="keyword">select</span> <span class="string">'[&#123;"bssid":"6C:59:40:21:05:C4","ssid":"MERCURY_05C4"&#125;,&#123;"bssid":"AC:9C:E4:04:EE:52","appid":"10003","ssid":"and-Business"&#125;]'</span> <span class="keyword">as</span> <span class="keyword">str</span></span><br><span class="line">  <span class="keyword">from</span> dual</span><br><span class="line">) pp</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(regexp_replace(regexp_extract(pp.str, <span class="string">'^\\[(.+)\\]$'</span>, <span class="number">1</span>),<span class="string">'\\&#125;\\,\\&#123;'</span>, <span class="string">'\\&#125;\\|\\|\\&#123;'</span>),<span class="string">'\\|\\|'</span>)) ss <span class="keyword">as</span> <span class="keyword">col</span>;</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">6C:59:40:21:05:C4</span><br><span class="line">AC:9C:E4:04:EE:52</span><br></pre></td></tr></table></figure>
<p>方法2：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select get_json_object(&#39;[&#123;&quot;bssid&quot;:&quot;6C:59:40:21:05:C4&quot;,&quot;ssid&quot;:&quot;MERCURY_05C4&quot;&#125;,&#123;&quot;bssid&quot;:&quot;AC:9C:E4:04:EE:52&quot;,&quot;appid&quot;:&quot;10003&quot;,&quot;ssid&quot;:&quot;and-Business&quot;&#125;]&#39;, &#39;$[@.bssid]&#39;) as str</span><br><span class="line">from dual;</span><br></pre></td></tr></table></figure>
<p>结果与方法1一样。</p>
<p>差别：第一种可以得到所有的数组，第二中只能得到数组里某个值。</p>
<h3 id="LEAD-LAG-FIRST-VALUE-LAST-VALUE窗口函数"><a href="#LEAD-LAG-FIRST-VALUE-LAST-VALUE窗口函数" class="headerlink" title="LEAD LAG FIRST_VALUE LAST_VALUE窗口函数"></a>LEAD LAG FIRST_VALUE LAST_VALUE窗口函数</h3><table>
<thead>
<tr>
<th align="left">窗口函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">LAG()</td>
<td align="left">LAG()窗口函数返回分区中当前行之前行（可以指定第几行）的值。 如果没有行，则返回null。</td>
</tr>
<tr>
<td align="left">LEAD()</td>
<td align="left">LEAD()窗口函数返回分区中当前行后面行（可以指定第几行）的值。 如果没有行，则返回null。</td>
</tr>
<tr>
<td align="left">FIRST_VALUE</td>
<td align="left">FIRST_VALUE窗口函数返回相对于窗口中第一行的指定列的值。</td>
</tr>
<tr>
<td align="left">LAST_VALUE</td>
<td align="left">LAST_VALUE窗口函数返回相对于窗口中最后一行的指定列的值。</td>
</tr>
</tbody></table>
<p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">LAG | LEAD</span><br><span class="line">( &lt;col&gt;, &lt;line_num&gt;, &lt;DEFAULT&gt; )</span><br><span class="line">OVER ( [ PARTITION BY ] [ ORDER BY ] )</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FIRST_VALUE | LAST_VALUE</span><br><span class="line">( &lt;col&gt;,&lt;ignore nulls as boolean&gt; ) OVER</span><br><span class="line">( [ PARTITION BY ] [ ORDER BY ][ window_clause ] )</span><br></pre></td></tr></table></figure>

<p>参考：<a href="https://blog.csdn.net/sunnyyoona/article/details/56484919" target="_blank" rel="noopener">https://blog.csdn.net/sunnyyoona/article/details/56484919</a></p>
<h3 id="row-num-rank-dense-rank-over-…-。"><a href="#row-num-rank-dense-rank-over-…-。" class="headerlink" title="row_num()/rank/dense_rank over (…)。"></a>row_num()/rank/dense_rank over (…)。</h3><p>从1开始，为每个分组的每条记录返回一个数字。<br>1例如，<code>ROW_NUMBER() OVER (ORDER BY xlh DESC)</code> 是先按照xlh列降序，再为降序以后的每条记录返回一个序号。<br>2例<br>数据库中有数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">empid       deptid      salary</span><br><span class="line">----------- ----------- ---------------------------------------</span><br><span class="line">1           10          5500.00</span><br><span class="line">2           10          4500.00</span><br><span class="line">3           20          1900.00</span><br><span class="line">4           20          4800.00</span><br><span class="line">5           40          6500.00</span><br><span class="line">6           40          14500.00</span><br><span class="line">7           40          44500.00</span><br><span class="line">8           50          6500.00</span><br><span class="line">9           50          7500.00</span><br></pre></td></tr></table></figure>
<p>需求<code>根据部门分组，显示每个部门的工资等级</code><br>sql：<code>SELECT *, Row_Number() OVER (partition by deptid ORDER BY salary desc) rank FROM employee</code><br>结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">empid       deptid      salary                                  rank</span><br><span class="line">----------- ----------- --------------------------------------- --------------------</span><br><span class="line">1           10          5500.00                                 1</span><br><span class="line">2           10          4500.00                                 2</span><br><span class="line">4           20          4800.00                                 1</span><br><span class="line">3           20          1900.00                                 2</span><br><span class="line">7           40          44500.00                                1</span><br><span class="line">6           40          14500.00                                2</span><br><span class="line">5           40          6500.00                                 3</span><br><span class="line">9           50          7500.00                                 1</span><br><span class="line">8           50          6500.00                                 2</span><br></pre></td></tr></table></figure>
<p>例子参考：<a href="https://blog.csdn.net/biaorger/article/details/38523527" target="_blank" rel="noopener">https://blog.csdn.net/biaorger/article/details/38523527</a><br>row_number()另一作用可以用来去除重复：先按分组字段分区，再通过 rownum = 1过滤即可。另外，去重还可以借助于group by。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a,row_number() <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> b) row_number,<span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> b) <span class="keyword">rank</span>,</span><br><span class="line"><span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> b) <span class="keyword">dense_rank</span> <span class="keyword">from</span> test_rank;</span><br><span class="line"></span><br><span class="line">result：</span><br><span class="line">a   row_number  rank    dense_rank</span><br><span class="line">A       1        1          1</span><br><span class="line">C       2        2          2</span><br><span class="line">D       3        3          3</span><br><span class="line">B       4        3          3</span><br><span class="line">E       5        5          4</span><br><span class="line">F       6        6          5</span><br><span class="line">G       7        7          6</span><br></pre></td></tr></table></figure>

<h3 id="partition-by与group-by-的区别"><a href="#partition-by与group-by-的区别" class="headerlink" title="partition by与group by 的区别"></a>partition by与group by 的区别</h3><p>后者是经典的使用，是对检索结果的保留行进行单纯分组，如果有sum函数，就是先分组再对每个分组求和；<br>前者类似虽然也具有分组功能，但同时也具有其他的功能，如果有sum函数，是先分组，再累加，会把分组里累加的过程输出。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">表：</span><br><span class="line">B  C  D  </span><br><span class="line">02 02 1</span><br><span class="line">02 02 13</span><br><span class="line"></span><br><span class="line">select b,c,sum(d) e from a group by b,c;   </span><br><span class="line">结果：</span><br><span class="line">B   C  E  </span><br><span class="line">02 02 13</span><br><span class="line">SELECT b, c, d, SUM(d) OVER(PARTITION BY b,c ORDER BY d) e FROM a;  </span><br><span class="line">结果：</span><br><span class="line">B C E  </span><br><span class="line">02 02 1  </span><br><span class="line">02 02 13</span><br></pre></td></tr></table></figure>
<p>从上面的例子中可以看到第二条语句的累加过程</p>
<p><strong><em>hive中group by和mysql不同。mysql可以接受select处理后的别名作为group by，hive的group by不能接受。</em></strong></p>
<h3 id="ORDER-SORT-DISTRIBUTE-BY"><a href="#ORDER-SORT-DISTRIBUTE-BY" class="headerlink" title="ORDER /SORT /DISTRIBUTE BY"></a>ORDER /SORT /DISTRIBUTE BY</h3><ul>
<li><code>ORDER BY</code> 全局排序，会将所有数据送到同一个Reducer中后再对所有数据进行排序，对于大数据会很慢，谨慎使用</li>
<li><code>SORT BY</code> 局部排序，只会在每一个Reducer中对数据进行排序，在每个Reducer输出是有序的，但并非全局排序（每个reducer出来的数据是有序的，但是不能保证所有的数据是有序的——即文件(分区)之间无序，除非只有一个reducer）</li>
<li><code>DISTRIBUTE BY</code> 控制map的输出被送到哪个reducer端进行汇总计算，相同字段的map输出会发到一个reduce节点去处理。通过这个特性可以强行使hql有reduce，伴随有减少mapper输出文件个数、减轻数据倾斜等功效，可看下面链接里的例子。用distribute by 会对指定的字段按照hashCode值对reduce的个数取模，然后将任务分配到对应的reduce中去执行</li>
</ul>
<p>关与<code>DISTRIBUTE BY</code>使用非常好的文章：<a href="https://www.iteblog.com/archives/1533.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/1533.html</a></p>
<p><del>注：HIVE reducer分区个数由mapreduce.job.reduces来决定，该选项只决定使用哪些字段做为分区依据，如果没通过DISTRIBUTE BY指定分区字段，则默认将整个文本行做为分区依据。分区算法默认是HASH，也可以自己实现。这里DISTRIBUTE BY讲的分区概念是指Hadoop里的，而非我们HIVE数据文本存储分区。Hadoop里的Partition主要作用就是将map的结果发送到相应的reduce，默认使用HASH算法，不过可以重写.</del></p>
<h3 id="group-by-1-2-3"><a href="#group-by-1-2-3" class="headerlink" title="group by 1, 2, 3"></a>group by 1, 2, 3</h3><p><code>SET hive.groupby.orderby.position.alias=true</code> 默认是false。（mysql可直接使用。）<br>打开这个开关后，group by可以通过1， 2， 3这样的数字指定 使用select的第几个字段。<br>示例：<code>SELECT substr(date, 1, 4), count(1) year FROM *** GROUP BY 1;</code></p>
<h3 id="having-by"><a href="#having-by" class="headerlink" title="having by"></a>having by</h3><p>GROUP BY子句之后使用Having子句<br>可应用限定条件进行分组，以便系统仅对满足条件的组返回结果。<br>在GROUP BY子句后面包含了一个HAVING子句。HAVING类似于WHERE（唯一的差别是WHERE过滤行，HAVING过滤组）AVING支持所有WHERE操作符。</p>
<h3 id="导出数据到本地"><a href="#导出数据到本地" class="headerlink" title="导出数据到本地"></a>导出数据到本地</h3><p>hive的-e和-f参数可以用来导出数据。<br>-e 表示后面直接接带双引号的sql语句；而-f是接一个文件，文件的内容为sql语句。<br>（1）<code>hive -e &quot;use test; select * from student where sex = &#39;男&#39;&quot; &gt; /tmp/output.txt</code><br>（2）<code>insert overwrite local directory &quot;/tmp/out&quot; select cno,avg(grade) from sc group by(cno);</code><br>（3）<code>insert overwrite local directory &quot;/tmp/out&quot; row format delimited fields terminated by &#39; &#39; select cno,avg(grade) from sc group by(cno);</code></p>
<p>（2）也可以作为（1）中-e的参数执行。<br>（2）这条HQL的执行需要启用Mapreduce完成，运行完这条语句之后，将会在本地文件系统的/tmp/out目录下生成文件，这个文件是Reduce产生的结果（这里生成的文件名是000000_0），数据的分割使用的就是上面提到的^A<br>（3）通过加入<code>row format delimited fields terminated by &#39; &#39;</code>使的数据的分割是空格，而不是^A.<br>（1）中会直接保存成本地文件，把数据直接保存在/tmp/output.txt中，数据默认由空格分割。<br>（2）这条HQL的‘local’去掉，数据会被保存在hdfs系统的/tmp/out目录下。<br>（2）不能使用<code>insert into</code>或者<code>insert local</code></p>
<h3 id="into-overwrite-导出数据到表"><a href="#into-overwrite-导出数据到表" class="headerlink" title="into/overwrite 导出数据到表"></a>into/overwrite 导出数据到表</h3><p>把表2的数据导出到表1：<br>(1)<code>insert into table_name1(...) select ... from table_name2</code><br>(2)<code>insert overwrite table_name1(...) select ... from table_name2</code><br>select 部分不能用括号，否则会被认为是表1的字段；<br>(…)中是表1的字段，可以省略； <code>select ...</code> 可以用<code>select *</code> 代替；<br>(1)是直接导入，(2)是覆盖原来数据导入。</p>
<p>导入到分区表：<br>(1)<code>insert into table_name1 partition(dt=&#39;2018-03-11&#39;) select ... from table_name2</code><br>(2)<code>set hive.exec.dynamic.partition.mode=nonstrict;insert into table_name1 partition(dt) select ... from table table_name2</code></p>
<p>同样可以把<code>into</code>换成<code>overwrite table</code>以达到覆盖的效果。注意：只会覆盖table_name2中存在的对应分区，table_name1中已经存在的分区，table_name2中没有是不会进行覆盖。 即，覆盖只是覆盖分区里的数据数据、追加分区，原分区不变。<br>(1)是导入一个分区的数据 <code>select ...</code>部分不用带dt(分区)的值。注意，如果表2也是分区表，此时不能用<code>select *</code>，因为它查出来的数据有分区字段，比<code>insert</code>的多一个字段。<br>(2)是导入多个分区的表，执行前需要<code>set hive.exec.dynamic.partition.mode=nonstrict;</code>，因为严格模式下，不允许所有的分区都被动态指定，目的是为了防止生成太多的目录.此时<code>select ...</code>必须有dt分区的字段。<br>(2)是动态分区，不指定分区，一次可以导入多个分区。</p>
<h3 id="文件导入数据到表"><a href="#文件导入数据到表" class="headerlink" title="文件导入数据到表"></a>文件导入数据到表</h3><p><code>load data [local] inpath &#39;file1.txt&#39; [overwrite] into table table_name [partition(partcol=val)]</code><br>通常情况下，会不只是一个文件，而是一个目录，load操作会把目录下的文件全部拷贝到表的location下。<br><code>local</code> 决定文件是来自本地还是hdfs。<br><code>overwrite</code> 决定是否要覆盖。<br><code>load</code>命令不支持动态分区，必须指定分区。(可以把数据先转到非分区表，再利用上面小节“导出数据到表”的方法把非分区表的数据导入到分区表)。不指定分区，会报错<code>FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Invalid partition key &amp; values; keys [dt, ], values [])</code><br>load不能加载桶表数据，只能从另一张表加载数据。(和动态分区的解决方案一样，建一个中间表作为过渡表)。</p>
<p>hive不会检验用户装载的数据和表的模式是否匹配，但是会验证装载文件的类型和表的定义类型是否匹配。比如，表的定义是sequencefile，则数据文件必须是sequencefile</p>
<h3 id="同时插入多个表"><a href="#同时插入多个表" class="headerlink" title="同时插入多个表"></a>同时插入多个表</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from test t</span><br><span class="line">insert into table test1 select ...</span><br><span class="line">insert into table test3 select ...</span><br></pre></td></tr></table></figure>
<p>从test中查数同时插入到test1、test3。每个select都必须存在，可以用*<br>eg:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from test_part</span><br><span class="line">insert into table test_part2 partition(dt&#x3D;&#39;2018-08-19&#39;, source&#x3D;&#39;app&#39;) select id</span><br><span class="line">insert into table test_part4 partition(dt&#x3D;&#39;2018-08-10&#39;, source&#x3D;&#39;app&#39;) select id;</span><br></pre></td></tr></table></figure>
<h3 id="自定义UDF-UDTF-UDAF"><a href="#自定义UDF-UDTF-UDAF" class="headerlink" title="自定义UDF/UDTF/UDAF"></a>自定义UDF/UDTF/UDAF</h3><p>网上介绍了四中方法。只验证过第一种。<br>方法（1）最常用也最不被喜欢的方法。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">add jar testUDF-0.0.1-SNAPSHOT.jar;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">function</span> zodiac <span class="keyword">as</span> <span class="string">"com.hive.udf.UDFZodiacSign"</span>;</span><br></pre></td></tr></table></figure>
<p>之后就可以在sql里直接使用<code>zodiac()</code>。但是这种方法只存在在当前会话中。<br>每次会话都要重新add、create。（下面的.hiverc文件可以解决每次都要add、create问题）<br>其他方法：<a href="https://www.cnblogs.com/chushiyaoyue/p/6632090.html?utm_source=itdadao&amp;utm_medium=referral" target="_blank" rel="noopener">https://www.cnblogs.com/chushiyaoyue/p/6632090.html?utm_source=itdadao&amp;utm_medium=referral</a></p>
<p>UDF：可直接应用于select语句，对查询结构做格式化处理后，再输出内容。<br>UDTF：用来解决 输入一行输出多行(On-to-many maping) 的需求。<code>lateral view</code>一行转多行，有些字段无法使用split等函数剪切成数组。<br>UDAF：实现聚类函数（eg，sum/agv）。<br>参考：<a href="https://blog.csdn.net/liuj2511981/article/details/8523084" target="_blank" rel="noopener">https://blog.csdn.net/liuj2511981/article/details/8523084</a></p>
<h3 id="python-transform"><a href="#python-transform" class="headerlink" title="python-transform"></a>python-transform</h3><p>transform中的值作为输入， 然后传递给python脚本，最后经过python的处理后，输出想要得到的字符串格式。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">add file *.py;</span><br><span class="line">select transform(intput columns)</span><br><span class="line">using &#39;python *.py&#39;</span><br><span class="line">as (output columns)</span><br></pre></td></tr></table></figure>
<p>例子：<br>hive map中字段自增的写法（转）  </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 1、建立表结构  </span></span><br><span class="line">hive&gt; CREATE TABLE t3 (foo STRING, bar MAP&lt;STRING,INT&gt;)  </span><br><span class="line">    &gt; ROW FORMAT DELIMITED  </span><br><span class="line">    &gt; FIELDS TERMINATED BY '/t'  </span><br><span class="line">    &gt; COLLECTION ITEMS TERMINATED BY ','  </span><br><span class="line">    &gt; MAP KEYS TERMINATED BY ':'  </span><br><span class="line">    &gt; STORED AS TEXTFILE;  </span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2、load test.txt文件  </span></span><br><span class="line">jeffgeng        click:13,uid:15</span><br><span class="line"><span class="comment">-- 3、编写add_mapper，python脚本要去除字典转换后遗留下来的空格，引号，左右花排号等   </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#!/usr/bin/python  </span></span><br><span class="line">import sys  </span><br><span class="line">import datetime  </span><br><span class="line"></span><br><span class="line">for line in sys.stdin:  </span><br><span class="line">    line = line.strip('/t')  </span><br><span class="line">    foo, bar = line.split('/t')  </span><br><span class="line">    d = eval(bar)  </span><br><span class="line">    d['click'] += 1  </span><br><span class="line">    d['uid'] += 1  </span><br><span class="line">    strmap = ''  </span><br><span class="line">    for x in str(d):  </span><br><span class="line">        if x in (' ', "'"):  </span><br><span class="line">            continue  </span><br><span class="line">        strmap += x  </span><br><span class="line">    print '/t'.join([foo, strmap])  </span><br><span class="line"></span><br><span class="line"><span class="comment">-- 4、使用</span></span><br><span class="line">add FILE add_mapper.py;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> t4  </span><br><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">  TRANSFORM (foo, bar)  </span><br><span class="line">  <span class="keyword">USING</span> <span class="string">'python add_mapper.py'</span>  </span><br><span class="line">  <span class="keyword">AS</span> (foo <span class="keyword">string</span>, bar <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="built_in">int</span>&gt;)  </span><br><span class="line"><span class="keyword">FROM</span> t3;</span><br></pre></td></tr></table></figure>

<h3 id="e-f-S"><a href="#e-f-S" class="headerlink" title="-e/f/S"></a>-e/f/S</h3><p><code>-e</code> : 执行短命令<br><code>-f</code> :  执行文件（适合脚本封装）<br><code>-S</code> : 安静模式，不显示MR的运行过程</p>
<h3 id="hiverc文件"><a href="#hiverc文件" class="headerlink" title=".hiverc文件"></a>.hiverc文件</h3><p>网上说在<code>${HIVE_HOME}/bin</code>目录下（我目前遇到别人部署的hive是在用户目录下）<br>（<code>ls -a</code>命令查看隐藏文件）<br>它是在hive启动的时候被调用，可以在里面定义常用的参数。<br>写到这个是因为，还可以把上面加载udf的最常用最不被喜欢的第一种方法的add、create语句写到.hiverc文件里，这样每次启动hive时都默认加载了udf方法。</p>
<h3 id="i"><a href="#i" class="headerlink" title="-i"></a>-i</h3><p>-i 参数可以指定一个hive启动就被调用的文件。对，默认就是上面的.hiverc文件！</p>
<h3 id="set变量"><a href="#set变量" class="headerlink" title="set变量"></a>set变量</h3><table>
<thead>
<tr>
<th align="left">命名空间</th>
<th align="left">使用权限</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">hivevar</td>
<td align="left">可读可写</td>
<td align="left">hive 0.18.0 版本及之后。用户自定义变量</td>
</tr>
<tr>
<td align="left">hiveconf</td>
<td align="left">可读可写</td>
<td align="left">hive相关的配置属性</td>
</tr>
<tr>
<td align="left">system</td>
<td align="left">可读可写</td>
<td align="left">java相关的配置属性</td>
</tr>
<tr>
<td align="left">env</td>
<td align="left">只可读</td>
<td align="left">shell环境定义的环境变量</td>
</tr>
</tbody></table>
<p>hivevar例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hivevar:dd&#x3D;&#39;aa&#39;;</span><br><span class="line">select $&#123;hivevar:dd&#125;</span><br></pre></td></tr></table></figure>
<p>hivevar的前缀可以省略，但是可能会找不到变量，不建议省略。<br>system、env的前缀不能省。</p>
<p>上面是在hive的终端里，另一种是在shell里使用。在实践中使用时，<code>create_table.sql</code>需要用<code>&quot;${hivevar:dd}&quot;</code>,即需要单/双引号。<br>另外<code>-hivevar</code>可用<code>--define</code><br><code>hive -hivevar dd=&#39;aa&#39; -f ./create_table.sql</code><br><strong><em>多个参数</em></strong>多次指定<code>hivevar</code>： <code>hive -hivevar dd=&#39;aa&#39; -hivevar d=&#39;aaa&#39; -f ./create_table.sql</code></p>
<p>直接<code>set</code>命令可以看到所有变量值。<br><code>set</code>单个参数，可以看见这个参数的值。</p>
<h3 id="tblproperties"><a href="#tblproperties" class="headerlink" title="tblproperties"></a>tblproperties</h3><p><code>tblproperties</code> 主要的作用是以键值对的格式为表增加额外的文档说明。<br>（hive和像DymamoDB这样的数据库集成时，<code>tblproperties</code> 还有用作数据库连接的必要的元数据信息）<br>Hive会自动增加两个表属性：last_modified_by，保存最后修改这个表的用户的用户名；last_modified_time，保存最后一次修改的时间秒，但是如果用户没有手动定义任何的文档说明，这两个属性还是不会自动添加的。<br><code>show tblproperties table_name</code> 查看表的<code>tblproperties</code>信息</p>
<h3 id="LEFT-SEMI-JOIN"><a href="#LEFT-SEMI-JOIN" class="headerlink" title="LEFT SEMI JOIN"></a>LEFT SEMI JOIN</h3><p>hive中没有实现in/exist，使用<code>left semi join</code>代替<br><code>left semi join</code> 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。<br>例子<br>mysql中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT a.key, a.value</span><br><span class="line">  FROM a</span><br><span class="line">  WHERE a.key in</span><br><span class="line">   (SELECT b.key</span><br><span class="line">    FROM B);</span><br></pre></td></tr></table></figure>
<p>hive重写为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT a.key, a.val</span><br><span class="line">FROM a LEFT SEMI JOIN b on (a.key &#x3D; b.key)</span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://my.oschina.net/leejun2005/blog/188459" target="_blank" rel="noopener">https://my.oschina.net/leejun2005/blog/188459</a></p>
<h3 id="join原理、调优"><a href="#join原理、调优" class="headerlink" title="join原理、调优"></a>join原理、调优</h3><p>(join 时，每次 map/reduce 任务的逻辑是这样的：reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。)<br>hive允许多个表进行join，如果多个表的on字段都是依据的同一列，将只需要一个MR任务。流程是，前两个表放在reduce内存中，第三个表经过shuffle后通过流式的方式一行一行进入前面的reduce。<br>很好理解，一般来说（map side join除外）Map过程负责分发数据，具体JOIN操作在Reduce完成，因此，如果多表基于不同的列做JOIN，则无法在一轮MapReduce任务中将所有相关数据Shuffle到同一个Reduce。<br>因此把数据量大的表放在最后join，也可以通过关键字STREAMTABLE指定流式进入的表，这样可以节省reduce不必要的内存。</p>
<p>例如：以下将被“翻译”成1个MapReduce任务(join都基于table2.key1)<br><code>SELECT talble1.val,table2.val,table3.val from table1 JOIN table2 ON (table1.key=table2.key1) JOIN table3 ON(table3.key = table2.key1)</code><br>以下将被“翻译”成2个MapReduce任务<br><code>SElECT table1.val,table2.val,table3.val from table1 JOIN table2 ON(table1.key=table2.key1) JOIN table3 ON(table3.key = table2.key2)；</code></p>
<p>通过“STREAMTABLE”显示指定Reduce流式读入的表：<br><code>SELECT STREAMTABLE(table1) table1.val,table2.val,table3.val from table1 JOIN table2 on (table1.key = table2.key1) JOIN table3 ON(table3.key = table2.key1)</code></p>
<p>对于多表JOIN，Hive会将前面的表缓存在Reduce内存中，然后后面的表会流式的进入Reduce和Reduce内存中其他的表做JOIN。例如：<br><code>SELECT table1.val,table2.val,table3.val from a JOIN b on (table1.key = table2.key1) JOIN c ON(table3.key = table2.key1)</code><br>在Reduce中，table1、table2表等待JOIN的数据会放在内存中，这会引发一些问题，如果Reduce个数不足或者table1,table2表数据过大，可能导致Out of Memory<br>因此，需要将数据量最大的表放到最后，或者通过“STREAMTABLE”显示指定Reduce流式读入的表。例如：<br><code>SELECT STREAMTABLE(table1) table1.val,table2.val,table3.val from table1 JOIN table2 on (table1.key = table2.key1) JOIN table3 ON(table3.key = table2.key1)</code><br>此时，table2、table3表数据在Reduce将放在内存中。</p>
<p><strong><em>map端join：</em></strong><br>这里与下面“join出错”章节有关。<br>假如JOIN两张表，其中有一张表特别小(可以放在内存中),可以使用Map-side JOIN。Join计算时，将小表（驱动表）放在join的左边。<br>MapJoin是Hive的一种优化操作，其适用于小表JOIN大表的场景。表的JOIN操作会在Map端且在内存进行，所以其并不需要启动Reduce任务也就不需要经过shuffle阶段，从而能在一定程度上节省资源提高JOIN效率。<br>在Hive0.11后，Hive默认启动该优化。<br>通过以下两个属性来设置该优化的触发时机<br><code>hive.auto.convert.join</code> 默认值为true，自动开户MAPJOIN优化<br><code>hive.mapjoin.smalltable.filesize</code> 默认值为2500000(25M),通过配置该属性来确定使用该优化的表的大小，如果表的大小小于此值就会被加载进内存中（默认可自动优化，有时没有，可以用下面的语法指定要存起来的小表）。<br><code>hive.ignore.mapjoin.hint；</code>（默认值：true；是否忽略mapjoin hint 即mapjoin标记）<br>Map-Side JOIN是在Mapper中做JOIN,原理是将其中一张JOIN表放到每个Mapper任务的内存中，从而不用Reduce任务，在Mapper中就完成JOIN。<br>Map-SIde JOIN不适合FULL/RIGHT OUTER JOIN。<br>示例如下：<br><code>SELECT /*+MAPJOIN(b)*/ table1.key,table1.value from a join b on table1.key = table2.key;</code></p>
<p>参考：<a href="https://www.cnblogs.com/MOBIN/p/5702580.html" target="_blank" rel="noopener">https://www.cnblogs.com/MOBIN/p/5702580.html</a></p>
<p><strong>_join中处空值’’/null的语义区别_：</strong><br>‘’下hive关联操作的字段会被作为关联条件,这样会产生很多垃圾数据,在ETL中数据做了预处理后,<br>建议查询条件增加非空串判断: <code>from stu a join tea b on a.name = b.name and a.name !=&#39;&#39; and b.name != &#39;&#39; ;</code><br>NULL下hive关联操作的字段不会作为关联条件,即使是<code>null=null</code>的结果也是false.<br>实践中，这一语义区别也是经常导致数据倾斜的原因之一.(mysql操作结果一样)</p>
<p>作者：Bloo_m<br>转载自：<a href="https://www.jianshu.com/p/ae9b952abf6e" target="_blank" rel="noopener">https://www.jianshu.com/p/ae9b952abf6e</a> （原理值得仔细看，讲的很赞，但是有几处错误）</p>
<h3 id="配置hive"><a href="#配置hive" class="headerlink" title="配置hive"></a>配置hive</h3><p>编辑文件 <code>/etc/profile</code> 增加之后可以通过hive命令访问hive，hadoop相同：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#hive</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/usr/<span class="built_in">local</span>/hive</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br></pre></td></tr></table></figure>

<h3 id="hive-beeline常用参数"><a href="#hive-beeline常用参数" class="headerlink" title="hive beeline常用参数"></a>hive beeline常用参数</h3><p><code>myhive --silent=true --outputformat=csv2 --showHeader=false -e &quot;use database;&quot;</code><br><code>--outputformat=csv2</code> 消除多余的横线 <code>--outputformat=tsv2</code><br><code>--silent=true</code> 静默模式，不输出多余执行过程<br><code>--showHeader=false</code> 不输出表头<br>参考：<a href="https://www.cnblogs.com/30go/p/8706850.html" target="_blank" rel="noopener">https://www.cnblogs.com/30go/p/8706850.html</a></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>参考：<a href="http://www.cnblogs.com/smartloli/p/4288493.html" target="_blank" rel="noopener">http://www.cnblogs.com/smartloli/p/4288493.html</a><br><a href="https://www.jianshu.com/p/bd7820161a49?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation" target="_blank" rel="noopener">https://www.jianshu.com/p/bd7820161a49?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation</a><br>更多hive看：<a href="https://www.iteblog.com/" target="_blank" rel="noopener">https://www.iteblog.com/</a><br>hive 安装：<a href="https://www.jianshu.com/p/6108e0aed204" target="_blank" rel="noopener">https://www.jianshu.com/p/6108e0aed204</a><br>hive字符串：<a href="https://www.iteblog.com/archives/1639.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/1639.html</a><br>hadoop常用命令：<a href="https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#test" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#test</a><br>hive 常用总结（写的很好）：<a href="https://www.cnblogs.com/jiangzhengjun/p/6349226.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangzhengjun/p/6349226.html</a><br>mp调优：<a href="https://www.cnblogs.com/sunxucool/p/4459006.html" target="_blank" rel="noopener">https://www.cnblogs.com/sunxucool/p/4459006.html</a><br>函数（时间、字符串、数值）：<a href="https://blog.csdn.net/duan19056/article/details/17758819" target="_blank" rel="noopener">https://blog.csdn.net/duan19056/article/details/17758819</a><br>hive函数（时间、字符串、数值）:<a href="https://blog.csdn.net/yyywyr/article/details/51475410" target="_blank" rel="noopener">https://blog.csdn.net/yyywyr/article/details/51475410</a><br><a href="https://segmentfault.com/a/1190000011889191" target="_blank" rel="noopener">https://segmentfault.com/a/1190000011889191</a><br>（面试题）<a href="https://blog.csdn.net/best_luxi/article/details/82454770" target="_blank" rel="noopener">https://blog.csdn.net/best_luxi/article/details/82454770</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>hive-set设置总结</title>
    <url>/2018/10/19/hive-set%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<p>直接<code>set</code>命令可以看到所有变量值。<br><code>set</code>单个参数，可以看见这个参数的值。</p>
<h3 id="常用hiveconf"><a href="#常用hiveconf" class="headerlink" title="常用hiveconf"></a>常用hiveconf</h3><p>Hive相关的配置属性总结<br><code>set hive.cli.print.current.db=true;</code> 在cli hive提示符后显示当前数据库。<br><code>set hive.cli.print.header=true;</code> 显示表头。select时会显示对应字段。<br><code>set hive.mapred.mode=strict;</code> 防止笛卡儿积的执行;如果对分区表查询，且没有在where中对分区字段进行限制，报错<code>FAILED: SemanticException [Error 10041]: No partition predicate found for Alias &quot;test_part&quot; Table &quot;test_part&quot;</code>；对应还有<code>nonstrict</code>模式（默认模式）。<br><code>hive.support.sql11.reserved.keywords</code>该选项的目的是：是否启用对SQL2011保留关键字的支持。 启用后，将支持部分SQL2011保留关键字。</p>
<h3 id="设置作业优先级"><a href="#设置作业优先级" class="headerlink" title="设置作业优先级"></a>设置作业优先级</h3><p><code>mapred.job.priority=VERY_HIGH | HIGH | NORMAL | LOW | VERY_LOW</code></p>
<h3 id="手动指定队列"><a href="#手动指定队列" class="headerlink" title="手动指定队列"></a>手动指定队列</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> mapreduce.job.queuename=hive;</span><br></pre></td></tr></table></figure>
<h3 id="手动指定job-name"><a href="#手动指定job-name" class="headerlink" title="手动指定job name"></a>手动指定job name</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> mapreduce.job.name=uid_uname;</span><br></pre></td></tr></table></figure>
<h3 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"><span class="built_in">set</span> hive.exec.max.dynamic.partitions=100;</span><br><span class="line"><span class="built_in">set</span> hive.exec.max.dynamic.partitions.pernode=100;</span><br></pre></td></tr></table></figure>

<p><code>hive.exec.dynamic.partition</code> 开启动态分区<br><code>hive.exec.dynamic.partition.mode</code> 设置可以动态分区；因为严格模式下，不允许所有的分区都被动态指定。（详细使用看上面“导出数据到表”章节）<br><code>hive.exec.max.dynamic.partitions</code> 默认是1000；在所有执行的MR节点上，一共可以创建最大动态分区数<br><code>hive.exec.max.dynamic.partitions.pernode</code>  (上面参数也要加上)默认是100；在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</p>
<p>动态分区参考：<a href="http://lxw1234.com/archives/2015/06/286.htm" target="_blank" rel="noopener">http://lxw1234.com/archives/2015/06/286.htm</a></p>
<h3 id="并发优化"><a href="#并发优化" class="headerlink" title="并发优化"></a>并发优化</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.exec.parallel=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> hive.exec.parallel.thread.number=8;</span><br></pre></td></tr></table></figure>
<p>第一个参数：开启任务并行执行；<br>第二个参数：同一个sql允许并行任务的最大线程数</p>
<p>job之间没有前后依赖的都可以并行执行。</p>
<h3 id="join-group-by倾斜优化"><a href="#join-group-by倾斜优化" class="headerlink" title="join/group by倾斜优化"></a>join/group by倾斜优化</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.map.aggr=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> hive.groupby.skewindata=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> hive.groupby.mapaggr.checkinterval=100000;</span><br><span class="line"><span class="built_in">set</span> hive.optimize.skewjoin=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> hive.skewjoin.key=100000;</span><br></pre></td></tr></table></figure>
<p>参数解释：<a href="/2018/10/19/hive数据倾斜/">hive数据倾斜</a></p>
<h3 id="解决小文件问题"><a href="#解决小文件问题" class="headerlink" title="解决小文件问题"></a>解决小文件问题</h3><p>详细<a href="/2018/10/19/hive小文件合并/">hive小文件合并</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.merge.mapfiles=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> hive.merge.mapredfiles=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> hive.merge.size.per.task=256*1000*1000;</span><br><span class="line"><span class="built_in">set</span> hive.merge.smallfiles.avgsize=16000000;</span><br></pre></td></tr></table></figure>
<p>上面参数在 文件输出时合并。但是它们 和 压缩 并存时会失效，并对<code>orc</code>格式的表（orc本身就已经压缩）不起作用。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.hadoop.supports.splittable.combineinputformat=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br><span class="line"><span class="built_in">set</span> mapred.max.split.size=2048000000;</span><br><span class="line"><span class="built_in">set</span> mapred.min.split.size.per.node=2048000000;</span><br><span class="line"><span class="built_in">set</span> mapred.min.split.size.per.rack=2048000000;</span><br></pre></td></tr></table></figure>
<p>上面参数在 文件进入时合并文件，减少map个数。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=5120000000;</span><br><span class="line">insert overwrite table <span class="built_in">test</span> partition(dt)</span><br><span class="line">select * from iteblog_tmp</span><br><span class="line">DISTRIBUTE BY rand();</span><br></pre></td></tr></table></figure>
<p><code>DISTRIBUTE BY rand()</code> 强制产生reduce，<code>set hive.exec.reducers.bytes.per.reducer</code>控制reduce个数（reduce处理数据数量），两者一起使用控制小文件输出。</p>
<h3 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.map.memory.mb=<span class="number">2048</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.child.map.java.opts=<span class="string">'-Xmx2048M'</span>;</span><br><span class="line"><span class="keyword">set</span> mapreduce.map.java.opts=<span class="string">'-Xmx2048M'</span>;  </span><br><span class="line"><span class="keyword">set</span> mapreduce.reduce.memory.mb=<span class="number">2048</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.child.reduce.java.opts=<span class="string">'-Xmx2048m'</span>;</span><br><span class="line"><span class="keyword">set</span> mapreduce.reduce.java.opts=<span class="string">'-Xmx2048M'</span>;</span><br><span class="line"><span class="keyword">set</span> yarn.app.mapreduce.am.resource.mb=<span class="number">3000</span>;</span><br><span class="line"><span class="keyword">set</span> yarn.app.mapreduce.am.command-opts=<span class="string">'-Xmx2048m'</span>;</span><br></pre></td></tr></table></figure>
<p><code>set mapreduce.map.memory.mb</code>  container的内存 运行mapper的容器的物理内存，1024M = 1G<br><code>set mapreduce.map.java.opts</code>  jvm堆内存<br><code>set yarn.app.mapreduce.am.resource.mb</code> app内存。am指 Yarn中AppMaster，针对MapReduce计算框架就是MR AppMaster，通过配置这两个选项，可以设定MR AppMaster使用的内存。  一般看hadoop日志时可以看到map/reduce，但是当没有map/reduce时就开始报<code>beyond memory limit</code>类似的错时，说明是am的内存不够。<br>在yarn container这种模式下，map/reduce task是运行在Container之中的，所以上面提到的mapreduce.map(reduce).memory.mb大小<strong><em>都大于</em></strong>mapreduce.map(reduce).java.opts值的大小。mapreduce.{map|reduce}.java.opts能够通过Xmx设置JVM最大的heap的使用，<strong><em>一般设置为0.75倍的memory.mb，因为需要为java code等预留些空间</em></strong>。</p>
<p>来源于网络：虚拟内存的计算由 物理内存 和 yarn-site.xml中的yarn.nodemanager.vmem-pmem-ratio制定。<br><code>yarn.nodemanager.vmem-pmem-ratio</code>是 一个比例，默认是2.1   虚拟内存 = 物理内存 × 这个比例<br>yarn.nodemanager.vmem-pmem-ratio 的比率，默认是2.1.这个比率的控制影响着虚拟内存的使用，当yarn计算出来的虚拟内存，比在mapred-site.xml里的mapreduce.map.memory.mb或mapreduce.reduce.memory.mb的2.1倍还要多时，会被kill掉。</p>
<p>参考：<a href="https://blog.csdn.net/yisun123456/article/details/81327372" target="_blank" rel="noopener">https://blog.csdn.net/yisun123456/article/details/81327372</a></p>
<h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line"><span class="built_in">set</span> mapred.output.compression.type=BLOCK;</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> hive.exec.compress.intermediate=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>
<p>前三个参数是输出压缩；<br>最后两个参数是map输出压缩。<br>详细<a href="/2018/10/19/hive压缩/">hive压缩</a></p>
<h3 id="reducer个数相关"><a href="#reducer个数相关" class="headerlink" title="reducer个数相关"></a>reducer个数相关</h3><p><code>set mapreduce.job.reduces=15;</code> 指定reducer个数<br><code>set hive.exec.reducers.bytes.per.reducer</code> 每个reduce任务处理的数据量，默认为1000^3=1G</p>
<h3 id="推测式执行配置项"><a href="#推测式执行配置项" class="headerlink" title="推测式执行配置项"></a>推测式执行配置项</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> mapred.map.tasks.speculative.execution=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> mapred.reduce.tasks.speculative.execution=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>这是两个推测式执行的配置项,默认是true<br>所谓的推测执行，就是当所有task都开始运行之后，Job Tracker会统计所有任务的平均进度，如果某个task所在的task node机器配<br>置比较低或者CPU load很高（原因很多），导致任务执行比总体任务的平均执行要慢，此时Job Tracker会启动一个新的任务<br>（duplicate task），原有任务和新任务哪个先执行完就把另外一个kill掉，这也是我们经常在Job Tracker页面看到任务执行成功，但<br>是总有些任务被kill，就是这个原因。</p>
<h3 id="关闭mapjoin"><a href="#关闭mapjoin" class="headerlink" title="关闭mapjoin"></a>关闭mapjoin</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.auto.convert.join=<span class="literal">false</span>;</span><br><span class="line"><span class="built_in">set</span> hive.ignore.mapjoin.hint=<span class="literal">false</span>;</span><br></pre></td></tr></table></figure>

<h3 id="mapreduce-task-io-sort-mb"><a href="#mapreduce-task-io-sort-mb" class="headerlink" title="mapreduce.task.io.sort.mb"></a>mapreduce.task.io.sort.mb</h3><p><code>mapreduce.task.io.sort.mb</code> map shuffle时的内存  溢出</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blog.csdn.net/yycdaizi/article/details/43341239" target="_blank" rel="noopener">https://blog.csdn.net/yycdaizi/article/details/43341239</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>hive小文件问题</title>
    <url>/2018/10/19/hive%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6/</url>
    <content><![CDATA[<p>有多少个reducer（mapper）输出就会生成多少个输出文件，根据shuffle/sort的原理，每个文件按照某个值进行shuffle后的结果。</p>
<h3 id="小文件带来的问题"><a href="#小文件带来的问题" class="headerlink" title="小文件带来的问题"></a>小文件带来的问题</h3><p>HDFS的文件元信息，包括位置、大小、分块信息等，都是保存在NameNode的内存中的。每个对象大约占用150个字节，因此一千万个文件及分块就会占用约3G的内存空间，一旦接近这个量级，NameNode的性能就会开始下降了。</p>
<p>此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，job作为一个独立的jvm实例，每个job只处理很少的数据，其开启和停止的开销可能会大大超过实际的任务处理时间，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决</p>
<h3 id="分析解决方案"><a href="#分析解决方案" class="headerlink" title="分析解决方案"></a>分析解决方案</h3><p>解决小文件的问题可以从两个方向入手：</p>
<ol>
<li>输入合并。即在Map前合并小文件。这个方法即可以解决之前小文件数太多，导致mapper数太多的问题；还可以防止输出小文件合数太多的问题（因为mr只有map时，mapper数就是输出的文件个数）。</li>
<li>输出合并。即在输出结果的时候合并小文件。</li>
</ol>
<p><strong><em>对于输出结果为压缩文件形式存储的情况，如果使用输出合并，则必须配合SequenceFile来存储，否则无法进行合并。</em></strong></p>
<h3 id="输入文件合并"><a href="#输入文件合并" class="headerlink" title="输入文件合并"></a>输入文件合并</h3><p>文件合并失效，且job只有map时，map的个数就是文件个数；通过控制map大小控制map个数，以控制输出文件个数。<br><code>set hive.hadoop.supports.splittable.combineinputformat=true;</code> 开关<br><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</code> 执行Map前进行小文件合并<br><code>set mapred.max.split.size=2048000000;</code> 2G 每个Map最大输入大小<br><code>set mapred.min.split.size.per.node=2048000000;</code> 一个节点上split的至少的大小 ，决定了多个data node上的文件是否需要合并<br><code>set mapred.min.split.size.per.rack=2048000000;</code> 一个交换机下split的至少的大小，决定了多个交换机上的文件是否需要合并<br>MR-Job 默认的输入格式 FileInputFormat 为每一个小文件生成一个切片。<br>CombineFileInputFormat 通过将多个“小文件”合并为一个”切片”（在形成切片的过程中也考虑同一节点、同一机架的数据本地性），让每一个 Mapper 任务可以处理更多的数据，从而提高 MR 任务的执行速度。</p>
<p>原理（未细看）：<a href="https://www.cnblogs.com/skyl/p/4754999.html" target="_blank" rel="noopener">https://www.cnblogs.com/skyl/p/4754999.html</a></p>
<h3 id="输出文件合并"><a href="#输出文件合并" class="headerlink" title="输出文件合并"></a>输出文件合并</h3><p><strong><em>文件合并 和 压缩 并存时会失效。而且文件合并对<code>orc</code>格式的表（orc本身就已经压缩）不起作用。</em></strong><br><code>set hive.merge.mapfiles = true</code>#在Map-only的任务结束时合并小文件<br><code>set hive.merge.mapredfiles = true</code> #在Map-Reduce的任务结束时合并小文件<br><code>set hive.merge.size.per.task = 256*1000*1000</code> #合并后每个文件的大小，默认256000000<br><code>set hive.merge.smallfiles.avgsize=16000000</code>#平均文件大小，是决定是否执行合并操作的阈值，默认16000000<br>触发合并的条件：<br>根据查询类型不同，相应的mapfiles/mapredfiles参数必须需要打开，即前两个参数根据场景必须有一个为true；<br>结果文件的平均大小需要小于avgsize参数的值。</p>
<p>合并过程：结果文件进行合并时会执行一个额外的map-only脚本，mapper的数量是文件总大小除以size.per.task参数所得的值。</p>
<h3 id="解决Hive创建文件数过多的其他方法"><a href="#解决Hive创建文件数过多的其他方法" class="headerlink" title="解决Hive创建文件数过多的其他方法"></a>解决Hive创建文件数过多的其他方法</h3><p><code>set hive.exec.reducers.bytes.per.reducer=5120000000;</code> + <code>DISTRIBUTE BY rand();</code><br><code>DISTRIBUTE BY rand()</code> 强制产生reduce，<code>set hive.exec.reducers.bytes.per.reducer</code>控制reduce个数（reduce处理数据数量），两者一起使用控制小文件输出。<br>动态分区好用，但是会产生很多小文件。原因就在于，假设初始有N个mapper,最后生成了m个分区，最终会有多少个文件生成呢？答案是N*m,是的，每一个mapper会生成m个文件，就是每个分区都会对应一个文件，这样的话你算一下。所以小文件就会成倍的产生。<br>怎么解决这个问题，通常处理方式也是像上面那样，让数据尽量聚到少量reducer里面。但是有时候虽然动态分区不会产生reducer,但是也就意味着最后没有进行文件合并,我们也可以用distribute by rand()这句来保证数据聚类到相同的reducer。</p>
<p><a href="https://www.iteblog.com/archives/1533.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/1533.html</a></p>
<h3 id="参看"><a href="#参看" class="headerlink" title="参看"></a>参看</h3><p><a href="https://blog.csdn.net/yycdaizi/article/details/43341239" target="_blank" rel="noopener">https://blog.csdn.net/yycdaizi/article/details/43341239</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>hive数据倾斜</title>
    <url>/2018/10/19/hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</url>
    <content><![CDATA[<p>分析导致数据倾斜的数据：<br><a href="https://blog.csdn.net/bitcarmanlee/article/details/51694101" target="_blank" rel="noopener">https://blog.csdn.net/bitcarmanlee/article/details/51694101</a><br><a href="https://blog.csdn.net/wisgood/article/details/77063606" target="_blank" rel="noopener">https://blog.csdn.net/wisgood/article/details/77063606</a></p>
<h1 id="group-by数据倾斜"><a href="#group-by数据倾斜" class="headerlink" title="group by数据倾斜"></a>group by数据倾斜</h1><p>倾斜原因：<br><code>select count(distinct name) from user</code>时 使用distinct会将所有的name值都shuffle到一个reducer里面。<br>特别的有<code>select uid, count(distinct name) from user group by uid;</code> 即count distinct + （group by）的情况。</p>
<p>优化1-优化sql：<br>（1）主要是把count distinct改变成group by。<br>改变上面的sql为<code>select uid, count(name) from (select uid, name from user group by uid, name)t group by uid</code>.<br>（2）给group by 字段加随机数打散，聚合，之后把随机数去掉，再次聚合（有点类似下面的参数<code>SET hive.groupby.skewindata=true;</code>）：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">split</span>(uid, <span class="string">'_'</span>)[<span class="number">0</span>] uid, <span class="keyword">sum</span>(<span class="keyword">names</span>) <span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">  <span class="keyword">select</span> <span class="keyword">concat_ws</span>(<span class="string">'_'</span>, uid, <span class="keyword">substr</span>(<span class="keyword">rand</span>()*<span class="number">10</span>, <span class="number">1</span>, <span class="number">1</span>)) uid, <span class="keyword">count</span>(<span class="keyword">name</span>) <span class="keyword">names</span></span><br><span class="line">  <span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span> uid, <span class="keyword">name</span></span><br><span class="line">    <span class="keyword">from</span> <span class="keyword">user</span></span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> uid, <span class="keyword">name</span></span><br><span class="line">  )a</span><br><span class="line">  <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">concat_ws</span>(<span class="string">'_'</span>, uid, <span class="keyword">substr</span>(<span class="keyword">rand</span>()*<span class="number">10</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)b</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">split</span>(uid, <span class="string">'_'</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>如果一条语句里看总记录条数以及去重之后的记录条数，没有办法过滤，有两个选择，要么使用两个sql语句分别跑，然后<code>full join</code>。</p>
<p>优化2-加参数：<br><code>SET hive.groupby.skewindata=true;</code> 当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作.<br><del>从上面group by语句可以看出，这个变量是用于控制负载均衡的。当数据出现倾斜时，如果该变量设置为true，那么Hive会自动进行负载均衡</del><br><del>比如A日志表与B码表join，但是A中的关联字段id仅是B中id的一小部分，这时候很容易出现reduce阶段倾斜，大量的reduce空跑，因为这些空跑的reduce分到的B的id在A中不存在。</del><br><code>set hive.map.aggr=true;</code> 在mapper端部分聚合，相当于Combiner 。Map-Side聚合（一般在聚合函数sum,count时使用）。<br><code>set hive.groupby.mapaggr.checkinterval=100000；</code>–这个是group的键对应的记录条数超过这个值则会进行分拆,值根据具体数据量设置。<br><code>hive.map.aggr.hash.min.reduction=0.5(默认)</code>预先取100000条数据聚合,如果聚合后的条数/100000&gt;0.5，则不再聚合</p>
<h1 id="join-数据倾斜"><a href="#join-数据倾斜" class="headerlink" title="join 数据倾斜"></a>join 数据倾斜</h1><p>1、null或者某个无效字符太多导致数据倾斜。<br>   <code>null=null</code>结果是null，即false，在join时关联不上，join之前去掉不影响结果；<br>   ‘’关联得上，但是不需要时产生不必要的脏数据，可以在join之前把key为null/‘’的值去掉。<br>   因为null关联不上如果null有用不能去掉，可以用下面两种方法。<br>   办法（1）用union all<br>   <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Select * From <span class="built_in">log</span> a</span><br><span class="line">　　　Join users b</span><br><span class="line">  On a.user_id is not null And a.user_id = b.user_id</span><br><span class="line">　 Union all</span><br><span class="line">Select * from <span class="built_in">log</span> a　<span class="built_in">where</span> a.user_id is null;</span><br></pre></td></tr></table></figure><br>   办法（2）赋予null值新的随机值<br>   <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Select *</span><br><span class="line">  from <span class="built_in">log</span> a</span><br><span class="line">　　 left Join</span><br><span class="line">   bmw_users b</span><br><span class="line"> on <span class="keyword">case</span> when a.user_id is null <span class="keyword">then</span> concat(<span class="string">'dp_hive'</span>,rand())</span><br><span class="line">　　   <span class="keyword">else</span> a.user_id end = b.user_id;</span><br></pre></td></tr></table></figure><br>   方法2比1好，效率上。处理某个值的数据倾斜时都可以尝试方法2。<br>2、Map输出key数量极少，导致reduce端退化为单机作业。<br>   <strong><em>尽量尽早地过滤数据，减少每个阶段的数据量;把where、group by等操作放在join之前。</em></strong><br>   先对join的表去重，即把<code>group by</code>操作放在join之前，减少join的笛卡尔积大小。<br>   或者其中的小表做均匀打散，另一个表加随机数。<br>3、Map输出key分布不均，少量key对应大量value，导致reduce端单机瓶颈。<br>   下面的参考链接用的是切片取样的方法。<br>   我一般使用上面2中的办法，尽早去重。<br>   或者采用<code>group by</code>时的办法，加随机数把数据打散。<br>4、不同数据类型关联也会产生数据倾斜。<br>   在on时强转<code>On a.auction_id = cast(b.auction_id as string);</code><br>5、<code>set hive.optimize.skewjoin=true;</code><br>6、<code>set hive.skewjoin.key=100000;</code>join的键对应的记录条数超过这个值则会进行分拆,值根据具体数据量设置； hive 在运行的时候没有办法判断哪个 key 会产生多大的倾斜，所以使用这个参数控制倾斜的阈值，如果超过这个值，新的值会发送给那些还没有达到的 reduce。对<code>full outer join</code>无效。<br>如果你不知道设置多少，可以就按官方默认的1个reduce 只处理1G 的算法，那么  skew_key_threshold  = 1G/平均行长. 或者默认直接设成250000000 (差不多算平均行长4个字节)</p>
<p>reduce倾斜参考：<a href="https://www.cnblogs.com/skyl/p/4855099.html" target="_blank" rel="noopener">https://www.cnblogs.com/skyl/p/4855099.html</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-StreamingFileSink自定义Watermark+DateTimeBucket实现精准落仓</title>
    <url>/2020/09/11/Flink-StreamingFileSink%E8%87%AA%E5%AE%9A%E4%B9%89DateTimeBucket/</url>
    <content><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>flink自带的DateTimeBucket按照process time执行时间来落分区目录。<br>但有很多缺点：<br>1、数据延迟很常见 2、或者任务出问题暂停后再启动如果恰好跨分区，数据出问题的会更多。<br>因此很多时候我们希望的是 event time，本文的办法是<br>flink自带的<code>DateTimeBucketAssigner</code>定义了数据保存的hdfs时间路径,选用的是<code>currentProcessingTime</code>，重要的代码在方法中<code>getBucketId</code>。<br>因此，本文的重点是重新定义<code>getBucketId</code>方法。</p>
<p>另外，我们还要生成done（又称succed）文件来通知下游任务，上游数据已经好了。</p>
<h3 id="如何生成done"><a href="#如何生成done" class="headerlink" title="如何生成done"></a>如何生成done</h3><p>判断当前目录下是否还有progress文件</p>
<h3 id="直接使用event-time"><a href="#直接使用event-time" class="headerlink" title="直接使用event time"></a>直接使用event time</h3><p>但是日志中的脏数据本来就很多，如果单纯的使用event time，前面的分区将永远无法完成。<br>因此在代码里用了<code>BehaviorHistoryMergeUtils.minutesLater(0L, -60)</code>判断小于当前时间60的时间不再进入正确的分区，落入错误分区。<br>但是实时落仓本来的优势就是快，现在要等60min，不符合原意。如果改的更小如5min还是会有如果代码暂停了超过5分钟，还是会有很多问题。</p>
<p>Tips: StreamingFileSink的实例1.10.0有bug可以选择用java写，up版本已修复。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line"> * Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line"> * or more contributor license agreements.  See the NOTICE file</span><br><span class="line"> * distributed with this work for additional information</span><br><span class="line"> * regarding copyright ownership.  The ASF licenses this file</span><br><span class="line"> * to you under the Apache License, Version 2.0 (the</span><br><span class="line"> * &quot;License&quot;); you may not use this file except in compliance</span><br><span class="line"> * with the License.  You may obtain a copy of the License at</span><br><span class="line"> *</span><br><span class="line"> * http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0</span><br><span class="line"> *</span><br><span class="line"> * Unless required by applicable law or agreed to in writing, software</span><br><span class="line"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"> * See the License for the specific language governing permissions and</span><br><span class="line"> * limitations under the License.</span><br><span class="line"> *&#x2F;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import org.apache.flink.annotation.PublicEvolving;</span><br><span class="line">import org.apache.flink.core.io.SimpleVersionedSerializer;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.filesystem.BucketAssigner;</span><br><span class="line">import org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.SimpleVersionedStringSerializer;</span><br><span class="line">import org.apache.flink.util.Preconditions;</span><br><span class="line"></span><br><span class="line">import java.time.Instant;</span><br><span class="line">import java.time.ZoneId;</span><br><span class="line">import java.time.format.DateTimeFormatter;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * A &#123;@link BucketAssigner&#125; that assigns to buckets based on current system time.</span><br><span class="line"> *</span><br><span class="line"> *</span><br><span class="line"> * &lt;p&gt;The &#123;@code DateTimeBucketer&#125; will create directories of the following form:</span><br><span class="line"> * &#123;@code &#x2F;&#123;basePath&#125;&#x2F;&#123;dateTimePath&#125;&#x2F;&#125;. The &#123;@code basePath&#125; is the path</span><br><span class="line"> * that was specified as a base path when creating the</span><br><span class="line"> * &#123;@link org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink&#125;.</span><br><span class="line"> * The &#123;@code dateTimePath&#125; is determined based on the current system time and the</span><br><span class="line"> * user provided format string.</span><br><span class="line"> *</span><br><span class="line"> *</span><br><span class="line"> * &lt;p&gt;&#123;@link DateTimeFormatter&#125; is used to derive a date string from the current system time and</span><br><span class="line"> * the date format string. The default format string is &#123;@code &quot;yyyy-MM-dd--HH&quot;&#125; so the rolling</span><br><span class="line"> * files will have a granularity of hours.</span><br><span class="line"> *</span><br><span class="line"> * &lt;p&gt;Example:</span><br><span class="line"> *</span><br><span class="line"> * &lt;pre&gt;&#123;@code</span><br><span class="line"> *     BucketAssigner bucketAssigner &#x3D; new DateTimeBucketAssigner(&quot;yyyy-MM-dd--HH&quot;);</span><br><span class="line"> * &#125;&lt;&#x2F;pre&gt;</span><br><span class="line"> *</span><br><span class="line"> * &lt;p&gt;This will create for example the following bucket path:</span><br><span class="line"> * &#123;@code &#x2F;base&#x2F;1976-12-31-14&#x2F;&#125;</span><br><span class="line"> *</span><br><span class="line"> *&#x2F;</span><br><span class="line">@PublicEvolving</span><br><span class="line">public class DateTimeBucketWithEventAssigner&lt;IN&gt; implements BucketAssigner&lt;IN, String&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line"></span><br><span class="line">    private static final String DEFAULT_FORMAT_STRING &#x3D; &quot;yyyy-MM-dd--HH&quot;;</span><br><span class="line"></span><br><span class="line">    private final String formatString;</span><br><span class="line"></span><br><span class="line">    private final ZoneId zoneId;</span><br><span class="line"></span><br><span class="line">    private transient DateTimeFormatter dateTimeFormatter;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * Creates a new &#123;@code DateTimeBucketAssigner&#125; with format string &#123;@code &quot;yyyy-MM-dd--HH&quot;&#125;.</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public DateTimeBucketWithEventAssigner() &#123;</span><br><span class="line">        this(DEFAULT_FORMAT_STRING);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * Creates a new &#123;@code DateTimeBucketAssigner&#125; with the given date&#x2F;time format string.</span><br><span class="line">     *</span><br><span class="line">     * @param formatString The format string that will be given to &#123;@code SimpleDateFormat&#125; to determine</span><br><span class="line">     *                     the bucket id.</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public DateTimeBucketWithEventAssigner(String formatString) &#123;</span><br><span class="line">        this(formatString, ZoneId.systemDefault());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * Creates a new &#123;@code DateTimeBucketAssigner&#125; with format string &#123;@code &quot;yyyy-MM-dd--HH&quot;&#125; using the given timezone.</span><br><span class="line">     *</span><br><span class="line">     * @param zoneId The timezone used to format &#123;@code DateTimeFormatter&#125; for bucket id.</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public DateTimeBucketWithEventAssigner(ZoneId zoneId) &#123;</span><br><span class="line">        this(DEFAULT_FORMAT_STRING, zoneId);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * Creates a new &#123;@code DateTimeBucketAssigner&#125; with the given date&#x2F;time format string using the given timezone.</span><br><span class="line">     *</span><br><span class="line">     * @param formatString The format string that will be given to &#123;@code DateTimeFormatter&#125; to determine</span><br><span class="line">     *                     the bucket path.</span><br><span class="line">     * @param zoneId The timezone used to format &#123;@code DateTimeFormatter&#125; for bucket id.</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public DateTimeBucketWithEventAssigner(String formatString, ZoneId zoneId) &#123;</span><br><span class="line">        this.formatString &#x3D; Preconditions.checkNotNull(formatString);</span><br><span class="line">        this.zoneId &#x3D; Preconditions.checkNotNull(zoneId);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String getBucketId(IN element, BucketAssigner.Context context) &#123;</span><br><span class="line">        if (dateTimeFormatter &#x3D;&#x3D; null) &#123;</span><br><span class="line">            dateTimeFormatter &#x3D; DateTimeFormatter.ofPattern(formatString).withZone(zoneId);</span><br><span class="line">        &#125;</span><br><span class="line">        if (element instanceof SearchEtl) &#123;</span><br><span class="line">            long minutesAgo &#x3D; BehaviorHistoryMergeUtils.minutesLater(0L, -60);</span><br><span class="line">            Long pingbackTime &#x3D; ((SearchEtl) element).getStime();</span><br><span class="line">            if (pingbackTime &lt; minutesAgo)&#123;</span><br><span class="line">                return &quot;error_time&quot;;</span><br><span class="line">            &#125;</span><br><span class="line">            return dateTimeFormatter.format(Instant.ofEpochMilli(pingbackTime));</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            return &quot;error_time&quot;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public SimpleVersionedSerializer&lt;String&gt; getSerializer() &#123;</span><br><span class="line">        return SimpleVersionedStringSerializer.INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return &quot;DateTimeBucketAssigner&#123;&quot; +</span><br><span class="line">                &quot;formatString&#x3D;&#39;&quot; + formatString + &#39;\&#39;&#39; +</span><br><span class="line">                &quot;, zoneId&#x3D;&quot; + zoneId +</span><br><span class="line">                &#39;&#125;&#39;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其他：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def minutesLater(nowDate: Long, plus: Int): Long &#x3D; &#123;</span><br><span class="line">    if (nowDate &lt;&#x3D; 0) &#123;</span><br><span class="line">      new DateTime().plusMinutes(plus).getMillis</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      new DateTime(nowDate).plusMinutes(plus).getMillis</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>使用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">StreamingFileSink build &#x3D; StreamingFileSink</span><br><span class="line">        .forBulkFormat(new Path(hdfs), ParquetAvroWriters.forSpecificRecord(SearchEtl.class))</span><br><span class="line">        .withBucketAssigner(new DateTimeBucketWithEventAssigner())</span><br><span class="line">        .withRollingPolicy(OnCheckpointRollingPolicy.build())</span><br><span class="line">        .build();</span><br></pre></td></tr></table></figure>
<h3 id="改进-使用watermark"><a href="#改进-使用watermark" class="headerlink" title="改进 使用watermark"></a>改进 使用watermark</h3><p>在sink之前定义watermark。<br>不同的日志不同的定义方式，如果日志里还会有很多超过当前时间的日志，就可以参考这里的办法。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source.map(...).uid(...)</span><br><span class="line">.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[SearchEtl](Time.minutes(3)) &#123;</span><br><span class="line">  override def extractTimestamp(element: SearchEtl): Long &#x3D; &#123;</span><br><span class="line">    math.min(element.getStime, System.currentTimeMillis())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.addSink(FlinkConfig.getParquetHdfsSink(hdfs)).uid(&quot;sink&quot;)</span><br></pre></td></tr></table></figure>
<p><code>getBucketId</code>方法的定义。watermark使用的时候需要注意，任务刚刚启动的时候watermark可能是个默认值。<br>watermark的生成是每200ms，而且不是任务启动就会调用<code>extractTimestamp</code>再调用<code>getCurrentWatermark</code>.具体可以看另一个讲解watermark的文章。<br>（pingbackTime &lt; watermark &amp;&amp; pingbackTime &lt; 前一小时的零点）||（pingbackTime &gt; 下一小时的零点）时进入错误分区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">public String getBucketId(IN element, BucketAssigner.Context context) &#123;</span><br><span class="line">   if (dateTimeFormatter &#x3D;&#x3D; null) &#123;</span><br><span class="line">       dateTimeFormatter &#x3D; DateTimeFormatter.ofPattern(formatString).withZone(zoneId);</span><br><span class="line">   &#125;</span><br><span class="line">   if (element instanceof SearchEtl) &#123;</span><br><span class="line">       long watermark &#x3D; context.currentWatermark();</span><br><span class="line">       if (watermark &lt; 0)&#123;</span><br><span class="line">           watermark &#x3D; context.currentProcessingTime();</span><br><span class="line">       &#125;</span><br><span class="line">       Long pingbackTime &#x3D; ((SearchEtl) element).getStime();</span><br><span class="line">       if ((pingbackTime &lt; watermark &amp;&amp; pingbackTime &lt; BehaviorHistoryMergeUtils.getCurrentHourZeroTime())</span><br><span class="line">               || pingbackTime &gt; BehaviorHistoryMergeUtils.getNextHourZeroTime()) &#123;</span><br><span class="line">           return dateTimeFormatter.format(Instant.ofEpochMilli(context.currentProcessingTime())) + &quot;error_time&quot;;</span><br><span class="line">       &#125;</span><br><span class="line">       return dateTimeFormatter.format(Instant.ofEpochMilli(pingbackTime));</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">       return &quot;error_time&quot;;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def getNextHourZeroTime: Long &#x3D; &#123;</span><br><span class="line">  getHourZeroTime(1)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def getCurrentHourZeroTime: Long &#x3D; &#123;</span><br><span class="line">  getHourZeroTime(0)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def getHourZeroTime(plus: Int): Long &#x3D; &#123;</span><br><span class="line">  val calendar &#x3D; Calendar.getInstance()</span><br><span class="line">  calendar.add(Calendar.HOUR, plus)</span><br><span class="line">  calendar.set(Calendar.MINUTE, 0)</span><br><span class="line">  calendar.set(Calendar.SECOND, 0)</span><br><span class="line">  calendar.set(Calendar.MILLISECOND, 0)</span><br><span class="line">  calendar.getTimeInMillis</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>1、当程序突然停止时，文件仍处于inprogress状态。<br>2、默认桶下的文件名是 part-{parallel-task}-{count}。当程序重启时，选用上次所有subtask最大的编号值count加1继续开始。<br>3、写入HDFS时，会产生大量的小文件。此问题比较轻微可简单解决。</p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink窗口函数ReduceFunction、AggregateFunction、ProcessWindowFunction</title>
    <url>/2020/09/09/Flink%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0ReduceFunction%E3%80%81AggregateFunction%E3%80%81ProcessWindowFunction/</url>
    <content><![CDATA[<p>全文转自：<a href="https://blog.csdn.net/wangpei1949/article/details/102986845" target="_blank" rel="noopener">https://blog.csdn.net/wangpei1949/article/details/102986845</a></p>
<p>Window Function在窗口触发后，负责对窗口内的元素进行计算。Window Function分为两类: 增量聚合和全量聚合。</p>
<p>增量聚合: 窗口不维护原始数据，只维护中间结果，每次基于中间结果和增量数据进行聚合。如: ReduceFunction、AggregateFunction。<br>全量聚合: 窗口需要维护全部原始数据，窗口触发进行全量聚合。如:ProcessWindowFunction。<br>本文总结增量聚合函数(ReduceFunction、AggregateFunction)和全量聚合函数(ProcessWindowFunction)的使用。</p>
<p>注意:<br>FoldFunction也是增量聚合函数，但在Flink 1.9.0中已被标为过时(可用AggregateFunction代替)，这里不做总结。<br>WindowFunction也是全量聚合函数，已被更高级的ProcessWindowFunction逐渐代替，这里也不做总结。<br>ReduceFunction输入输出元素类型相同。</p>
<h3 id="增量聚合"><a href="#增量聚合" class="headerlink" title="增量聚合"></a>增量聚合</h3><h4 id="ReduceFunction"><a href="#ReduceFunction" class="headerlink" title="ReduceFunction"></a>ReduceFunction</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 测试数据: 某个用户在某个时刻浏览了某个商品，以及商品的价值</span><br><span class="line">&#x2F;&#x2F; &#123;&quot;userID&quot;: &quot;user_4&quot;, &quot;eventTime&quot;: &quot;2019-11-09 10:41:32&quot;, &quot;eventType&quot;: &quot;browse&quot;, &quot;productID&quot;: &quot;product_1&quot;, &quot;productPrice&quot;: 10&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; API</span><br><span class="line">&#x2F;&#x2F; T: 输入输出元素类型</span><br><span class="line">public interface ReduceFunction&lt;T&gt; extends Function, Serializable &#123;</span><br><span class="line">	T reduce(T value1, T value2) throws Exception;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 示例: 获取一段时间内(Window Size)每个用户(KeyBy)浏览的商品的最大价值的那条记录(ReduceFunction)</span><br><span class="line">kafkaStream</span><br><span class="line">    &#x2F;&#x2F; 将从Kafka获取的JSON数据解析成Java Bean</span><br><span class="line">    .process(new KafkaProcessFunction())</span><br><span class="line">    &#x2F;&#x2F; 提取时间戳生成水印</span><br><span class="line">    .assignTimestampsAndWatermarks(new MyCustomBoundedOutOfOrdernessTimestampExtractor(Time.seconds(maxOutOfOrdernessSeconds)))</span><br><span class="line">    &#x2F;&#x2F; 按用户分组</span><br><span class="line">    .keyBy((KeySelector&lt;UserActionLog, String&gt;) UserActionLog::getUserID)</span><br><span class="line">    &#x2F;&#x2F; 构造TimeWindow</span><br><span class="line">    .timeWindow(Time.seconds(windowLengthSeconds))</span><br><span class="line">    &#x2F;&#x2F; 窗口函数: 获取这段窗口时间内每个用户浏览的商品的最大价值对应的那条记录</span><br><span class="line">    .reduce(new ReduceFunction&lt;UserActionLog&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public UserActionLog reduce(UserActionLog value1, UserActionLog value2) throws Exception &#123;</span><br><span class="line">            return value1.getProductPrice() &gt; value2.getProductPrice() ? value1 : value2;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .print();</span><br><span class="line"></span><br><span class="line"># 结果</span><br><span class="line">UserActionLog&#123;userID&#x3D;&#39;user_4&#39;, eventTime&#x3D;&#39;2019-11-09 12:51:25&#39;, eventType&#x3D;&#39;browse&#39;, productID&#x3D;&#39;product_3&#39;, productPrice&#x3D;30&#125;</span><br><span class="line">UserActionLog&#123;userID&#x3D;&#39;user_2&#39;, eventTime&#x3D;&#39;2019-11-09 12:51:29&#39;, eventType&#x3D;&#39;browse&#39;, productID&#x3D;&#39;product_2&#39;, productPrice&#x3D;20&#125;</span><br><span class="line">UserActionLog&#123;userID&#x3D;&#39;user_1&#39;, eventTime&#x3D;&#39;2019-11-09 12:51:22&#39;, eventType&#x3D;&#39;browse&#39;, productID&#x3D;&#39;product_3&#39;, productPrice&#x3D;30&#125;</span><br><span class="line">UserActionLog&#123;userID&#x3D;&#39;user_5&#39;, eventTime&#x3D;&#39;2019-11-09 12:51:21&#39;, eventType&#x3D;&#39;browse&#39;, productID&#x3D;&#39;product_3&#39;, productPrice&#x3D;30&#125;</span><br></pre></td></tr></table></figure>
<h4 id="AggregateFunction"><a href="#AggregateFunction" class="headerlink" title="AggregateFunction"></a>AggregateFunction</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 测试数据: 某个用户在某个时刻浏览了某个商品，以及商品的价值</span><br><span class="line">&#x2F;&#x2F; &#123;&quot;userID&quot;: &quot;user_4&quot;, &quot;eventTime&quot;: &quot;2019-11-09 10:41:32&quot;, &quot;eventType&quot;: &quot;browse&quot;, &quot;productID&quot;: &quot;product_1&quot;, &quot;productPrice&quot;: 10&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; API</span><br><span class="line">&#x2F;&#x2F; IN:  输入元素类型</span><br><span class="line">&#x2F;&#x2F; ACC: 累加器类型</span><br><span class="line">&#x2F;&#x2F; OUT: 输出元素类型</span><br><span class="line">public interface AggregateFunction&lt;IN, ACC, OUT&gt; extends Function, Serializable &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 初始化累加器</span><br><span class="line">	ACC createAccumulator();</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 累加</span><br><span class="line">	ACC add(IN value, ACC accumulator);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 累加器合并</span><br><span class="line">	ACC merge(ACC a, ACC b);</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; 输出</span><br><span class="line">	OUT getResult(ACC accumulator);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 示例: 获取一段时间内(Window Size)每个用户(KeyBy)浏览的平均价值(AggregateFunction)</span><br><span class="line">kafkaStream</span><br><span class="line">   &#x2F;&#x2F; 将从Kafka获取的JSON数据解析成Java Bean</span><br><span class="line">   .process(new KafkaProcessFunction())</span><br><span class="line">   &#x2F;&#x2F; 提取时间戳生成水印</span><br><span class="line">   .assignTimestampsAndWatermarks(new MyCustomBoundedOutOfOrdernessTimestampExtractor(Time.seconds(maxOutOfOrdernessSeconds)))</span><br><span class="line">   &#x2F;&#x2F; 按用户分组</span><br><span class="line">   .keyBy((KeySelector&lt;UserActionLog, String&gt;) UserActionLog::getUserID)</span><br><span class="line">   &#x2F;&#x2F; 构造TimeWindow</span><br><span class="line">   .timeWindow(Time.seconds(windowLengthSeconds))</span><br><span class="line">   &#x2F;&#x2F; 窗口函数: 获取这段窗口时间内，每个用户浏览的平均价值</span><br><span class="line">   .aggregate(new AggregateFunction&lt;UserActionLog, Tuple2&lt;Long,Long&gt;, Double&gt;() &#123;</span><br><span class="line"></span><br><span class="line">       &#x2F;&#x2F; 1、初始值</span><br><span class="line">       &#x2F;&#x2F; 定义累加器初始值</span><br><span class="line">       @Override</span><br><span class="line">       public Tuple2&lt;Long, Long&gt; createAccumulator() &#123;</span><br><span class="line">           return new Tuple2&lt;&gt;(0L,0L);</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       &#x2F;&#x2F; 2、累加</span><br><span class="line">       &#x2F;&#x2F; 定义累加器如何基于输入数据进行累加</span><br><span class="line">       @Override</span><br><span class="line">       public Tuple2&lt;Long, Long&gt; add(UserActionLog value, Tuple2&lt;Long, Long&gt; accumulator) &#123;</span><br><span class="line">           accumulator.f0 +&#x3D; 1;</span><br><span class="line">           accumulator.f1 +&#x3D; value.getProductPrice();</span><br><span class="line">           return accumulator;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       &#x2F;&#x2F; 3、合并</span><br><span class="line">       &#x2F;&#x2F; 定义累加器如何和State中的累加器进行合并</span><br><span class="line">       @Override</span><br><span class="line">       public Tuple2&lt;Long, Long&gt; merge(Tuple2&lt;Long, Long&gt; acc1, Tuple2&lt;Long, Long&gt; acc2) &#123;</span><br><span class="line">           acc1.f0+&#x3D;acc2.f0;</span><br><span class="line">           acc1.f1+&#x3D;acc2.f1;</span><br><span class="line">           return acc1;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       &#x2F;&#x2F; 4、输出</span><br><span class="line">       &#x2F;&#x2F; 定义如何输出数据</span><br><span class="line">       @Override</span><br><span class="line">       public Double getResult(Tuple2&lt;Long, Long&gt; accumulator) &#123;</span><br><span class="line">           return accumulator.f1 &#x2F; (accumulator.f0 * 1.0);</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">   &#125;)</span><br><span class="line">   .print();</span><br><span class="line"></span><br><span class="line">#结果</span><br><span class="line">20.0</span><br><span class="line">10.0</span><br><span class="line">30.0</span><br><span class="line">25.0</span><br><span class="line">20.0</span><br></pre></td></tr></table></figure>
<h4 id="全量聚合"><a href="#全量聚合" class="headerlink" title="全量聚合"></a>全量聚合</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 测试数据: 某个用户在某个时刻浏览了某个商品，以及商品的价值</span><br><span class="line">&#x2F;&#x2F; &#123;&quot;userID&quot;: &quot;user_4&quot;, &quot;eventTime&quot;: &quot;2019-11-09 10:41:32&quot;, &quot;eventType&quot;: &quot;browse&quot;, &quot;productID&quot;: &quot;product_1&quot;, &quot;productPrice&quot;: 10&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; API</span><br><span class="line">&#x2F;&#x2F; IN:  输入元素类型</span><br><span class="line">&#x2F;&#x2F; OUT: 输出元素类型</span><br><span class="line">&#x2F;&#x2F; KEY: Key类型</span><br><span class="line">&#x2F;&#x2F; W: Window类型</span><br><span class="line">public abstract class ProcessWindowFunction&lt;IN, OUT, KEY, W extends Window&gt; extends AbstractRichFunction &#123;</span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">	public abstract void process(KEY key, Context context, Iterable&lt;IN&gt; elements, Collector&lt;OUT&gt; out) throws Exception;</span><br><span class="line"></span><br><span class="line">    ........</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 示例: 获取一段时间内(Window Size)每个用户(KeyBy)浏览的商品总价值(ProcessWindowFunction)</span><br><span class="line">kafkaStream</span><br><span class="line">    &#x2F;&#x2F; 将从Kafka获取的JSON数据解析成Java Bean</span><br><span class="line">    .process(new KafkaProcessFunction())</span><br><span class="line">    &#x2F;&#x2F; 提取时间戳生成水印</span><br><span class="line">    .assignTimestampsAndWatermarks(new MyCustomBoundedOutOfOrdernessTimestampExtractor(Time.seconds(maxOutOfOrdernessSeconds)))</span><br><span class="line">    &#x2F;&#x2F; 按用户分组</span><br><span class="line">    .keyBy((KeySelector&lt;UserActionLog, String&gt;) UserActionLog::getUserID)</span><br><span class="line">    &#x2F;&#x2F; 构造TimeWindow</span><br><span class="line">    .timeWindow(Time.seconds(windowLengthSeconds))</span><br><span class="line">    &#x2F;&#x2F; 窗口函数: 用ProcessWindowFunction计算这段时间内每个用户浏览的商品总价值</span><br><span class="line">    .process(new ProcessWindowFunction&lt;UserActionLog, String, String, TimeWindow&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public void process(String key, Context context, Iterable&lt;UserActionLog&gt; elements, Collector&lt;String&gt; out) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">            int sum&#x3D;0;</span><br><span class="line">            for (UserActionLog element : elements) &#123;</span><br><span class="line">                sum +&#x3D; element.getProductPrice();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            String windowStart&#x3D;new DateTime(context.window().getStart(), DateTimeZone.forID(&quot;+08:00&quot;)).toString(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">            String windowEnd&#x3D;new DateTime(context.window().getEnd(), DateTimeZone.forID(&quot;+08:00&quot;)).toString(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line"></span><br><span class="line">            String record&#x3D;&quot;Key: &quot;+key+&quot; 窗口开始时间: &quot;+windowStart+&quot; 窗口结束时间: &quot;+windowEnd+&quot; 浏览的商品总价值: &quot;+sum;</span><br><span class="line">            out.collect(record);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .print();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 结果</span><br><span class="line">Key: user_1 窗口开始时间: 2019-11-09 13:32:00 窗口结束时间: 2019-11-09 13:32:10 浏览的商品总价值: 60</span><br><span class="line">Key: user_5 窗口开始时间: 2019-11-09 13:32:00 窗口结束时间: 2019-11-09 13:32:10 浏览的商品总价值: 30</span><br><span class="line">Key: user_5 窗口开始时间: 2019-11-09 13:32:10 窗口结束时间: 2019-11-09 13:32:20 浏览的商品总价值: 80</span><br><span class="line">Key: user_3 窗口开始时间: 2019-11-09 13:32:10 窗口结束时间: 2019-11-09 13:32:20 浏览的商品总价值: 40</span><br><span class="line">Key: user_4 窗口开始时间: 2019-11-09 13:32:10 窗口结束时间: 2019-11-09 13:32:20 浏览的商品总价值: 70</span><br></pre></td></tr></table></figure>
<h3 id="ProcessWindowFunction与增量聚合结合"><a href="#ProcessWindowFunction与增量聚合结合" class="headerlink" title="ProcessWindowFunction与增量聚合结合"></a>ProcessWindowFunction与增量聚合结合</h3><p>1、可将ProcessWindowFunction与增量聚合函数ReduceFunction、AggregateFunction结合。<br>2、元素到达窗口时增量聚合，当窗口关闭时对增量聚合的结果用ProcessWindowFunction再进行全量聚合。<br>3、既可以增量聚合，也可以访问窗口的元数据信息(如开始结束时间、状态等)。</p>
<h4 id="ProcessWindowFunction与ReduceFunction结合"><a href="#ProcessWindowFunction与ReduceFunction结合" class="headerlink" title="ProcessWindowFunction与ReduceFunction结合"></a>ProcessWindowFunction与ReduceFunction结合</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 测试数据: 某个用户在某个时刻浏览了某个商品，以及商品的价值</span><br><span class="line">&#x2F;&#x2F; &#123;&quot;userID&quot;: &quot;user_4&quot;, &quot;eventTime&quot;: &quot;2019-11-09 10:41:32&quot;, &quot;eventType&quot;: &quot;browse&quot;, &quot;productID&quot;: &quot;product_1&quot;, &quot;productPrice&quot;: 10&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; API: 如上ReduceFunction与ProcessWindowFunction</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 示例: 获取一段时间内(Window Size)每个用户(KeyBy)浏览的商品的最大价值的那条记录(ReduceFunction),并获得Key和Window信息。</span><br><span class="line">kafkaStream</span><br><span class="line">    &#x2F;&#x2F; 将从Kafka获取的JSON数据解析成Java Bean</span><br><span class="line">    .process(new KafkaProcessFunction())</span><br><span class="line">    &#x2F;&#x2F; 提取时间戳生成水印</span><br><span class="line">    .assignTimestampsAndWatermarks(new MyCustomBoundedOutOfOrdernessTimestampExtractor(Time.seconds(maxOutOfOrdernessSeconds)))</span><br><span class="line">    &#x2F;&#x2F; 按用户分组</span><br><span class="line">    .keyBy((KeySelector&lt;UserActionLog, String&gt;) UserActionLog::getUserID)</span><br><span class="line">    &#x2F;&#x2F; 构造TimeWindow</span><br><span class="line">    .timeWindow(Time.seconds(windowLengthSeconds))</span><br><span class="line">    &#x2F;&#x2F; 窗口函数: 获取这段窗口时间内每个用户浏览的商品的最大价值对应的那条记录</span><br><span class="line">    .reduce(</span><br><span class="line">            new ReduceFunction&lt;UserActionLog&gt;() &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public UserActionLog reduce(UserActionLog value1, UserActionLog value2) throws Exception &#123;</span><br><span class="line">                    return value1.getProductPrice() &gt; value2.getProductPrice() ? value1 : value2;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            new ProcessWindowFunction&lt;UserActionLog, String, String, TimeWindow&gt;() &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public void process(String key, Context context, Iterable&lt;UserActionLog&gt; elements, Collector&lt;String&gt; out) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">                    UserActionLog max &#x3D; elements.iterator().next();</span><br><span class="line"></span><br><span class="line">                    String windowStart&#x3D;new DateTime(context.window().getStart(), DateTimeZone.forID(&quot;+08:00&quot;)).toString(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">                    String windowEnd&#x3D;new DateTime(context.window().getEnd(), DateTimeZone.forID(&quot;+08:00&quot;)).toString(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line"></span><br><span class="line">                    String record&#x3D;&quot;Key: &quot;+key+&quot; 窗口开始时间: &quot;+windowStart+&quot; 窗口结束时间: &quot;+windowEnd+&quot; 浏览的商品的最大价值对应的那条记录: &quot;+max;</span><br><span class="line">                    out.collect(record);</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">    )</span><br><span class="line">    .print();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 结果</span><br><span class="line">Key: user_2 窗口开始时间: 2019-11-09 13:54:10 窗口结束时间: 2019-11-09 13:54:20 浏览的商品的最大价值对应的那条记录: UserActionLog&#123;userID&#x3D;&#39;user_2&#39;, eventTime&#x3D;&#39;2019-11-09 13:54:10&#39;, eventType&#x3D;&#39;browse&#39;, productID&#x3D;&#39;product_3&#39;, productPrice&#x3D;30&#125;</span><br><span class="line">Key: user_4 窗口开始时间: 2019-11-09 13:54:10 窗口结束时间: 2019-11-09 13:54:20 浏览的商品的最大价值对应的那条记录: UserActionLog&#123;userID&#x3D;&#39;user_4&#39;, eventTime&#x3D;&#39;2019-11-09 13:54:15&#39;, eventType&#x3D;&#39;browse&#39;, productID&#x3D;&#39;product_3&#39;, productPrice&#x3D;30&#125;</span><br><span class="line">Key: user_3 窗口开始时间: 2019-11-09 13:54:10 窗口结束时间: 2019-11-09 13:54:20 浏览的商品的最大价值对应的那条记录: UserActionLog&#123;userID&#x3D;&#39;user_3&#39;, eventTime&#x3D;&#39;2019-11-09 13:54:12&#39;, eventType&#x3D;&#39;browse&#39;, productID&#x3D;&#39;product_2&#39;, productPrice&#x3D;20&#125;</span><br><span class="line">Key: user_5 窗口开始时间: 2019-11-09 13:54:10 窗口结束时间: 2019-11-09 13:54:20 浏览的商品的最大价值对应的那条记录: UserActionLog&#123;userID&#x3D;&#39;user_5&#39;, eventTime&#x3D;&#39;2019-11-09 13:54:17&#39;, eventType&#x3D;&#39;browse&#39;, productID&#x3D;&#39;product_2&#39;, productPrice&#x3D;20&#125;</span><br></pre></td></tr></table></figure>
<h4 id="ProcessWindowFunction与AggregateFunction结合"><a href="#ProcessWindowFunction与AggregateFunction结合" class="headerlink" title="ProcessWindowFunction与AggregateFunction结合"></a>ProcessWindowFunction与AggregateFunction结合</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 测试数据: 某个用户在某个时刻浏览了某个商品，以及商品的价值</span><br><span class="line">&#x2F;&#x2F; &#123;&quot;userID&quot;: &quot;user_4&quot;, &quot;eventTime&quot;: &quot;2019-11-09 10:41:32&quot;, &quot;eventType&quot;: &quot;browse&quot;, &quot;productID&quot;: &quot;product_1&quot;, &quot;productPrice&quot;: 10&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; API: 如上AggregateFunction与ProcessWindowFunction</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 示例: 获取一段时间内(Window Size)每个用户(KeyBy)浏览的平均价值(AggregateFunction),并获得Key和Window信息。</span><br><span class="line">kafkaStream</span><br><span class="line">    &#x2F;&#x2F; 将从Kafka获取的JSON数据解析成Java Bean</span><br><span class="line">    .process(new KafkaProcessFunction())</span><br><span class="line">    &#x2F;&#x2F; 提取时间戳生成水印</span><br><span class="line">    .assignTimestampsAndWatermarks(new MyCustomBoundedOutOfOrdernessTimestampExtractor(Time.seconds(maxOutOfOrdernessSeconds)))</span><br><span class="line">    &#x2F;&#x2F; 按用户分组</span><br><span class="line">    .keyBy((KeySelector&lt;UserActionLog, String&gt;) UserActionLog::getUserID)</span><br><span class="line">    &#x2F;&#x2F; 构造TimeWindow</span><br><span class="line">    .timeWindow(Time.seconds(windowLengthSeconds))</span><br><span class="line">    &#x2F;&#x2F; 窗口函数: 获取这段窗口时间内，每个用户浏览的商品的平均价值，并发出Key和Window信息</span><br><span class="line">    .aggregate(</span><br><span class="line">         new AggregateFunction&lt;UserActionLog, Tuple2&lt;Long, Long&gt;, Double&gt;() &#123;</span><br><span class="line"></span><br><span class="line">             &#x2F;&#x2F; 1、初始值</span><br><span class="line">             &#x2F;&#x2F; 定义累加器初始值</span><br><span class="line">             @Override</span><br><span class="line">             public Tuple2&lt;Long, Long&gt; createAccumulator() &#123;</span><br><span class="line">                 return new Tuple2&lt;&gt;(0L, 0L);</span><br><span class="line">             &#125;</span><br><span class="line"></span><br><span class="line">             &#x2F;&#x2F; 2、累加</span><br><span class="line">             &#x2F;&#x2F; 定义累加器如何基于输入数据进行累加</span><br><span class="line">             @Override</span><br><span class="line">             public Tuple2&lt;Long, Long&gt; add(UserActionLog value, Tuple2&lt;Long, Long&gt; accumulator) &#123;</span><br><span class="line">                 accumulator.f0 +&#x3D; 1;</span><br><span class="line">                 accumulator.f1 +&#x3D; value.getProductPrice();</span><br><span class="line">                 return accumulator;</span><br><span class="line">             &#125;</span><br><span class="line"></span><br><span class="line">             &#x2F;&#x2F; 3、合并</span><br><span class="line">             &#x2F;&#x2F; 定义累加器如何和State中的累加器进行合并</span><br><span class="line">             @Override</span><br><span class="line">             public Tuple2&lt;Long, Long&gt; merge(Tuple2&lt;Long, Long&gt; acc1, Tuple2&lt;Long, Long&gt; acc2) &#123;</span><br><span class="line">                 acc1.f0 +&#x3D; acc2.f0;</span><br><span class="line">                 acc1.f1 +&#x3D; acc2.f1;</span><br><span class="line">                 return acc1;</span><br><span class="line">             &#125;</span><br><span class="line"></span><br><span class="line">             &#x2F;&#x2F; 4、输出</span><br><span class="line">             &#x2F;&#x2F; 定义如何输出数据</span><br><span class="line">             @Override</span><br><span class="line">             public Double getResult(Tuple2&lt;Long, Long&gt; accumulator) &#123;</span><br><span class="line">                 return accumulator.f1 &#x2F; (accumulator.f0 * 1.0);</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;,</span><br><span class="line">         new ProcessWindowFunction&lt;Double, String, String, TimeWindow&gt;() &#123;</span><br><span class="line">             @Override</span><br><span class="line">             public void process(String key, Context context, Iterable&lt;Double&gt; elements, Collector&lt;String&gt; out) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">                 Double avg &#x3D; elements.iterator().next();</span><br><span class="line"></span><br><span class="line">                 String windowStart&#x3D;new DateTime(context.window().getStart(), DateTimeZone.forID(&quot;+08:00&quot;)).toString(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">                 String windowEnd&#x3D;new DateTime(context.window().getEnd(), DateTimeZone.forID(&quot;+08:00&quot;)).toString(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line"></span><br><span class="line">                 String record&#x3D;&quot;Key: &quot;+key+&quot; 窗口开始时间: &quot;+windowStart+&quot; 窗口结束时间: &quot;+windowEnd+&quot; 浏览的商品的平均价值: &quot;+String.format(&quot;%.2f&quot;,avg);</span><br><span class="line">                 out.collect(record);</span><br><span class="line"></span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">    )</span><br><span class="line">    .print();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;结果</span><br><span class="line">Key: user_2 窗口开始时间: 2019-11-09 14:05:40 窗口结束时间: 2019-11-09 14:05:50 浏览的商品的平均价值: 13.33</span><br><span class="line">Key: user_3 窗口开始时间: 2019-11-09 14:05:50 窗口结束时间: 2019-11-09 14:06:00 浏览的商品的平均价值: 25.00</span><br><span class="line">Key: user_4 窗口开始时间: 2019-11-09 14:05:50 窗口结束时间: 2019-11-09 14:06:00 浏览的商品的平均价值: 20.00</span><br><span class="line">Key: user_2 窗口开始时间: 2019-11-09 14:05:50 窗口结束时间: 2019-11-09 14:06:00 浏览的商品的平均价值: 30.00</span><br><span class="line">Key: user_5 窗口开始时间: 2019-11-09 14:05:50 窗口结束时间: 2019-11-09 14:06:00 浏览的商品的平均价值: 20.00</span><br><span class="line">Key: user_1 窗口开始时间: 2019-11-09 14:05:50 窗口结束时间: 2019-11-09 14:06:00 浏览的商品的平均价值: 23.33</span><br></pre></td></tr></table></figure>

<h4 id="ProcessWindowFunction与AggregateFunction结合-—为了好看"><a href="#ProcessWindowFunction与AggregateFunction结合-—为了好看" class="headerlink" title="ProcessWindowFunction与AggregateFunction结合 —为了好看"></a>ProcessWindowFunction与AggregateFunction结合 —为了好看</h4><p><a href="https://www.cnblogs.com/bjwu/p/10393146.html" target="_blank" rel="noopener">https://www.cnblogs.com/bjwu/p/10393146.html</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-Watermark</title>
    <url>/2020/05/11/Flink-Watermark/</url>
    <content><![CDATA[<h3 id="官网指路"><a href="#官网指路" class="headerlink" title="官网指路"></a>官网指路</h3><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/event_timestamps_watermarks.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/event_timestamps_watermarks.html</a></p>
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>!!! 经实际使用发现任务刚刚启动时watermark没有调用<code>extractTimestamp</code>方法，因此会是初始值。。。。</p>
<p>Flink中测量进度使用的是水印（Watermark）in event time。水印流作为数据流的一部分，携带了一个时间戳t。<br>一个Watermark(t)标志数据流中“事件时间”已经到了t，即不再会有时间戳小于等于t数据过来。</p>
<p>Watermark：算子通过Watermark推断当前的事件时间。Watermark用于通知算子没有比水位更小的时间戳的事件会发生了。</p>
<p>一般来讲Watermark经常和Window一起被用来处理乱序事件。解决窗口的触发时机以及对于乱序数据的数据保障(或者说数据乱序情况下依然分配进属于它的窗口中)。</p>
<p>WaterMark绝大部分时候是和eventTime配合使用，最大程度上确保了Flink窗口处理中数据的顺序（注意是watermark只是最大程度上确保了数据顺序即一定程度上缓解了数据乱序的问题，某些情况下，数据延迟非常严重即watermark机制也无法确保数据全部进入窗口，关于延迟数据参考allowed lateness机制）。</p>
<p>watermark是一个全局的值，不是某一个key下的值，所以即使不是同一个key的数据，其warmark也会增加.currentMaxTimestamp也增加了。<br>1.窗口的触发时机问题: (有两个关键词 1:Partition Watermark , 2:事件时间时钟)<br>  – 任务会对flink的输入分区维护相应的Partition Watermark.<br>     当检测到某一分区有输入后,系统会提取该输入事件的watermark与当前分区的watermark 进行比较<br>     并将大的watermark更新当前Partition watermark(相当于是一个更新操作);<br>    更新后会将 Partition Watermark (分区) 中最小的那个watermark 作为事件时间时钟 向下游发送 后续就是trigger的操作了</p>
<h3 id="传递"><a href="#传递" class="headerlink" title="传递"></a>传递</h3><p>Watermark在定义处随数据流一同往下传递，当一个operate有多个输入流或者或者channel时，采用最小的watermark。<br>水印是单调递增，新的来了会替换旧的。输出的水印是所有输入最小的水印。</p>
<h3 id="与window一起使用时"><a href="#与window一起使用时" class="headerlink" title="与window一起使用时"></a>与window一起使用时</h3><p>查看源码<code>EventTimeTrigger</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">	public TriggerResult onElement(Object element, long timestamp, TimeWindow window, TriggerContext ctx) throws Exception &#123;</span><br><span class="line">		if (window.maxTimestamp() &lt;&#x3D; ctx.getCurrentWatermark()) &#123;</span><br><span class="line">			&#x2F;&#x2F; if the watermark is already past the window fire immediately</span><br><span class="line">			return TriggerResult.FIRE;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			ctx.registerEventTimeTimer(window.maxTimestamp());</span><br><span class="line">			return TriggerResult.CONTINUE;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#96;&#96;&#96;  </span><br><span class="line">&#96;watermark &gt;&#x3D; end_of_window + allowedLateness(默认是0) &#96;时窗口关闭. 但窗口里真实计算的是&#96;event time&lt;end_of_window + allowedLateness&#96;</span><br><span class="line"></span><br><span class="line">Flink何时触发window？</span><br><span class="line"></span><br><span class="line">&#96;1、watermark时间 &gt; Event Time（对于late element太多的数据而言）&#96;</span><br><span class="line">或者</span><br><span class="line">&#96;1、watermark时间 &gt;&#x3D; window_end_time（对于out-of-order以及正常的数据而言）&#96;</span><br><span class="line">&#96;2、在[window_start_time, window_end_time)中有数据存在&#96;</span><br><span class="line"></span><br><span class="line">参考：https:&#x2F;&#x2F;blog.csdn.net&#x2F;xorxos&#x2F;article&#x2F;details&#x2F;80715113</span><br><span class="line">### 使用流程</span><br><span class="line">1、指定使用event time</span><br><span class="line">&#96;env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#96;</span><br><span class="line">2、定义获取event time的字段和watermark</span><br><span class="line">方式1： 在source中定义，使数据刚进去flink时就分配event time和watermark。具体使用可以参看&#96;SourceFunction&#96;,但如果是使用的不是自定义的source，那就无法使用此方法。</span><br></pre></td></tr></table></figure>
<p>@Override<br>public void run(SourceContext<MyType> ctx) throws Exception {<br>    while (/* condition */) {<br>        MyType next = getNext();<br>        ctx.collectWithTimestamp(next, next.getEventTimestamp());</p>
<pre><code>    if (next.hasWatermarkTime()) {
        ctx.emitWatermark(new Watermark(next.getWatermarkTime()));
    }
}</code></pre><p>}</p>
<pre><code>方式2： 在要使用watermark之前的operate后定义`assignTimestampsAndWatermarks`
两种方法同时使用，第2中会覆盖第一种。
3、如果在上面选用第二种方式，timestamp 和 watermark 的生成器主要有两类，`AssignerWithPeriodicWatermarks` and `AssignerWithPunctuatedWatermarks`。或者可以直接实现抽象类`BoundedOutOfOrdernessTimestampExtractor`，传入允许延迟的最大时间，和实现获取event time的方法。

### Watermarks提取器类型
1、periodic watermark根据设定周期性的生成watermarks，默认是每200ms发一次，通过这个配置修改`env.getConfig().setAutoWatermarkInterval(200);`。每次会调用`getCurrentWatermark()`方法然后发射出一个新的watemark（如果这个watermark不为null &amp;&amp; 这个新的watermark大于当前的watermark）
2、puncuated watermarks 几乎每条数据都会生成水印。
![](1.png)

#### periodic watermarks
通常使用的watermarks生成选用方式。Periodic Watermark分为两种，升序模式和固定时间间隔模式。
(1) 升序模式
Ascending Timestamp分配器会指定数据中的Timestamp字段，并用当前的Timestamp作为最小的watermark。这个模式比较适合于事件按顺序生成，没有乱序事件的情况。
升序模式通过实现抽象类`AscendingTimestampExtractor`定义。`AscendingTimestampExtractor`的抽象方法`extractAscendingTimestamp`定义了如何从元素中抽取Timestamp。
(2) 固定时间间隔
该模式是通过设置固定的时间来指定watermark落后于Timestamp的区间长度。该固定时间就是最长容忍迟到的时间，也即系统最大容忍迟到多长时间内的数据到达系统。
通过创建抽象类`BoundedOutOfOrdernessTimestampExtractor`的子类实现Timestamp Assigner。`BoundedOutOfOrdernessTimestampExtractor`定义了`extractTimestamp`抽象方法用于从元素中抽取Timestamp，其子类要在该方法实现Timestamp抽取策略;实现抽象类时传入参数`maxOutOfOrderness`,是再大容忍的延迟时间。 Watermarks创建是根据Timestamp减去固定时间长度生成。如果当前数据中时间小于等于watermarks时间，则认为是迟到时间。

#### puncuated watermarks
间断性调用getCurrentWatermark，它会根据一个条件发送watermark，这个条件可以自己去定义。
数据流中每一个递增的EventTime都会产生一个Watermark，在实际生产中TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。
实现接口`AssignerWithPunctuatedWatermarks`.

### allowedLateness &amp;&amp; 什么情况下数据会被丢弃或者说不会被计算？
https://www.cnblogs.com/gouhaiping/p/12556802.html
默认情况下，如果不指定allowedLateness，其值是0
allowedLateness就是针对event time而言，对于watermark超过end-of-window之后，还允许有一段时间（也是以event time来衡量）来等待之前的数据到达，以便再次处理这些数据。
注意：对于trigger是默认的EventTimeTrigger的情况下，allowedLateness会再次触发窗口的计算，而之前触发的数据，会buffer起来，直到watermark超过end-of-window + allowedLateness（）的时间，窗口的数据及元数据信息才会被删除。再次计算就是DataFlow模型中的Accumulating（积累）的情况。


两种情况：
a.未设置allowedLateness情况下，某条数据属于某个窗口，但是watermark超过了窗口的结束时间，则该条数据会被丢弃；
b.设置allowedLateness情况下，某条数据属于某个窗口，但是watermark超过了窗口的结束时间+延迟时间，则该条数据会被丢弃；

 也就是说如果一个key下面的某条数据如果延迟到来太多，就会被丢弃，这个问题无法解决的

题外话：非要用Processing Time，只要加上env.getConfig().setAutoWatermarkInterval(200);这句话就可以了，是没有任何效果。（https://blog.csdn.net/lvwenyuan_1/article/details/91389124?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf）

参考：https://blog.csdn.net/xorxos/article/details/80715113  (!!!有事例做演示)
https://blog.csdn.net/weixin_42261489/article/details/105756384
https://blog.csdn.net/oTengYue/article/details/102689538?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1</code></pre>]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-clickhouse</title>
    <url>/2020/05/11/Flink-clickhouse/</url>
    <content><![CDATA[<h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><p>使用flink的<code>JDBCAppendTableSink</code>,和写入mysql方式都类似。<br>1、mvn中引入jar</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;properties&gt;</span><br><span class="line">  &lt;flink.version&gt;1.10.0&lt;&#x2F;flink.version&gt;</span><br><span class="line">  &lt;scala.version&gt;2.11.12&lt;&#x2F;scala.version&gt;</span><br><span class="line">&lt;&#x2F;properties&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">   &lt;groupId&gt;ru.yandex.clickhouse&lt;&#x2F;groupId&gt;</span><br><span class="line">   &lt;artifactId&gt;clickhouse-jdbc&lt;&#x2F;artifactId&gt;</span><br><span class="line">   &lt;version&gt;0.2&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">   &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">   &lt;artifactId&gt;flink-jdbc_$&#123;scala.tools.version&#125;&lt;&#x2F;artifactId&gt;</span><br><span class="line">   &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&#96;&#96;&#96;       </span><br><span class="line">2、代码</span><br></pre></td></tr></table></figure>
<p>fields = [‘uid’, ‘type’, ‘stime’, ‘r_query’, ‘tokens’,<br>                  ‘e’, ‘source’, ‘ip’, ‘position’, ‘platform’,<br>                  ‘query’, ‘doc_qipu_pair’, ‘rpage’, ‘bucket’,<br>                  ‘page’, ‘clk_type’, ‘dt’, ‘dh’]<br>tableName = “search_etl”<br>sql = “insert into search_etl(uid, type, stime, r_query, tokens, e, source, ip, position, “ +<br>        “platform, query, doc_qipu_pair, rpage, bucket, page, clk_type, dt, dh) “ +<br>        “values(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)”</p>
<p>val CH_DRIVER = “ru.yandex.clickhouse.ClickHouseDriver”<br>var inputFields: Array[String] = _<br>var fieldsSchema: List[TypeInformation[_]] = _<br>var sql: String = _<br>var sinkJdbc: JDBCAppendTableSink = _</p>
<p>sql = taskInfo.getSql.getSql<br>inputFields = taskInfo.getProjecting.getFields.asScala.toArray<br>fieldsSchema = List.fill(inputFields.length)(BasicTypeInfo.STRING_TYPE_INFO)<br>sinkJdbc = JDBCAppendTableSink.builder()<br>  .setDrivername(CH_DRIVER)<br>  .setUsername(CH_USER)<br>  .setPassword(CH_PWD)<br>  .setDBUrl(CH_DRIVER_URL)<br>  .setQuery(sql)<br>  .setParameterTypes(fieldsSchema.toArray: _*)<br>  .setBatchSize(1000)<br>  .build()</p>
<p>val sourceStream: DataStream[String] = addSource</p>
<p>val stream: DataStream[Row] = {<br>      implicit val tpe: TypeInformation[Row] = new RowTypeInfo(fieldsSchema.toArray, inputFields)<br>      sourceStream<br>        .map[Row](new EtlLoaderMap(inputFields.slice(0, inputFields.length - 2)))<br>        .uid(“parse_json_2_hive”)<br>    }</p>
<pre><code>sinkJdbc.emitDataStream(stream.javaStream)      </code></pre><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">3、其他</span><br></pre></td></tr></table></figure>
<p>import com.couchbase.client.java.document.json.JsonObject<br>import org.apache.flink.api.common.functions.MapFunction<br>import org.apache.flink.types.Row<br>import org.joda.time.DateTime<br>import scala.collection.JavaConverters._</p>
<p>class EtlLoaderMap(inputFields: Array[String]) extends MapFunction[String, Row]{</p>
<p>  override def map(value: String): Row = {</p>
<pre><code>val values = JsonObject.fromJson(value).toMap
val minutesAgo =  BehaviorHistoryMergeUtils.minutesLater(0L, -120)
val pingbackTime = values.get(&quot;stime&quot;).toString.toLong
val row = new Row(inputFields.length + 2)
values.asScala.foreach(f =&gt; if(inputFields.indexOf(f._1) &gt;= 0) row.setField(inputFields.indexOf(f._1), f._2.toString) else println(&quot;wrong field&quot;+f._1))
if (pingbackTime &gt;= minutesAgo) {
  row.setField(inputFields.length, new DateTime(pingbackTime).toString(&quot;yyyy-MM-dd&quot;))
  row.setField(inputFields.length + 1, new DateTime(pingbackTime).toString(&quot;HH&quot;))
} else {
  row.setField(inputFields.length, &quot;2012-12-21&quot;)
  row.setField(inputFields.length + 1, &quot;24&quot;)
}

row</code></pre><p>  }</p>
<p>}</p>
<pre><code>### 其他文章
https://mp.weixin.qq.com/s/yEnsXPtERMh7tBwvxx8GVw （比较详细，比较好）
https://mp.weixin.qq.com/s/afSQPcTMePNNLoDSgMBRWg
### 参考
https://wchch.github.io/2019/11/27/%E4%BD%BF%E7%94%A8Flink-SQL%E8%AF%BB%E5%8F%96kafka%E6%95%B0%E6%8D%AE%E5%B9%B6%E9%80%9A%E8%BF%87JDBC%E6%96%B9%E5%BC%8F%E5%86%99%E5%85%A5Clickhouse%E5%AE%9E%E6%97%B6%E5%9C%BA%E6%99%AF%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E4%BE%8B/ （flink sql）

https://blog.csdn.net/wflh323/article/details/95794719 （写mysql）
https://www.cnblogs.com/qiu-hua/p/13871460.html (flink1.10&amp;1.11写入ch的例子)</code></pre>]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink-窗口</title>
    <url>/2020/05/11/Flink-%E7%AA%97%E5%8F%A3/</url>
    <content><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Keyed Windows</span><br><span class="line"></span><br><span class="line">stream</span><br><span class="line">       .keyBy(...)               &lt;-  keyed versus non-keyed windows</span><br><span class="line">       .window(...)              &lt;-  required: &quot;assigner&quot;</span><br><span class="line">      [.trigger(...)]            &lt;-  optional: &quot;trigger&quot; (else default trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: &quot;evictor&quot; (else no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: &quot;lateness&quot; (else zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: &quot;output tag&quot; (else no side output for late data)</span><br><span class="line">       .reduce&#x2F;aggregate&#x2F;fold&#x2F;apply()      &lt;-  required: &quot;function&quot;</span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: &quot;output tag&quot;</span><br><span class="line"></span><br><span class="line">Non-Keyed Windows</span><br><span class="line"></span><br><span class="line">stream</span><br><span class="line">       .windowAll(...)           &lt;-  required: &quot;assigner&quot;</span><br><span class="line">      [.trigger(...)]            &lt;-  optional: &quot;trigger&quot; (else default trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: &quot;evictor&quot; (else no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: &quot;lateness&quot; (else zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: &quot;output tag&quot; (else no side output for late data)</span><br><span class="line">       .reduce&#x2F;aggregate&#x2F;fold&#x2F;apply()      &lt;-  required: &quot;function&quot;</span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: &quot;output tag&quot;</span><br></pre></td></tr></table></figure>
<p>Flink必须在调用函数之前在内部缓冲窗口中的所有元素，所以使用非增量计算的reduce/aggregate/fold的执行效率都不高。<br>AllWindowedStream 警告：所有数据将发送给下游的单个实例，或者说下游算子的并行度为1。</p>
<h3 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h3><p><img src="tumbling.svg" alt=""><br>特殊用法,修改对齐窗口时间：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; daily tumbling event-time windows offset by -8 hours.</span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;)</span><br></pre></td></tr></table></figure>
<p>For example, without offsets hourly tumbling windows are aligned with epoch, that is you will get windows such as 1:00:00.000 - 1:59:59.999, 2:00:00.000 - 2:59:59.999 and so on. If you want to change that you can give an offset. With an offset of 15 minutes you would, for example, get 1:15:00.000 - 2:14:59.999, 2:15:00.000 - 3:14:59.999 etc.An important use case for offsets is to adjust windows to timezones other than UTC-0. For example, in China you would have to specify an offset of Time.hours(-8).    </p>
<h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><p><img src="sliding.svg" alt=""><br>size&gt;interval,那么就会形成sliding-window(有重叠数据)<br>如果size&lt; interval, 那么这种窗口将会丢失数据。比如每5秒钟，统计过去3秒的通过路口汽车的数据，将会漏掉2秒钟的数据。<br>特殊用法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; sliding processing-time windows offset by -8 hours</span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;)</span><br></pre></td></tr></table></figure>
<p>As shown in the last example, sliding window assigners also take an optional offset parameter that can be used to change the alignment of windows. For example, without offsets hourly windows sliding by 30 minutes are aligned with epoch, that is you will get windows such as 1:00:00.000 - 1:59:59.999, 1:30:00.000 - 2:29:59.999 and so on. If you want to change that you can give an offset. With an offset of 15 minutes you would, for example, get 1:15:00.000 - 2:14:59.999, 1:45:00.000 - 2:44:59.999 etc.</p>
<h3 id="会话窗口"><a href="#会话窗口" class="headerlink" title="会话窗口"></a>会话窗口</h3><p><img src="session.svg" alt=""><br>活动的事件进行窗口化。窗口的长度可变，每个窗口的开始和结束时间并不是确定的。可以设置定长的Session gap，也可以使用SessionWindowTimeGapExtractor动态地确定Session gap的长度。<br>当一个窗口在大于Session gap的时间内没有接收到新数据时，窗口将关闭。</p>
<pre><code>val input: DataStream[T] = ...

// event-time session windows with static gap
input
    .keyBy(&lt;key selector&gt;)
    .window(EventTimeSessionWindows.withGap(Time.minutes(10)))
    .&lt;windowed transformation&gt;(&lt;window function&gt;)

// event-time session windows with dynamic gap
input
    .keyBy(&lt;key selector&gt;)
    .window(EventTimeSessionWindows.withDynamicGap(new SessionWindowTimeGapExtractor[String] {
      override def extract(element: String): Long = {
        // determine and return session gap
      }
    }))
    .&lt;windowed transformation&gt;(&lt;window function&gt;)

// processing-time session windows with static gap
input
    .keyBy(&lt;key selector&gt;)
    .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10)))
    .&lt;windowed transformation&gt;(&lt;window function&gt;)


// processing-time session windows with dynamic gap
input
    .keyBy(&lt;key selector&gt;)
    .window(DynamicProcessingTimeSessionWindows.withDynamicGap(new SessionWindowTimeGapExtractor[String] {
      override def extract(element: String): Long = {
        // determine and return session gap
      }
    }))
    .&lt;windowed transformation&gt;(&lt;window function&gt;)</code></pre><h3 id="其他操作"><a href="#其他操作" class="headerlink" title="其他操作"></a>其他操作</h3><p><a href="https://www.cnblogs.com/bjwu/p/10393146.html" target="_blank" rel="noopener">https://www.cnblogs.com/bjwu/p/10393146.html</a></p>
<h4 id="Triggers（触发器）"><a href="#Triggers（触发器）" class="headerlink" title="Triggers（触发器）"></a>Triggers（触发器）</h4><p>触发器定义了窗口何时准备好被窗口处理。每个窗口分配器默认都有一个触发器，如果默认的触发器不符合你的要求，就可以使用trigger(…)自定义触发器。<br>通常来说，默认的触发器适用于多种场景。例如，多有的event-time窗口分配器都有一个EventTimeTrigger作为默认触发器。该触发器在watermark通过窗口末尾时出发。<br>PS：GlobalWindow默认的触发器时NeverTrigger，该触发器从不出发，所以在使用GlobalWindow时必须自定义触发器。</p>
<h4 id="Allowed-Lateness"><a href="#Allowed-Lateness" class="headerlink" title="Allowed Lateness"></a>Allowed Lateness</h4><p>当使用event-time窗口时，元素可能会晚到，例如Flink用于跟踪event-time进度的watermark已经超过了窗口的结束时间戳。<br>默认来说，当watermark超过窗口的末尾时，晚到的元素会被丢弃。但是flink也允许为窗口operator指定最大的allowed lateness，以至于可以容忍在彻底删除元素之前依然接收晚到的元素，其默认值是0。<br>为了支持该功能，Flink会保持窗口的状态，知道allowed lateness到期。一旦到期，flink会删除窗口并删除其状态。<br>把晚到的元素可以用side output输出使用。</p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink读取kafka数据并以parquet格式写入HDFS</title>
    <url>/2020/05/11/Flink%E8%AF%BB%E5%8F%96kafka%E6%95%B0%E6%8D%AE%E5%B9%B6%E4%BB%A5parquet%E6%A0%BC%E5%BC%8F%E5%86%99%E5%85%A5HDFS/</url>
    <content><![CDATA[<p>！！如果是flink 1.11建议尝试flink-sql的直接写入hive的方式。对生成success文件等都有封装。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html" target="_blank" rel="noopener">官网</a> <a href="https://blog.csdn.net/nazeniwaresakini/article/details/107811860" target="_blank" rel="noopener">文章</a></p>
<p>Flink目前对于外部Exactly-Once写支持提供了两种的sink，一个是Kafka-Sink，另一个是Hdfs-Sink，这两种sink实现的Exactly-Once都是基于Flink checkpoint提供的hook来实现的两阶段提交模式来保证的。</p>
<p>1、Flink 提供了两个分桶策略，分桶策略实现了<br>org.apache.flink.streaming.api.functions.sink.filesystem.BucketAssigner 接口：<br>BasePathBucketAssigner，不分桶，所有文件写到根目录；<br>DateTimeBucketAssigner，基于系统时间(yyyy-MM-dd–HH)分桶。</p>
<p>除此之外，还可以实现BucketAssigner接口，自定义分桶策略。</p>
<p>2、Flink 提供了两个滚动策略，滚动策略实现了<br>org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicy 接口：<br>DefaultRollingPolicy 当超过最大桶大小（默认为 128 MB），或超过了滚动周期（默认为 60 秒），或未写入数据处于不活跃状态超时（默认为 60 秒）的时候，滚动文件；<br>OnCheckpointRollingPolicy 当 checkpoint 的时候，滚动文件。</p>
<p>3、StreamingFileSink提供了基于行、列两种文件写入格式。<br>forRowFormat行写可基于文件大小、滚动时间、不活跃时间进行滚动。<br>forBulkFormat列写方式只能基于checkpoint机制进行文件滚动，即在执行snapshotState方法时滚动文件。<br>如果基于大小或者时间滚动文件，那么在任务失败恢复时就必须对处于in-processing状态的文件按照指定的offset进行truncate，我想这是由于列式存储是无法针对文件offset进行truncate的，因此就必须在每次checkpoint使文件滚动，其使用的滚动策略实现是OnCheckpointRollingPolicy。<br>压缩：自定义<code>ParquetAvroWriters</code>方法，创建<code>AvroParquetWriter</code>时传入压缩方式。<br>以上来自：<a href="https://blog.csdn.net/u013516966/article/details/104726234/（压缩与合并小文件）" target="_blank" rel="noopener">https://blog.csdn.net/u013516966/article/details/104726234/（压缩与合并小文件）</a></p>
<p>Hdfs 上有三个状态<br> 第一个状态是 in-progress ，正在进行状态。<br> 第二个状态是 pending 状态，<br> 第三个状态是 finished 状态。</p>
<p>如何使用 Flink 实现二阶段提交协议 2pc<br>首先，StreamingFileSink 实现两个接口，CheckpointedFunction 和 CheckpointListener。 CheckpointedFunction 实现 initializeState 和 snapshotState 函数。 CheckpointListener 是 notifyCheckpointComplete 的方法实现，因此这两个接口可以实现二阶段提交语义。<br>snapshotState<br>触发 CheckPoint 时会将 in-progress 文件转化为 pending state，同时记录数据长度（truncate 方式需要截断长度）。 snapshotState 并非真正将数据写入 HDFS，而是写入 ListState。 Flink 在 Barrier 对齐状态时内部实现 Exactly-Once 语义，但是实现外部端到端的 Exactly-Once 语义比较困难。 Flink 内部实现 Exactly-Once 通过 ListState，将数据全部存入 ListState，等待所有算子 CheckPoint 完成，再将 ListState 中的数据刷到 HDFS 中。<br>notifyCheckpointComplete<br>notifyCheckpointComplete <code>会触发 pending 到 finished state 的数据写入</code>。 实现方法是 rename，Streaming 不断向 HDFS 写入临时文件，所有动作结束后通过 rename 动作写成正式文件。</p>
<p>使用 Hadoop &lt; 2.7 时，请使用 OnCheckpointRollingPolicy 滚动策略，该策略会在每次checkpoint检查点时进行文件切割。 这样做的原因是如果部分文件的生命周期跨多个检查点，当 StreamingFileSink 从之前的检查点进行恢复时会调用文件系统的 truncate() 方法清理 in-progress 文件中未提交的数据。 Hadoop 2.7 之前的版本不支持这个方法，因此 Flink 会报异常，因此使用OnCheckpointRollingPolicy避免文件跨多个检查点。</p>
<p>part file有三种状态：</p>
<ul>
<li>in-progress:表示当前part file正在被写入</li>
<li>pending:写入完成后等待提交的状态（flink写hdfs的二段提交的体现）</li>
<li>finished:写入完成状态，只有finished状态下的文件才会保证不会再有修改，下游可安全读取</li>
</ul>
<p>为防止原文被删，以下全文转载自：<a href="https://blog.csdn.net/u013411339/article/details/88937671" target="_blank" rel="noopener">https://blog.csdn.net/u013411339/article/details/88937671</a></p>
<p>大数据业务场景中，经常有一种场景：外部数据发送到kafka中，flink作为中间件消费kafka数据并进行业务处理；处理完成之后的数据可能还需要写入到数据库或者文件系统中，比如写入hdfs中；<br>目前基于spark进行计算比较主流，需要读取hdfs上的数据，可以通过读取parquet：spark.read.parquet(path)</p>
<h3 id="第一种方式"><a href="#第一种方式" class="headerlink" title="第一种方式"></a>第一种方式</h3><p>数据实体：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class Prti &#123;</span><br><span class="line"></span><br><span class="line">    private String passingTime;</span><br><span class="line"></span><br><span class="line">    private String plateNo;</span><br><span class="line"></span><br><span class="line">    public Prti() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   &#x2F;&#x2F;gettter and setter 方法....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class FlinkReadKafkaToHdfs &#123;</span><br><span class="line"></span><br><span class="line">    private final static StreamExecutionEnvironment environment &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">    private final static Properties properties &#x3D; new Properties();</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * kafka 中发送数据JSON格式：</span><br><span class="line">     * &#123;&quot;passingTime&quot;:&quot;1546676393000&quot;,&quot;plateNo&quot;:&quot;1&quot;&#125;</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        init();</span><br><span class="line">        readKafkaToHdfsByReflect(environment, properties);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static void init() &#123;</span><br><span class="line">        environment.enableCheckpointing(5000);</span><br><span class="line">        environment.setParallelism(1);</span><br><span class="line">        environment.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">        &#x2F;&#x2F;kafka的节点的IP或者hostName，多个使用逗号分隔</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;192.168.0.10:9092&quot;);</span><br><span class="line">        &#x2F;&#x2F;only required for Kafka 0.8;</span><br><span class="line">&#x2F;&#x2F;        properties.setProperty(&quot;zookeeper.connect&quot;, &quot;192.168.0.10:2181&quot;);</span><br><span class="line">        &#x2F;&#x2F;flink consumer flink的消费者的group.id</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;, &quot;test-consumer-group&quot;);</span><br><span class="line">        &#x2F;&#x2F;第一种方式：路径写自己代码上的路径</span><br><span class="line">&#x2F;&#x2F;        properties.setProperty(&quot;fs.hdfs.hadoopconf&quot;, &quot;...\\src\\main\\resources&quot;);</span><br><span class="line">        &#x2F;&#x2F;第二种方式：填写一个schema参数即可</span><br><span class="line">        properties.setProperty(&quot;fs.default-scheme&quot;, &quot;hdfs:&#x2F;&#x2F;hostname:8020&quot;);</span><br><span class="line"></span><br><span class="line">        properties.setProperty(&quot;kafka.topic&quot;, &quot;test&quot;);</span><br><span class="line">        properties.setProperty(&quot;hfds.path&quot;, &quot;hdfs:&#x2F;&#x2F;hostname&#x2F;test&quot;);</span><br><span class="line">        properties.setProperty(&quot;hdfs.path.date.format&quot;, &quot;yyyy-MM-dd&quot;);</span><br><span class="line">        properties.setProperty(&quot;hdfs.path.date.zone&quot;, &quot;Asia&#x2F;Shanghai&quot;);</span><br><span class="line">        properties.setProperty(&quot;window.time.second&quot;, &quot;60&quot;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public static void readKafkaToHdfsByReflect(StreamExecutionEnvironment environment, Properties properties) throws Exception &#123;</span><br><span class="line">        String topic &#x3D; properties.getProperty(&quot;kafka.topic&quot;);</span><br><span class="line">        String path &#x3D; properties.getProperty(&quot;hfds.path&quot;);</span><br><span class="line">        String pathFormat &#x3D; properties.getProperty(&quot;hdfs.path.date.format&quot;);</span><br><span class="line">        String zone &#x3D; properties.getProperty(&quot;hdfs.path.date.zone&quot;);</span><br><span class="line">        Long windowTime &#x3D; Long.valueOf(properties.getProperty(&quot;window.time.second&quot;));</span><br><span class="line">        FlinkKafkaConsumer010&lt;String&gt; flinkKafkaConsumer010 &#x3D; new FlinkKafkaConsumer010&lt;&gt;(topic, new SimpleStringSchema(), properties);</span><br><span class="line">        KeyedStream&lt;Prti, String&gt; KeyedStream &#x3D; environment.addSource(flinkKafkaConsumer010)</span><br><span class="line">            .map(FlinkReadKafkaToHdfs::transformData)</span><br><span class="line">            .assignTimestampsAndWatermarks(new CustomWatermarks&lt;Prti&gt;())</span><br><span class="line">            .keyBy(Prti::getPlateNo);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;Prti&gt; output &#x3D; KeyedStream.window(TumblingEventTimeWindows.of(Time.seconds(windowTime)))</span><br><span class="line">            .apply(new WindowFunction&lt;Prti, Prti, String, TimeWindow&gt;() &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public void apply(String key, TimeWindow timeWindow, Iterable&lt;Prti&gt; iterable, Collector&lt;Prti&gt; collector) throws Exception &#123;</span><br><span class="line">                    System.out.println(&quot;keyBy: &quot; + key + &quot;, window: &quot; + timeWindow.toString());</span><br><span class="line">                    iterable.forEach(collector::collect);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#x2F;&#x2F;写入HDFS，parquet格式</span><br><span class="line">        DateTimeBucketAssigner&lt;Prti&gt; bucketAssigner &#x3D; new DateTimeBucketAssigner&lt;&gt;(pathFormat, ZoneId.of(zone));</span><br><span class="line">        StreamingFileSink&lt;Prti&gt; streamingFileSink &#x3D; StreamingFileSink.</span><br><span class="line">            forBulkFormat(new Path(path), ParquetAvroWriters.forReflectRecord(Prti.class))</span><br><span class="line">            .withBucketAssigner(bucketAssigner)</span><br><span class="line">            .build();</span><br><span class="line">        output.addSink(streamingFileSink).name(&quot;Hdfs Sink&quot;);</span><br><span class="line">        environment.execute(&quot;PrtiData&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   private static Prti transformData(String data) &#123;</span><br><span class="line">        if (data !&#x3D; null &amp;&amp; !data.isEmpty()) &#123;</span><br><span class="line">            JSONObject value &#x3D; JSON.parseObject(data);</span><br><span class="line">            Prti prti &#x3D; new Prti();</span><br><span class="line">            prti.setPlateNo(value.getString(&quot;plate_no&quot;));</span><br><span class="line">            prti.setPassingTime(value.getString(&quot;passing_time&quot;));</span><br><span class="line">            return prti;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            return new Prti();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static class CustomWatermarks&lt;T&gt; implements AssignerWithPunctuatedWatermarks&lt;Prti&gt; &#123;</span><br><span class="line"></span><br><span class="line">        private Long cuurentTime &#x3D; 0L;</span><br><span class="line"></span><br><span class="line">        @Nullable</span><br><span class="line">        @Override</span><br><span class="line">        public Watermark checkAndGetNextWatermark(Prti prti, long l) &#123;</span><br><span class="line">            return new Watermark(cuurentTime);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public long extractTimestamp(Prti prti, long l) &#123;</span><br><span class="line">            Long passingTime &#x3D; Long.valueOf(prti.getPassingTime());</span><br><span class="line">            cuurentTime &#x3D; Math.max(passingTime, cuurentTime);</span><br><span class="line">            return passingTime;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>进入spark shell中，执行：spark.read.parquet(“/test/日期路径”)，即可读取；<br>注意点：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">StreamingFileSink&lt;Prti&gt; streamingFileSink &#x3D; StreamingFileSink.</span><br><span class="line">            forBulkFormat(new Path(path), ParquetAvroWriters.forReflectRecord(Prti.class))</span><br><span class="line">            .withBucketAssigner(bucketAssigner)</span><br><span class="line">            .build();</span><br></pre></td></tr></table></figure>

<p>上面就是最简单直接的第一种方法：<br><code>ParquetAvroWriters.forReflectRecord(Prti.class)</code></p>
<h3 id="第二种方式"><a href="#第二种方式" class="headerlink" title="第二种方式"></a>第二种方式</h3><p>这种方式对实体类有很高的要求，需要借助avro的插件编译生成数据实体类即可；<br><code>ParquetAvroWriters.forSpecificRecord(Prti.class)</code><br>编写好一个prti.avsc的文件，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;namespace&quot;: &quot;com.xxx.streaming.entity&quot;,</span><br><span class="line">  &quot;type&quot;: &quot;record&quot;,</span><br><span class="line">  &quot;name&quot;: &quot;Prti&quot;,</span><br><span class="line">  &quot;fields&quot;: [</span><br><span class="line">      &#123;&quot;name&quot;: &quot;passingTime&quot;, &quot;type&quot;: &quot;string&quot;&#125;,</span><br><span class="line">      &#123;&quot;name&quot;: &quot;plateNo&quot;,  &quot;type&quot;: &quot;string&quot;&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中：com.xxx.streaming.entity是生成的实体放置的包路径；<br>在pom中引入插件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line">   &lt;groupId&gt;org.apache.avro&lt;&#x2F;groupId&gt;</span><br><span class="line">   &lt;artifactId&gt;avro-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">   &lt;version&gt;1.8.2&lt;&#x2F;version&gt;</span><br><span class="line">   &lt;executions&gt;</span><br><span class="line">       &lt;execution&gt;</span><br><span class="line">           &lt;phase&gt;generate-sources&lt;&#x2F;phase&gt;</span><br><span class="line">           &lt;goals&gt;</span><br><span class="line">               &lt;goal&gt;schema&lt;&#x2F;goal&gt;</span><br><span class="line">           &lt;&#x2F;goals&gt;</span><br><span class="line">           &lt;configuration&gt;</span><br><span class="line">               &lt;sourceDirectory&gt;$&#123;project.basedir&#125;&#x2F;src&#x2F;main&#x2F;resources&#x2F;&lt;&#x2F;sourceDirectory&gt;</span><br><span class="line">               &lt;outputDirectory&gt;$&#123;project.basedir&#125;&#x2F;src&#x2F;main&#x2F;java&#x2F;&lt;&#x2F;outputDirectory&gt;</span><br><span class="line">           &lt;&#x2F;configuration&gt;</span><br><span class="line">       &lt;&#x2F;execution&gt;</span><br><span class="line">   &lt;&#x2F;executions&gt;</span><br><span class="line">&lt;&#x2F;plugin&gt;</span><br></pre></td></tr></table></figure>
<h3 id="第三种方式"><a href="#第三种方式" class="headerlink" title="第三种方式"></a>第三种方式</h3><p>ParquetAvroWriters.forGenericRecord(“schema”)<br>传入一个avsc的文件进去即可。</p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>druid</title>
    <url>/2020/05/11/druid/</url>
    <content><![CDATA[<p>分布式的内存OLAP（On-Line Analytical Processing） 系统，推荐需要做预聚合的数据接入。<br>常用作需要快速聚合的高度并发API的后端。</p>
<p>Druid 的核心特性主要包括：<br>列式存储。列式存储的优势在于查询的时候可以只返回指定的列的数据，其次同一列数据往往具有很多共性，这带来另一个好处就是存储的时候压缩效果比较好。<br>可扩展的分布式架构。<br>并行计算。<br>数据摄入支持实时和批量。这里的实时的意思是输入摄入即可查。如果大家看过我之前关于实时计算的文章，应该猜到了这就是典型的 lambda 架构，后面再细说。<br>运维友好。<br>云原生架构，高容错性。<br>支持索引，便于快速查询。<br>基于时间的分区<br>自动聚合。</p>
<p>其中很多特性其实应该算是 OLAP 系统的共同特性，比如列式存储等。不像大多数传统系统，Druid 可以在数据摄入前对数据进行预聚合。这种预聚合操作被称之为 rollup，这样就可以显著的节省存储成本。</p>
<p>Druid：是一个实时处理时序数据的OLAP数据库，因为它的索引首先按照时间分片，查询的时候也是按照时间线去路由索引。<br>Kylin：核心是Cube，Cube是一种预计算技术，基本思路是预先对数据作多维索引，查询时只扫描索引而不访问原始数据从而提速。<br>ES：最大的特点是使用了倒排索引解决索引问题。根据研究，ES在数据获取和聚集用的资源比在Druid高。<br>Spark SQL：基于Spark平台上的一个OLAP框架，基本思路是增加机器来并行计算，从而提高查询速度。</p>
<h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>大比例的插入操作，少量的更新操作<br>大部分查询应用聚合和报告查询使用group by、查询或者扫描操作<br>数据有一个时间列<br>load data from kafka HDFS Amazon S3</p>
<h3 id="摄入"><a href="#摄入" class="headerlink" title="摄入"></a>摄入</h3><p>Druid 提供两种数据摄入方式，其中一种是实时数据摄入kafka；另一种是批处理数据摄入hdfs、csv。<br>（摄入-概念）<a href="https://www.afenxi.com/41702.html" target="_blank" rel="noopener">https://www.afenxi.com/41702.html</a></p>
<p>（只适用于kafka）<br>通过提交配置json文件到druid的overlord使它可以消费druid。<br>提交可以用curl：<code>curl -L -X &#39;POST&#39; -H &#39;Content-Type:application/json&#39; -d @log_config_kis.json overlordLeader地址</code><br>json的具体格式参考：<a href="https://www.jianshu.com/p/1054ba0502df?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation" target="_blank" rel="noopener">https://www.jianshu.com/p/1054ba0502df?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation</a><br><a href="https://druid.apache.org/docs/latest/development/extensions-core/kafka-ingestion.html" target="_blank" rel="noopener">https://druid.apache.org/docs/latest/development/extensions-core/kafka-ingestion.html</a> （官网）</p>
<h3 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h3><p>  支持两种查询：JSON-HTTP，SQL两种方式<br>  CURL（<a href="http://druid.io/docs/0.10.1/querying/querying.html）、" target="_blank" rel="noopener">http://druid.io/docs/0.10.1/querying/querying.html）、</a><br>  jdbc（<a href="https://druid.apache.org/docs/0.14.2-incubating/querying/sql.html#jdbc）" target="_blank" rel="noopener">https://druid.apache.org/docs/0.14.2-incubating/querying/sql.html#jdbc）</a><br>查询类型:<br>    Timeseries:基于时间范围查询的类型<br>    TopN:基于单维度的排名查询<br>    GroupBy:基于多维度的分组查询</p>
<h3 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h3><p>时间序列<br>　时间序列（Timestamp），本身 Druid 是时间序列数据库，Druid 中所有查询以及索引过程都和时间维度有关。<br>Druid 底层使用绝对毫秒数保存时间戳，默认使用 ISO-8601 格式展示时间：yyyy-MM-ddThh:mm:sss.SSSZ<br>时间戳字段<code>&quot;format&quot; : &quot;auto&quot;</code> 时必须是long类型的，而且是毫秒。<br>看日志会有receivedCount=5, sentCount=5, droppedCount=0, unparseableCount=0  表示已经录入。</p>
<p>维度列<br>　维度列（Dimensions），Druid 的维度概念和广义的 OLAP 定义一致，一条记录中的字符、数值、多值等类型的数据均可看作是维度列。<br>维度列可用于过滤筛选（filter）、分组（group）数据</p>
<p>度量列<br>　度量列（Metrics），Druid 的度量概念也与广义的 OLAP 定义一致，一条记录中的数值（Numeric）类型数据可看作是度量列，<br>度量列被用于聚合（aggregation）和计算（computation）操作</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><a href="https://druid.apache.org/docs/0.18.1/design/architecture.html" target="_blank" rel="noopener">https://druid.apache.org/docs/0.18.1/design/architecture.html</a> （官网）<br><a href="https://blog.csdn.net/run_bigdata/article/details/101048408" target="_blank" rel="noopener">https://blog.csdn.net/run_bigdata/article/details/101048408</a> （讲的很好）<br><a href="https://yuzhouwan.com/posts/5845/" target="_blank" rel="noopener">https://yuzhouwan.com/posts/5845/</a> （也很不错，尤其架构那的图非常清晰）</p>
<p><img src="druid_architecture.png" alt=""><br>Coordinator：顾名思义，Coordinator 就是协调器，主要负责 segment 的分发确保segments自平衡等。比如我们只保存 30 天的数据，这个规则就是由 Coordinator 来定时执行的。</p>
<p>摄入：<br>Overlord：处理数据摄入的 task，将 task 提交到 MiddleManager。比如使用 Tranquility 做数据摄入的时候，每个 segment 都会生成一个对应的 task。<br>MiddleManager : MiddleManager 可以认为是一个任务调度进程，主要用来处理 Overload 提交过来的 task。每个 task 会以一个 JVM 进程启动。<br>Peon（苦力）Peons运行一个单独的任务在一个单独的JVM,MiddleManager负责创建执行任务的peon；peons自己运行是非常稀少的。</p>
<p>query：<br>Broker : 处理外部请求，并对结果进行汇总。<br>Router : Router 相当于多个 Broker 前面的路由，不是必须的。<br>Historical ：Historical 可以理解为将 segment 存储到本地，相当于 cache。相比于 Deep Storage 的，Historical 将 segment 直接存储到本地磁盘，只有 segment 存储到本地才能被查询。其实这个地方是有点异于直观感受的。正常我们可能会认为查询先查本地，如果本地没有数据才去查 Deep Storage，但是实际上如果本地没有相应的 segment，则查询是无法查询的。Historical 处理那些 segment 是由 Coordinator 指定的，但是 Historical 并不会和 Coordinator 直接交互，而是通过 Zookeeper 来解耦。<br>Deep storage：一个被druid可访问的共享的文件存储；比如分布式文件系统HDFS、S3、一个网络挂在的文件系统；用它来存储已经陪摄入的任何数据；</p>
<p>MiddleManager &amp; Historical数据服务。<br>Queries: Routers 将请求路由到 Broker，Broker 向 MiddleManager 和 Historical 进行数据查询。这里 MiddleManager 主要负责查询正在进行摄入的数据查询，比如现在正在摄入 12:00 ~ 13:00 的数据，那么我们要查询就去查询 MiddleManager，MiddleManager 再将请求分发到具体的 peon，也就是 task 的运行实体上。而历史数据的查询是通过 Historical 查询的，然后数据返回到 Broker 进行汇总。这里需要注意的时候数据查询并不会落到 Deep Storage 上去，也就是查询的数据一定是 cache 到本地磁盘的。很多人一个直观理解查询的过程应该是先查询 Historical，Historical 没有这部分数据则去查 Deep Storage。Druid 并不是这么设计的。</p>
<p>Data/Segment: 这里包括两个部分，MiddleManager 的 task 在结束的时候会将数据写入到 Deep Storage，这个过程一般称作 Segment Handoff。然后 Historical 定期的去下载 Deep Storage 中的 segment 数据到本地。</p>
<p>Metadata: Druid 的元数据主要存储到两个部分，一个是 Metadata Storage，这个一般是 MySQL 等关系型数据库；另一个是 Zookeeper。下图是 Druid 在 Zookeeper 中的 znode。zk 的作用主要是用来给各个组件进行解耦。</p>
<p>总结<br>Historical是历史数据摄取和查询到节点，Coordinator监控协调Historical节点上的任务，确保segments自平衡；<br>MiddleManager是一个新数据摄取和查询的节点；overlord监控和协调task任务的分配和segments的发布。<br>三种托管计划： “Data” servers run Historical and MiddleManager processes.<br>“Query” servers run Broker and (optionally) Router processes.<br>“Master” servers run Coordinator and Overlord processes. They may run ZooKeeper as well.</p>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p><a href="https://www.jianshu.com/p/1054ba0502df?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation" target="_blank" rel="noopener">https://www.jianshu.com/p/1054ba0502df?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation</a></p>
<p>时间列 必须有。format可以是iso, millis, posix，或者auto<br><a href="https://druid.apache.org/docs/0.18.1/tutorials/tutorial-ingestion-spec.html#rollup" target="_blank" rel="noopener">https://druid.apache.org/docs/0.18.1/tutorials/tutorial-ingestion-spec.html#rollup</a><br>通过设置roll-up 为false不做预聚合，但是不推荐使用。</p>
<p>“segmentGranularity” : “HOUR”, 指一个segment应该包含多大时间间隔的数据<br>“queryGranularity”:”minute” 查询的力度，设置之后druid摄入数据的时间序列会约等。”timestamp”:”2018-01-01T01:02:14Z” —-》2018-01-01T01:02:00.000Z<br>reportParseExceptions默认是false，如果开启这个功能，当摄入数据过程中出现数据异常将会导致摄入数据停止。<br>一个task可能消费一个或多个kafka partition,partition的编号被哪个task消费存在这样的一个映射关系：Id = partition % taskCount</p>
<p>Druid 通过 Indexing 处理将原始数据以 segment 的方式存储在数据节点，segment 是一种查询优化的数据结构。<br>Kafka索引任务存在两种状态, reading 状态和publish状态，当task读取数据到达duration配置的时间，则进行publish状态，publish也会持续completionTimeout 时间，当task进入publish状态的时候立马又创建下一轮的任务开始从上一轮的task消费到的位置开始reading，这么一直不停地交错进行。Supervisor 也维护这两个队列用于存放两种状态的task，并且还维护一个全局的kafka 分区与offset的映射关系表：<br><a href="https://blog.csdn.net/weixin_38736107/article/details/98474160" target="_blank" rel="noopener">https://blog.csdn.net/weixin_38736107/article/details/98474160</a></p>
<p><a href="https://druid.apache.org/docs/0.14.2-incubating/development/extensions-core/kafka-ingestion.html#submitting-a-supervisor-spec" target="_blank" rel="noopener">https://druid.apache.org/docs/0.14.2-incubating/development/extensions-core/kafka-ingestion.html#submitting-a-supervisor-spec</a><br><a href="https://druid.apache.org/docs/latest/development/extensions-core/kafka-ingestion.html" target="_blank" rel="noopener">https://druid.apache.org/docs/latest/development/extensions-core/kafka-ingestion.html</a><br>lateMessageRejectionPeriod<br>Configure tasks to reject messages with timestamps earlier than this period before the task was created; for example if this is set to PT1H and the supervisor creates a task at 2016-01-01T12:00Z, messages with timestamps earlier than 2016-01-01T11:00Z will be dropped. This may help prevent concurrency issues if your data stream has late messages and you have multiple pipelines that need to operate on the same segments (e.g. a realtime and a nightly batch ingestion pipeline).</p>
<p>earlyMessageRejectionPeriod<br>Configure tasks to reject messages with timestamps later than this period after the task reached its taskDuration; for example if this is set to PT1H, the taskDuration is set to PT1H and the supervisor creates a task at 2016-01-01T12:00Z, messages with timestamps later than 2016-01-01T14:00Z will be dropped. Note: Tasks sometimes run past their task duration, for example, in cases of supervisor failover. Setting earlyMessageRejectionPeriod too low may cause messages to be dropped unexpectedly whenever a task runs past its originally configured task duration.</p>
<p>taskDuration: 间隔多长时间任务停止read开始publish。从任务开始启动后开始计数，一般设成1h<br>completionTimeout：publish的持续最长时间，超时时任务会失败。时间从taskDuration时间结束开始。</p>
<h3 id="出错"><a href="#出错" class="headerlink" title="出错"></a>出错</h3><p>druid集群从0.14.2升级到0.18.1，新版本不兼容下面Fastjon的 第一种写法，改成第二种解决。<br>第一种会把格式序列化进去，即一个json不再是一行而是多行的可视化展示，druid的新版本会把没一行当成一个json。。。<br>（具体源码没有研究过，通过报错的log猜出来的原因，尝试后druid可消费新数据。）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 将JavaBean序列化为带格式的JSON文本</span><br><span class="line">public static final String toJSONString(Object object, boolean prettyFormat);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 将JavaBean序列化为JSON文本</span><br><span class="line">public static final String toJSONString(Object object);</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://zhuanlan.zhihu.com/p/133156613" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/133156613</a></p>
<p>官方文档：<a href="http://druidio.cn/docs/0.9.0/tutorials/tutorial-kafka.html" target="_blank" rel="noopener">http://druidio.cn/docs/0.9.0/tutorials/tutorial-kafka.html</a><br>取数文档：<a href="http://lxw1234.com/archives/2015/11/561.htm" target="_blank" rel="noopener">http://lxw1234.com/archives/2015/11/561.htm</a></p>
<p>（原理）<a href="https://yuzhouwan.com/posts/5845/" target="_blank" rel="noopener">https://yuzhouwan.com/posts/5845/</a></p>
<p>数据模型：<a href="http://lxw1234.com/archives/2018/01/890.htm" target="_blank" rel="noopener">http://lxw1234.com/archives/2018/01/890.htm</a><br>druid intervals inclusive：Left (start) is inclusive and right (end) is exclusive.<br>it is left closed, right open</p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>flink数据序列化</title>
    <url>/2020/05/11/flink%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96/</url>
    <content><![CDATA[<p>Flink 的类型分类<br><img src="ser.png" alt=""><br>Flink 的类型系统源码位于 org.apache.flink.api.common.typeinfo 包,类的继承关系:<br><img src="ser2.png" alt=""></p>
<p>TypeInformation<br>TypeInformation类是所有类型描述符的基类，它和它的所有子类必须可序列化（Serializable），因为类型信息将会伴随 Flink 的作业提交，被传递给每个执行节点。<br>如，POJO在Flink内部使用PojoTypeInfo来表示，PojoTypeInfo继承自CompositeType，CompositeType继承自TypeInformation。TypeInformation的一个重要的功能就是创建TypeSerializer序列化器，为该类型的数据做序列化。每种类型都有一个对应的序列化器来进行序列化。</p>
<p>Flink会自动探测传入的数据类型，生成对应的TypeInformation，调用对应的序列化器。比如，Flink的map函数Scala签名为：<code>def map[R: TypeInformation](fun: T =&gt; R): DataStream[R]</code>，传入map的数据类型是T，生成的数据类型是R，Flink会推测T和R的数据类型，并使用对应的序列化器进行序列化。<br>以一个字符串String类型为例，Flink首先推断出该类型，并生成对应的TypeInformation，然后在序列化时调用对应的序列化器，将一个内存对象写入内存块。<br><img src="flink.jpeg" alt=""><br>手动创建typeInformation<br><code>import org.apache.flink.streaming.api.scala._ //must import</code> 如果使用flink-sql，引入<code>import org.apache.flink.table.api.scala._</code><br>1）<code>TypeInformation.of(classOf[SensorReading])</code> or <code>TypeInformation.of(new TypeHint[(String, Array[Byte])] {})</code><br>2）<code>import org.apache.flink.streaming.api.scala.createTypeInformation  createTypeInformation[SensorReading]</code><br>3）<code>BasicTypeInfo.STRING_TYPE_INFO</code></p>
<p>flink会尝试识别出更多的 在分布式计算框架上传递和存储的 数据类型；比如把它当作是一个推断 表元数据 的数据库。大多数情况下，flink可以自己识别到所有信息。<br>有这些类型信息flink可以做：<br>1、用POJOs做grouping / joining / aggregating时可以用字段名(like dataSet.keyBy(“username”)).这个可以让flink在执行之前就识别到一些类型的错误。<br>2、flink可以更好的做序列化和数据布局。这对flink的内存优化非常重要。尽可能处理堆内部/外部的序列化数据，会使序列化非常容易。<br>3、<br>经常出现的问题：<br>1、注册子类。使用继承的类时，需要用<code>StreamExecutionEnvironment or ExecutionEnvironment .registerType(clazz)</code>注册子类，否则flink会多花很多时间来识别这个子类<br>2、自定义类型。对于用于自定义的数据类型，flink无法透明识别时，会回退到Kyro来识别这个类。而且并不是所有的数据类型Kyro都可以识别（flink同）。比如Google Guava里的集合类collection 都无法被自动识别。可以通过自定义序列化类来使它被识别。<code>getConfig().addDefaultKryoSerializer(clazz, serializer) on the StreamExecutionEnvironment or ExecutionEnvironment</code>.一般情况下很多包里已经自带了序列化类，比如<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/custom_serializers.html" target="_blank" rel="noopener">protoBuffer、Thrift</a>。代码可看下面<br>3、增加Type Hints。这种一般只在Java API中需要。有时候使用了所有的办法都无法识别数据类型，这种时候需要指定一个数据<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/types_serialization.html#type-hints-in-the-java-api" target="_blank" rel="noopener">暗示Hints<code>.returns(SomeType.class);</code></a>。<br><code>dataSet.map(new MyGenericNonInferrableFunction&lt;Long, SomeType&gt;()).returns(SomeType.class);</code>returns指定生产类型。<br>returns() 接受三种类型的参数：<br>  (1)字符串描述的类名（例如 “String”）字符串形式的用法即将废弃，如果确实有必要，请使用 Class.forName() 等方法来解决。<br>  (2)TypeHint（接下来会讲到，用于泛型类型参数）<br>  (3)Java 原生 Class（例如 String.class) 等<br>TypeHint支持：<br> （1）类，无参<br> （2）<code>returns(new TypeHint&lt;Tuple2&lt;Integer, SomeType&gt;&gt;(){}).</code>TypeHint类可以捕获通用类型信息并将其保留到运行时。</p>
<p>4、<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/types_serialization.html#creating-a-typeinformation-or-typeserializer" target="_blank" rel="noopener">手动创建 TypeInformation</a>。一般在由于Java的通用类型擦除 flink无法识别出 时使用。</p>
<p>protoBuffer、Thrift序列化：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">final ExecutionEnvironment env &#x3D; ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; register the Google Protobuf serializer with Kryo</span><br><span class="line">env.getConfig().registerTypeWithKryoSerializer(MyCustomType.class, ProtobufSerializer.class);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; register the serializer included with Apache Thrift as the standard serializer</span><br><span class="line">&#x2F;&#x2F; TBaseSerializer states it should be initialized as a default Kryo serializer</span><br><span class="line">env.getConfig().addDefaultKryoSerializer(MyCustomType.class, TBaseSerializer.class);</span><br><span class="line"></span><br><span class="line">&lt;!-- Thrift --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;com.twitter&lt;&#x2F;groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;chill-thrift&lt;&#x2F;artifactId&gt;</span><br><span class="line">	&lt;version&gt;0.7.6&lt;&#x2F;version&gt;</span><br><span class="line">	&lt;!-- exclusions for dependency conversion --&gt;</span><br><span class="line">	&lt;exclusions&gt;</span><br><span class="line">		&lt;exclusion&gt;</span><br><span class="line">			&lt;groupId&gt;com.esotericsoftware.kryo&lt;&#x2F;groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;kryo&lt;&#x2F;artifactId&gt;</span><br><span class="line">		&lt;&#x2F;exclusion&gt;</span><br><span class="line">	&lt;&#x2F;exclusions&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;!-- libthrift is required by chill-thrift --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.apache.thrift&lt;&#x2F;groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;libthrift&lt;&#x2F;artifactId&gt;</span><br><span class="line">	&lt;version&gt;0.11.0&lt;&#x2F;version&gt;</span><br><span class="line">	&lt;exclusions&gt;</span><br><span class="line">		&lt;exclusion&gt;</span><br><span class="line">			&lt;groupId&gt;javax.servlet&lt;&#x2F;groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;servlet-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">		&lt;&#x2F;exclusion&gt;</span><br><span class="line">		&lt;exclusion&gt;</span><br><span class="line">			&lt;groupId&gt;org.apache.httpcomponents&lt;&#x2F;groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;httpclient&lt;&#x2F;artifactId&gt;</span><br><span class="line">		&lt;&#x2F;exclusion&gt;</span><br><span class="line">	&lt;&#x2F;exclusions&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- protoBuf --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;com.twitter&lt;&#x2F;groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;chill-protobuf&lt;&#x2F;artifactId&gt;</span><br><span class="line">	&lt;version&gt;0.7.6&lt;&#x2F;version&gt;</span><br><span class="line">	&lt;!-- exclusions for dependency conversion --&gt;</span><br><span class="line">	&lt;exclusions&gt;</span><br><span class="line">		&lt;exclusion&gt;</span><br><span class="line">			&lt;groupId&gt;com.esotericsoftware.kryo&lt;&#x2F;groupId&gt;</span><br><span class="line">			&lt;artifactId&gt;kryo&lt;&#x2F;artifactId&gt;</span><br><span class="line">		&lt;&#x2F;exclusion&gt;</span><br><span class="line">	&lt;&#x2F;exclusions&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;!-- We need protobuf for chill-protobuf --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;com.google.protobuf&lt;&#x2F;groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;protobuf-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">	&lt;version&gt;3.7.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<p>Flink支持的数据类型：<br>1、Java的基础类型和装箱（Integer等）<br>2、基础类型数组和Object数组<br>3、合成类型：<br>  （1）Java 元组Tuples（Flink Java API）；最长25个字段，不允许null<br>  （2）Scala case classes （包括scala的元组）；不允许null<br>  （3）Row。允许null<br>  （4）POJOs。<br>4、辅助类型。(Option, Either, Lists, Maps, …)<br>5、范型和其他类型Generic Kyro<br>第5个Kyro是最后的备选，尽量优化不要使用Kyro，会有大量的性能损耗。当不满足前四种时，就会被Flink认为是一种泛型（GenericType）,使用Kryo来进行序列化和反序列化。但Kryo在有些流处理场景效率非常低，有可能造成流数据的积压。我们可以使用<code>senv.getConfig.disableGenericTypes()</code>来禁用Kryo，禁用后，Flink遇到无法处理的数据类型将抛出异常，这种方法对于调试非常有效。<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/types_serialization.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/types_serialization.html</a></p>
<p>注意：scala的元组从_1开始，为Java准备的元组Tuple从0开始。</p>
<p>FLink POJOs比较强大，必须满足的条件：<br>1、类是个public的独立类（没有非静态内部类）<br>2、必须有公共的无参构造器<br>3、类中的所有非静态，非瞬态成员都是公共public的，不能是final，否则有对应的getter、setter方法<br>通过<code>env.registerType(BaseLog.class);</code>或者<code>env.getConfig().registerPojoType(BaseLog.class)</code>注册<br>通过<code>System.out.println(TypeInformation.of(BaseLog.class).createSerializer(new ExecutionConfig()));</code>的输出可以判断自定义类是否是POJO，kyro不是POJO。<br>org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer@fe305308<br>org.apache.flink.api.java.typeutils.runtime.PojoSerializer@f1db634d</p>
<p>参考：<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/types_serialization.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/types_serialization.html</a> （官网，建议）<br><a href="https://cloud.tencent.com/developer/article/1240444" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1240444</a><br><a href="https://blog.csdn.net/qq_42596142/article/details/103763612" target="_blank" rel="noopener">https://blog.csdn.net/qq_42596142/article/details/103763612</a><br><a href="https://blog.csdn.net/shufangreal/article/details/105916100" target="_blank" rel="noopener">https://blog.csdn.net/shufangreal/article/details/105916100</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>flink</title>
    <url>/2020/05/11/flink/</url>
    <content><![CDATA[<h3 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h3><p><img src="jm.jpeg" alt=""><br>主要有 TaskManager，JobManager，Client三种角色。<br>1、Client是Flink程序提交的客户端，当用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager。<br>2、JobManager扮演着集群中的管理者Master的角色，它是整个集群的协调者，负责接收Flink Job，协调检查点，Failover 故障恢复等，同时管理Flink集群中从节点TaskManager。<br>3、TaskManager是实际负责执行计算的Worker，在其上执行Flink Job的一组Task，每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。<br><img src="slot.jpeg" alt=""><br>TaskManager是实际负责执行计算的Worker，TaskManager 是一个 JVM 进程，并会以独立的线程来执行一个task或多个subtask。为了控制一个 TaskManager 能接受多少个 task，Flink 提出了 Task Slot 的概念。</p>
<p>简单的说，TaskManager会将自己节点上管理的资源分为不同的Slot：固定大小的资源子集。这样就避免了不同Job的Task互相竞争内存资源，但是需要主要的是，Slot只会做内存的隔离。没有做CPU的隔离。</p>
<h3 id="执行计划"><a href="#执行计划" class="headerlink" title="执行计划"></a>执行计划</h3><p><code>env.getExecutionPlan</code> 返回任务的执行计划，结构是json，粘贴到<code>https://flink.apache.org/visualizer/</code>flink的可视化官网可以看到流程图<br>StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图<br>StreamGraph, JobGraph 这两个执行图在 client 端生成。ExecutionGraph 在 JobManager 中生成。<br>Client 向 JobManager 提交 JobGraph 后， JobManager 就会根据 JobGraph 来创建对应的 ExecutionGraph,并以此来调度任务。<br>源码:StreamingJobGraphGenerator 就是讲StreamGraph转换为JobGraph<br>    ExecutionGraphBuilder.buildGraph 构建ExecutionGraph<br>    SchedulerBase</p>
<p>StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。<br>JobGraph：StreamGraph经过优化后生成了 JobGraph，然后提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 串联 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。<br>ExecutionGraph：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。<br>物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</p>
<p><a href="https://blog.jrwang.me/2019/flink-source-code-executiongraph/" target="_blank" rel="noopener">https://blog.jrwang.me/2019/flink-source-code-executiongraph/</a></p>
<h3 id="Operator-Chains"><a href="#Operator-Chains" class="headerlink" title="Operator Chains"></a>Operator Chains</h3><p>来源：<a href="https://blog.csdn.net/u013343882/article/details/82355860" target="_blank" rel="noopener">https://blog.csdn.net/u013343882/article/details/82355860</a></p>
<p>为了更高效地分布式执行，Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。</p>
<p>我们以经典的 WordCount 为例，下面这幅图，展示了Source并行度为1，FlatMap、KeyAggregation、Sink并行度均为2，最终以5个并行的线程来执行的优化过程。<br><img src="chain.jpeg" alt=""><br>上图中将KeyAggregation和Sink两个operator进行了合并，因为这两个合并后并不会改变整体的拓扑结构。但是，并不是任意两个 operator 就能 chain 一起的。其条件还是很苛刻的：<br>1、上下游的并行度一致<br>2、下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入）<br>3、上下游节点都在同一个 slot group 中（下面会解释 slot group）<br>4、下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS）<br>5、上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD）<br>6、两个节点间数据分区方式是 forward<br>7、用户没有禁用 chain<br>Operator chain的行为可以通过编程API中进行指定。可以通过在DataStream的operator后面（如someStream.map(..))调用<code>startNewChain()</code>来指示从该operator开始一个新的chain（与前面截断，不会被chain到前面）。或者调用<code>disableChaining()</code>来指示该operator不参与chaining（不会与前后的operator chain一起）。在底层，这两个方法都是通过调整operator的 chain 策略（HEAD、NEVER）来实现的。另外，也可以通过调用<code>StreamExecutionEnvironment.disableOperatorChaining()</code>来全局禁用chaining。</p>
<h3 id="key的指定方法"><a href="#key的指定方法" class="headerlink" title="key的指定方法"></a>key的指定方法</h3><p>1、根据字段位置。主要对tuple类型，pojo类会出错；不能指定嵌套里面的位置<br>2、根据字段名称，主要是pojo类。但是对sum这种聚合函数都可以指定字段位置和名称，如果是位置则是从0开始<br>3、自定义keyselector<br>不能作为key的：<br>1、POJO类型但不覆盖hashCode（）方法并依赖于Object.hashCode（）实现<br>2、任何类型的数组<br>用户代码的process function中改变了keyby的key的值会导致窗口状态无法清理<br>在keyby的时候key一定要是不变量，不然有可能导致状态无法清理。还有就是在分布式系统中，大量使用不变量是规避风险的最佳途径之一。</p>
<p><a href="https://blog.csdn.net/weixin_42412645/article/details/92196433?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42412645/article/details/92196433?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase</a></p>
<h3 id="operates"><a href="#operates" class="headerlink" title="operates"></a>operates</h3><h4 id="KeyedStream"><a href="#KeyedStream" class="headerlink" title="KeyedStream"></a>KeyedStream</h4><p>KeyedStream用来表示根据指定的key进行分组的数据流。一个KeyedStream可以通过调用DataStream.keyBy()来获得。<br>而在KeyedStream上进行任何transformation都将转变回DataStream。<br>在实现中，KeyedStream是把key的信息写入到了transformation中。每条记录只能访问所属key的状态，其上的聚合函数可以方便地操作和保存对应key的状态。</p>
<h4 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h4><p>用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。<br>window 又可以分为基于时间（Time-based）的 window 以及基于数量（Count-based）的 window。</p>
<h5 id="WindowedStream"><a href="#WindowedStream" class="headerlink" title="WindowedStream"></a>WindowedStream</h5><p>WindowedStream代表了根据key分组，并且基于WindowAssigner切分窗口的数据流。所以WindowedStream都是从KeyedStream衍生而来的。而在WindowedStream上进行任何transformation也都将转变回DataStream。<br>=======》即，一般窗口必须在keyby之后；</p>
<h5 id="AllWindowedStream"><a href="#AllWindowedStream" class="headerlink" title="AllWindowedStream"></a>AllWindowedStream</h5><p>但是还有AllWindowedStream，不需要是keyBy之后，可在普通流上操作<br>警告：在许多情况下，这是<code>非并行转换</code>。所有记录将收集在windowAll 算子的一个任务中。<br>所有数据将发送给下游的单个实例，或者说下游算子的并行度为1。</p>
<h5 id="timeWindow"><a href="#timeWindow" class="headerlink" title="timeWindow"></a>timeWindow</h5><p>flink中keyedStream中还有一个timeWindow方法，这个方法是在第一种window的基础上做的封装<br>看源码，timeWindow一个参数是滚动，两个参数是滑动</p>
<h5 id="countWindow"><a href="#countWindow" class="headerlink" title="countWindow"></a>countWindow</h5><p>积攒是针对<code>每个key值</code>。<br>例子：<a href="https://blog.csdn.net/vincent_duan/article/details/102619887" target="_blank" rel="noopener">https://blog.csdn.net/vincent_duan/article/details/102619887</a></p>
<h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><p>Tumbling Window的窗口划分是和窗口大小对齐的</p>
<p><a href="http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/（讲的很好！！！！）" target="_blank" rel="noopener">http://wuchong.me/blog/2016/05/20/flink-internals-streams-and-operations-on-streams/（讲的很好！！！！）</a><br><a href="https://www.jianshu.com/p/68463ff911d9" target="_blank" rel="noopener">https://www.jianshu.com/p/68463ff911d9</a></p>
<h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h4><p>reduce需要针对分组或者一个window(窗口)来执行，也就是分别对应于keyBy、window/timeWindow 处理后的数据，<br>根据ReduceFunction将元素与上一个reduce后的结果合并，产出合并之后的结果。<br>跟在窗口后面，输出为窗口计算的总值；跟在keyBy之后则是每条数据都输出。</p>
<p>链接：<a href="https://www.jianshu.com/p/0cdf1112d995" target="_blank" rel="noopener">https://www.jianshu.com/p/0cdf1112d995</a></p>
<h4 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h4><p>跟reduce一样，针对分组或者一个window(窗口)来执行，也就是分别对应于keyBy、window/timeWindow 处理后的数据，单其实1.8的sdk已经指出在以后为去掉这个方法<br>有一个初始值，计算初始值+new_value,看下面官网例子：<br>A fold function that, when applied on the sequence (1,2,3,4,5), emits the sequence “start-1”, “start-1-2”, “start-1-2-3”, …</p>
<p><code>val result: DataStream[String] = keyedStream.fold(&quot;start&quot;)((str, i) =&gt; { str + &quot;-&quot; + i })</code></p>
<h4 id="union-amp-connect-amp-split"><a href="#union-amp-connect-amp-split" class="headerlink" title="union &amp; connect &amp; split"></a>union &amp; connect &amp; split</h4><p>union可以将多个流合并到一个流中，以便对合并的流进行统一处理。是对多个流的水平拼接。需要数据类型一样。<br>经验例子：多个source，且kafka的分区不同要分别setParallelism，可以在设置分区后使用union到一起处理</p>
<p>connect将两个流纵向地连接起来，不需要数据类型一样。</p>
<p>split把一个流拆成多个流<br>例子参考：<a href="https://blog.csdn.net/chybin500/article/details/87260869" target="_blank" rel="noopener">https://blog.csdn.net/chybin500/article/details/87260869</a> （java）<br>官网：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/</a> （scala）</p>
<h4 id="SideOutput"><a href="#SideOutput" class="headerlink" title="SideOutput"></a>SideOutput</h4><p>一个流变多个流<br>使用侧输出SideOutput必须使用 ProcessFunction、CoProcessFunction、ProcessWindowFunction、ProcessAllWindowFunction，因为它们暴漏了Context参数给用户，让用户可以将数据通过outputtag发给侧输出流。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val outputTag: OutputTag[java.util.List[String]] &#x3D; new OutputTag[java.util.List[String]](&quot;delay-behavior&quot;)</span><br><span class="line">ctx.output(outputTag, delayKey)</span><br></pre></td></tr></table></figure>
<p>官网指路：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/side_output.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/side_output.html</a></p>
<h3 id="三种time类型"><a href="#三种time类型" class="headerlink" title="三种time类型"></a>三种time类型</h3><p>Processing Time、Event Time、Ingestion Time<br><code>Event Time</code> 时间发生时时间，自定义.必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制；需要等待一些时间，确保一些无序事件都被处理掉，也就导致了会出现延迟。<br><code>Processing Time</code> 代码执行时机器的时间；最好的性能和最低的延迟，分布式和异步的环境下不能提供确定性<br><code>Ingestion Time</code> 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。具有自动分配时间戳和自动生成水印功能。</p>
<h3 id="kafka默认分区FlinkFixedPartitioner"><a href="#kafka默认分区FlinkFixedPartitioner" class="headerlink" title="kafka默认分区FlinkFixedPartitioner"></a>kafka默认分区FlinkFixedPartitioner</h3><p>没有自定义分区时，使用的默认分区方式 <code>FlinkFixedPartitioner</code><br>这个里面最重要的实现<code>parallelInstanceId % partitions.length</code><br>parallelInstanceId表示当前task的index，partitions表示kafka的topic的分区; 该逻辑求得的分区就是根据当前task index 对partition取余得到的.<br>eg:<br>flink并行度为3（F0，F1，F2），partition数量为2（P0，P1），则F0-&gt;P0,F1-&gt;P1,F2-&gt;P0<br>flink并行度为2（F0，F1），partition数量为3（P0，P1，P2），则F0-&gt;P0,F1-&gt;P1<br>！！！重要问题：<br>1、当 topic 的 partition 扩容时，则需要重启作业，以便发现新的 partition。<br>2、当<code>parallelInstanceId &lt; partitions.length</code> 会导致部分分区完全没有数据;如假设sinkParallelism是4，topic的分区数是6，取余永远不会得到4、5，所以就导致分区4、5一直没有数据写入。</p>
<p>参考：<a href="https://www.jianshu.com/p/222afef21c52" target="_blank" rel="noopener">https://www.jianshu.com/p/222afef21c52</a><br><a href="https://blog.csdn.net/weidaoyouwen/article/details/85233032" target="_blank" rel="noopener">https://blog.csdn.net/weidaoyouwen/article/details/85233032</a></p>
<h3 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h3><p>Flink源码中有一个独立的connector模块，所有的其他connector都依赖于此模块，Flink 在1.9版本发布的全新kafka连接器，摒弃了之前连接不同版本的kafka集群需要依赖不同版本的connector这种做法，只需要依赖一个connector即可。</p>
<p>参考（源码以及各种配置参数，写的很好）：<a href="https://jiamaoxiang.top/2020/04/02/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F/" target="_blank" rel="noopener">https://jiamaoxiang.top/2020/04/02/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F/</a><br>开启checkpoint的情况下指定offset从最早、最晚开始消费：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 指定kafka的消费者从哪里开始消费数据</span><br><span class="line">&#x2F;&#x2F; 共有三种方式，</span><br><span class="line">&#x2F;&#x2F; #earliest</span><br><span class="line">&#x2F;&#x2F; 当各分区下有已提交的offset时，从提交的offset开始消费；</span><br><span class="line">&#x2F;&#x2F; 无提交的offset时，从头开始消费</span><br><span class="line">&#x2F;&#x2F; #latest</span><br><span class="line">&#x2F;&#x2F; 当各分区下有已提交的offset时，从提交的offset开始消费；</span><br><span class="line">&#x2F;&#x2F; 无提交的offset时，消费新产生的该分区下的数据</span><br><span class="line">&#x2F;&#x2F; #none</span><br><span class="line">&#x2F;&#x2F; topic各分区都存在已提交的offset时，</span><br><span class="line">&#x2F;&#x2F; 从offset后开始消费；</span><br><span class="line">&#x2F;&#x2F; 只要有一个分区不存在已提交的offset，则抛出异常</span><br><span class="line">props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);</span><br></pre></td></tr></table></figure>
<p>FlinkKafkaConsumer源码：<br>1、<code>FlinkKafkaConsumer</code>在<code>createFetcher</code> 方法中<code>adjustAutoCommitConfig(properties, offsetCommitMode)</code>(实现在父类中)它主要做的是判断如果开启了checkpoint或者<code>consumer.setCommitOffsetsOnCheckpoints(true))</code>(oncheckpoint模式，默认为true)时，把kafka的自动提交offset功能关闭，kafka properties的enable.auto.commit强制置为false，覆盖操作。<br><code>FlinkKafkaConsumer</code>在<code>createFetcher</code> 方法的作用是返回一个fetcher实例，fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)<br>2、<code>getIsAutoCommitEnabled</code>判断是否在kafka的参数开启了自动提交，即enable.auto.commit=true，并且auto.commit.interval.ms&gt;0.<br>注意：如果没有没有设置enable.auto.commit的参数，则默认为true, 如果没有设置auto.commit.interval.ms的参数，则默认为5000毫秒。该方法会在FlinkKafkaConsumerBase的open方法进行初始化的时候调用。<br>3、<code>setDeserializer</code>确保配置了kafka消息的key与value的反序列化方式，如果没有配置，则使用ByteArrayDeserializer序列化器，<br>ByteArrayDeserializer类的deserialize方法是直接将数据进行return，未做任何处理。</p>
<p>OffsetCommitMode kafka offset的三种提交模式<br>offset的提交模式,有三种，分别为：<br>DISABLED(禁用偏移量自动提交)<br>ON_CHECKPOINTS(仅仅当checkpoints完成之后，才提交偏移量给kafka)<br>KAFKA_PERIODIC(使用kafka自动提交函数，周期性自动提交偏移量)</p>
<p>配置 offset 提交行为的方法是否相同，取决于是否为 job 启用了 checkpointing。在这里先给出提交模式的具体结论，下面会对两种方式进行具体的分析。基本的结论为：</p>
<ul>
<li><p>开启checkpoint</p>
<ul>
<li><p>情况1：用户通过调用 consumer 上的 setCommitOffsetsOnCheckpoints(true) 方法来启用 offset 的提交(默认情况下为 true )<br>那么当 checkpointing 完成时，Flink Kafka Consumer 将提交的 offset 存储在 checkpoint 状态中。<br>这确保 Kafka broker 中提交的 offset 与 checkpoint 状态中的 offset 一致。<br>注意，在这个场景中，Properties 中的自动定期 offset 提交设置会被完全忽略。<br>此情况使用的是ON_CHECKPOINTS</p>
</li>
<li><p>情况2：用户通过调用 consumer 上的 setCommitOffsetsOnCheckpoints(“false”) 方法来禁用 offset 的提交，则使用DISABLED模式提交offset</p>
</li>
</ul>
</li>
<li><p>未开启checkpoint<br>Flink Kafka Consumer 依赖于内部使用的 Kafka client 自动定期 offset 提交功能，因此，要禁用或启用 offset 的提交</p>
<ul>
<li><p>情况1：配置了Kafka properties的参数配置了”enable.auto.commit” = “true”或者 Kafka 0.8 的 auto.commit.enable=true，使用KAFKA_PERIODIC模式提交offset，即自动提交offset</p>
</li>
<li><p>情况2：没有配置enable.auto.commit参数，使用DISABLED模式提交offset，这意味着kafka不知道当前的消费者组的消费者每次消费的偏移量。</p>
</li>
</ul>
</li>
</ul>
<p><code>OffsetCommitModes</code>提交模式的调用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public static OffsetCommitMode fromConfiguration(</span><br><span class="line">		boolean enableAutoCommit,</span><br><span class="line">		boolean enableCommitOnCheckpoint,</span><br><span class="line">		boolean enableCheckpointing) &#123;</span><br><span class="line"></span><br><span class="line">	if (enableCheckpointing) &#123;</span><br><span class="line">		&#x2F;&#x2F; if checkpointing is enabled, the mode depends only on whether committing on checkpoints is enabled</span><br><span class="line">		return (enableCommitOnCheckpoint) ? OffsetCommitMode.ON_CHECKPOINTS : OffsetCommitMode.DISABLED;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		&#x2F;&#x2F; else, the mode depends only on whether auto committing is enabled in the provided Kafka properties</span><br><span class="line">		return (enableAutoCommit) ? OffsetCommitMode.KAFKA_PERIODIC : OffsetCommitMode.DISABLED;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Flink消费Kafka数据时指定offset的五种方式"><a href="#Flink消费Kafka数据时指定offset的五种方式" class="headerlink" title="Flink消费Kafka数据时指定offset的五种方式"></a>Flink消费Kafka数据时指定offset的五种方式</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">consumer010.setStartFromEarliest();   &#x2F;&#x2F; 设置 offset 为最旧</span><br><span class="line">consumer010.setStartFromGroupOffsets();  &#x2F;&#x2F; 设置 offset 为 group offset</span><br><span class="line">consumer010.setStartFromLatest();  &#x2F;&#x2F; 设置 offset 为最新</span><br><span class="line">consumer010.setStartFromSpecificOffsets(specificStartupOffsets);  &#x2F;&#x2F; 指定 offset</span><br></pre></td></tr></table></figure>
<p>1、setStartFromGroupOffsets，如果 group offset 不存在，或者 group offset 无效的话，将依据 “auto.offset.reset” 该属性来决定初始 offset。auto.offset.reset 默认为 largest。<br>2、setStartFromSpecificOffsets，如果指定 offset 无效时，则将该 topic partition 的 offset 将设置为 group offset。<br>3、<code>如果该作业是从 checkpoint 或 savepoint 中恢复，则所有设置初始 offset 的函数均将失效，初始 offset 将从 checkpoint 中恢复。</code></p>
<p>链接：<a href="https://www.jianshu.com/p/f9d447a3c48f" target="_blank" rel="noopener">https://www.jianshu.com/p/f9d447a3c48f</a><br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/connectors/kafka.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/connectors/kafka.html</a></p>
<h3 id="Kafka-Consumer-提交-Offset-的行为配置"><a href="#Kafka-Consumer-提交-Offset-的行为配置" class="headerlink" title="Kafka Consumer 提交 Offset 的行为配置"></a>Kafka Consumer 提交 Offset 的行为配置</h3><p>Flink Kafka Consumer 允许配置将 offset 提交回 Kafka broker（或 0.8 版本的 Zookeeper）的行为,但是请注意：<code>Flink Kafka Consumer 不依赖于提交的 offset
来实现容错保证。提交的 offset 只是一种方法，只是Flink将消费的状态暴露在外以便于监控</code>。</p>
<p>配置 offset 提交行为的方法是否相同，取决于是否为 job 启用了 checkpointing。</p>
<p>禁用 Checkpointing： 如果禁用了 checkpointing，则 Flink Kafka Consumer 依赖于内部使用的 Kafka client 自动定期 offset 提交功能。<br>因此，要禁用或启用 offset 的提交，只需将 enable.auto.commit（或 Kafka 0.8 的 auto.commit.enable）或者 auto.commit.interval.ms 的Key 值设置为提供的 Properties 配置中的适当值。</p>
<p>启用 Checkpointing： 如果启用了 checkpointing，那么当 checkpointing 完成时，Flink Kafka Consumer 将提交的 offset 存储在 checkpoint 状态中。<br>这确保 Kafka broker 中提交的 offset 与 checkpoint 状态中的 offset 一致。 用户可以通过调用 consumer 上的 setCommitOffsetsOnCheckpoints(boolean) 方法来禁用或启用 offset 的提交(默认情况下，这个值是 true )。 注意，在这个场景中，Properties 中的自动定期 offset 提交设置会被完全忽略。</p>
<h3 id="Kafka-Consumer-lifeCycle"><a href="#Kafka-Consumer-lifeCycle" class="headerlink" title="Kafka Consumer lifeCycle"></a>Kafka Consumer lifeCycle</h3><p>修改cousemerGroup，checkpoint/savepoint中的offset仍有效<br>修改topic则会无效<br><code>FlinkKafkaConsumerBase.initializeState</code>从最后一个成功的checkpoint中获取各个partition的offset到restoredState中。</p>
<h3 id="savepoint-amp-uid"><a href="#savepoint-amp-uid" class="headerlink" title="savepoint &amp; uid"></a>savepoint &amp; uid</h3><p>operator IDs 存在savepoint中。每个operate有默认的ID，通过operate在整个应用的operator topology的位置获得。因此一定要在代码中指定uid，否则代码升级中可能会丢失状态。<br>如果要使用savepoint功能，一定要给有状态的operate使用uid。但是一般我们不清楚哪些operate会有状态，因此最好是给所有operate都使用uid。<br>savepoint的功能就是给代码升级使用，uid给特定的operate起唯一名，保证在这个operate修改移动等情况下状态都不会丢！！！<br>flink对代码优化生成执行图，对于合并到一个节点上的多个operate，如果有改动且没有指定uid，savepoint会把状态清零。<br>可以开启<code>ExecutionConfig#disableAutoGeneratedUIDs</code> 当任务中没有指定uid时，提交失败。<br><code>任务重启成功，并不代表复用的状态是正确的（平台开启了--allowNonRestoredState参数）</code><br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/state/savepoints.html#%E5%A6%82%E6%9E%9C%E6%88%91%E5%9C%A8%E4%BD%9C%E4%B8%9A%E4%B8%AD%E9%87%8D%E6%96%B0%E6%8E%92%E5%BA%8F%E6%9C%89%E7%8A%B6%E6%80%81%E7%AE%97%E5%AD%90%E4%BC%9A%E5%8F%91%E7%94%9F%E4%BB%80%E4%B9%88" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/state/savepoints.html#%E5%A6%82%E6%9E%9C%E6%88%91%E5%9C%A8%E4%BD%9C%E4%B8%9A%E4%B8%AD%E9%87%8D%E6%96%B0%E6%8E%92%E5%BA%8F%E6%9C%89%E7%8A%B6%E6%80%81%E7%AE%97%E5%AD%90%E4%BC%9A%E5%8F%91%E7%94%9F%E4%BB%80%E4%B9%88</a><br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/upgrading.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/upgrading.html</a> （代码升级时的注意事项）</p>
<h3 id="MaxParallelism"><a href="#MaxParallelism" class="headerlink" title="MaxParallelism"></a>MaxParallelism</h3><p>最大并行度默认值粗略地使用 operatorParallelism * 1.5 ，下限 128，上限 32768 。<br>一旦设置了最大并发度（无论是手动设置，还是默认设置），之后就无法再对该值做更新。想要改变一个作业的最大并发度，就只能将作业从全新的状态重新开始执行。目前还无法在更改最大并发度后，从上一个 checkpoint 或 savepoint 恢复。<br>最大并发度不能设置过大，一个很高的最大并发度会导致 Flink 需要维护大量的元数据（用于扩缩容），这可能会增加 Flink 应用程序的整体状态大小。<br>key-groups (which are the internal implementation mechanism for rescalable state)是可伸缩状态的内部实现机制，通过MaxParallelism计算.<br>最大并行度可以设置在整个作业上，也可以设置在单个operate上。<br>设置整个作业最大并行度：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.getConfig().setMaxParallelism(4);</span><br></pre></td></tr></table></figure>
<p><a href="http://wuchong.me/blog/2018/12/03/flink-tips-4-steps-flink-application-production-ready/" target="_blank" rel="noopener">http://wuchong.me/blog/2018/12/03/flink-tips-4-steps-flink-application-production-ready/</a><br><a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/parallel.html#setting-the-maximum-parallelism" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-stable/dev/parallel.html#setting-the-maximum-parallelism</a> （Parallelism）</p>
<h3 id="重启策略机制"><a href="#重启策略机制" class="headerlink" title="重启策略机制"></a>重启策略机制</h3><p>常用的重启策略：固定间隔 (Fixed delay)、失败率 (Failure rate)[如1d中重启3次失败]、无重启 (No restart)<br>失败率：失败率重启策略在Job失败后会重启，但是超过失败率后，Job会最终被认定失败。在两个连续的重启尝试之间，重启策略会等待一个固定的时间。<br>固定间隔：重启超过一定次数后，被认定为失败。在两个连续的重启尝试之间，重启策略会等待一个固定的时间。</p>
<p>如果没有启用 checkpointing，则使用无重启 (no restart) 策略。<br>如果启用了 checkpointing，但没有配置重启策略，则使用固定间隔 (fixed-delay) 策略</p>
<p>配置方法：<br>1、在flink平台flink-conf.yaml中配置<br>2、在单个任务flink任务代码中配置，会覆盖flink平台配置<br>具体代码使用看下面链接</p>
<p>实际使用中，如果在flink-conf.yaml中配置了重启，没有开启checkpoint也会继承这个重启策略，flink 1.8<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/task_failure_recovery.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/task_failure_recovery.html</a><br><a href="https://juejin.im/post/6844903721671852046" target="_blank" rel="noopener">https://juejin.im/post/6844903721671852046</a></p>
<h3 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h3><p>默认情况下一个flink job只启动一个jobManager，因此存在单点故障问题，所以生产环境中需要HA。<br>判断 Attempt 失败的标准是 Flink 通过 AMRMClientAsyncImpl 通知 YARN RM Application 失败并注销自己.<br>体现方式：yarn日志有多个</p>
<p>配置方式：<br>配置yarn集群级别AM重启上限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vim yarn-site.xml</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;100&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>配置flink HA相关</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vim &#x2F;home&#x2F;admin&#x2F;flink-1.9.1&#x2F;conf&#x2F;flink-conf.yaml</span><br><span class="line"></span><br><span class="line">high-availability: zookeeper  只有zookeeper一种选项</span><br><span class="line">high-availability.storageDir: hdfs:&#x2F;&#x2F;&#x2F;flink&#x2F;ha&#x2F;  指定job manager的元数据存储位置，需要指定在HDFS下的目录，这样多个节点上运行的job manager都可以访问到。</span><br><span class="line">high-availability.zookeeper.quorum: uhadoop-op3raf-master1,uhadoop-op3raf-master2,uhadoop-op3raf-core1 zookeeper的地址。</span><br><span class="line"># 单个flink job的重试次数</span><br><span class="line">yarn.application-attempts: 10</span><br></pre></td></tr></table></figure>
<p>建议flink-conf.yaml配置的AM重启次数不要超过yarn-site.xml中配置的次数。<br><code>yarn.application-attempt-failures-validity-interval</code> 默认是10s，需要在设置的这个interval内失败重试多少次，才认为flink job是失败的，如果超过这个interval，就会重新开始计数。首次启动时attempts计数，值为1<br>打个比方，yarn.application-attempts: 2，yarn.application-attempt-failures-validity-interval = 10000（默认值，10s），只有在10s内 flink job 失败重启2次才会真正的失败。<br>YarnSessionClusterEntrypoint（ApplicationMaster）为FlinkJobManager进程<br>flink 1.10版本JobManager进程为：YarnJobClusterEntrypoint</p>
<p><a href="https://blog.csdn.net/cndotaci/article/details/106870413" target="_blank" rel="noopener">https://blog.csdn.net/cndotaci/article/details/106870413</a> (HA配置方式)<br><a href="http://apache-flink.147419.n8.nabble.com/flink-yarn-HA-HA-state-td4372.html" target="_blank" rel="noopener">http://apache-flink.147419.n8.nabble.com/flink-yarn-HA-HA-state-td4372.html</a> （参数的理解）<br><a href="https://blog.csdn.net/cndotaci/article/details/107076252" target="_blank" rel="noopener">https://blog.csdn.net/cndotaci/article/details/107076252</a><br><a href="https://www.jianshu.com/p/1c5f54f2451a" target="_blank" rel="noopener">https://www.jianshu.com/p/1c5f54f2451a</a></p>
<h3 id="State"><a href="#State" class="headerlink" title="State"></a>State</h3><p>State状态是指一个Flink Job中的task中的每一个operator的状态，state可以存在本地系统（flink的堆内存或者堆外内存；堆内存指jvm掌管的内存，堆外内存指直接的内存）或者rocksdb中，存储计算的中间结果，提供给后续的算子使用;</p>
<p>state管理器有三种：1、基于内存  2、基于hdfs   3、基于rocksdb<br>第三种任务的状态数据首先写入rocksdb，然后周期性异步写入文件系统；是数据的冷热分离，正在计算的数据在rocksdb，其他数据存入hdfs.</p>
<p>State状态的作用是合操作，机器学习迭代训练模型。</p>
<p>State种类：主要分为Operator State以及KeyedState，在Flink 1.5之后，又推出了BroadCast State<br>KeyedState（类似于定义一个全局map）可以设置ttl，Operator State（map、filter等）貌似不行。<br>TTLs目前仅支持processing time；任意类型的keyed state都可以制定有效期。如果配置了TTL，并且状态值已经过期，那么将尽最大努力清理存储值；具体的ttl可以看本文ttl章节。</p>
<p>Operator State是指在一个job中的一个task中的每一个operator对应着一个state，比如在一个job中，涉及到map，filter，sink等操作，那么在这些operator中，每一个可以对应着一个state（一个并行度），如果是多个并行度，那么每一个并行度都对应着一个state。对于Operator State主要有ListState可以进行使用。</p>
<p>keystate 只能用在keystream流中；liststate 是所有并行度（subtask）平均分，所以每个子任务拿到的都只是一部分<br>List State中 ，每一个并行度对应着一个state，KeyState是指一个key对应着一个state<br><a href="https://blog.csdn.net/qq_37142346/article/details/90667283" target="_blank" rel="noopener">https://blog.csdn.net/qq_37142346/article/details/90667283</a></p>
<h3 id="State-ttl"><a href="#State-ttl" class="headerlink" title="State ttl"></a>State ttl</h3><p>state过期的实现方式是带了时间戳。时间戳可以指定是在读/写/更新时reset。<br>ttl是懒策略模式，因此可能读到未过期数据；可以通过设置指定是否返回已经过期的数据。<br>目前，过期值只有在显式读取时才会被删除。这意味着默认情况下，如果未读取过期状态就不会删除它，这可能导致状态不断增长。</p>
<p>1、checkpoint可以指定是否增量；如果是全量，通过设置会在快照时ttl<code>StateTtlConfig.newBuilder(Time.seconds(1)).cleanupFullSnapshot.build</code><br>2、Incremental cleanup in Heap state backends（JobManager内存，适用于FSStateBackend and MemoryStateBackend，看下面链接）<br>3、1.8开始增量rocksdb checkpoint提供后台background清理，此策略是一个压缩过滤；rocksdb本身会阶段性的异步执行状态合并和压缩，在这个过程中检验时间戳过滤；设置<code>state.backend.rocksdb.ttl.compaction.filter.enabled</code> 或者<code>RocksDBStateBackend::enableTtlCompactionFilter</code>(看源码注释，默认是开启状态) &amp;&amp; <code>StateTtlConfig .newBuilder(Time.days(7)).cleanupInRocksdbCompactFilter(10000).build();</code>  在每处理10000条状态记录之后，更新检测过期的时间戳。这个参数要小心设定，更新太频繁会降低compaction的性能，更新过慢会使得compaction不及时，状态空间膨胀。<br><code>需要重点注意的是，这个方法会减慢rocksdb的压缩</code><br><a href="https://flink.apache.org/2019/05/19/state-ttl.html" target="_blank" rel="noopener">https://flink.apache.org/2019/05/19/state-ttl.html</a></p>
<h3 id="去重方案"><a href="#去重方案" class="headerlink" title="去重方案"></a>去重方案</h3><p>1、布隆过滤器<br>2、rocksdb状态，注意使用cleanupInRocksdbCompactFilter，否则状态会越来越大（原因看上节ttl，未读取时不会主动过期）<br>3、外部kv存储<br><a href="https://www.jianshu.com/p/f6042288a6e3" target="_blank" rel="noopener">https://www.jianshu.com/p/f6042288a6e3</a></p>
<h3 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h3><p>Checkpoint是指在某个特定的时刻下，对整个job一个全局的快照，当我们遇到故障或者重启的时候可以从备份中进行恢复。<br>Checkpoint参数介绍：<a href="https://www.jianshu.com/p/7ec404b020c4" target="_blank" rel="noopener">https://www.jianshu.com/p/7ec404b020c4</a><br>SHARED 目录保存了可能被多个 checkpoint 引用的文件，<br>TASKOWNED 保存了不会被 JobManager 删除的文件，<br>EXCLUSIVE 则保存那些仅被单个 checkpoint 引用的文件。</p>
<p>Checkpoint是Flink实现容错机制最核心的功能，它能够根据配置周期性地基于Stream中各个Operator的状态来生成Snapshot，从而将这些状态数据定期持久化存储下来.<br>Checkpoint指定触发生成时间间隔后，每当需要触发Checkpoint时，会向Flink程序运行时的多个分布式的Stream Source中插入一个Barrier标记，<br>这些Barrier会根据Stream中的数据记录一起流向下游的各个Operator。当一个Operator接收到一个Barrier时，它会<code>暂停</code>处理Steam中新接收到的数据记录。因为一个Operator可能存在多个输入的Stream，而每个Stream中都会存在对应的Barrier，该Operator要等到所有的输入Stream中的Barrier都到达。当所有Stream中的Barrier都已经到达该Operator，这时所有的Barrier在时间上看来是同一个时刻点（表示已经对齐），在等待所有Barrier到达的过程中，Operator的Buffer中可能已经缓存了一些比Barrier早到达Operator的数据记录（Outgoing Records），这时该Operator会将数据记录（Outgoing Records）发射（Emit）出去，作为下游Operator的输入，最后将Barrier对应Snapshot发射（Emit）出去作为此次Checkpoint的结果数据。</p>
<p><code>对齐阶段</code>，当还有其他输入流的barrier还没有到达时，会把已到达的barrier之后的数据搁置在缓冲区，等待其他流的barrier到达后才能处理(Exactly Once)<br><code>barrier不对齐</code>就是指当还有其他流的barrier还没到达时，为了不影响性能，也不用理会，直接处理barrier之后的数据。等到所有流的barrier的都到达后，就可以对该Operator做CheckPoint了.<code>AT_LEAST_ONCE类型不需要alignment（对齐）</code><br>Exactly Once时必须barrier对齐，如果barrier不对齐就变成了At Least Once；<br>例子详解：CheckPoint的目的就是为了保存快照，如果不对齐，那么在chk-100快照之前，已经处理了一些chk-100 对应的offset之后的数据，当程序从chk-100恢复任务时，chk-100对应的offset之后的数据还会被处理一次，所以就出现了重复消费。<br>如果上下游是rescale或者forward的形式，下游只需要等待1个并发的barrier，因为是point-to-point的形式，如果是hash或者rebalance，下游的每一个task开始checkpoint的前提就是要收集齐上游所有并发的barrier。</p>
<p>一旦最后所有输入流都接收到barrier n，Operator就会把缓冲区中pending 的输出数据发出去，然后把CheckPoint barrier n接着往下游发送<br>为了下游尽快做CheckPoint，所以会先发送barrier到下游，自身再同步进行快照；所以可能会出现下游比上游快照还早的情况<br>之后，Operator将继续处理来自所有输入流的记录，在处理来自流的记录之前先处理来自输入缓冲区的记录。</p>
<p>如果只有一个partition，对应flink任务的Source Task并行度只能是1，是否对齐确实没有区别，不会有至少一次的存在了，肯定是精确一次。因为只有barrier不对齐才会有可能重复处理，这里并行度都已经为1，默认就是对齐的，只有当上游有多个并行度的时候，多个并行度发到下游的barrier才需要对齐，单并行度不会出现barrier不对齐，所以必然精确一次。<br>其实还是要理解barrier对齐就是Exactly Once不会重复消费，barrier不对齐就是 At Least Once可能重复消费，这里只有单个并行度根本不会存在barrier不对齐，所以不会存在至少一次语义；</p>
<p>链接：<a href="https://www.jianshu.com/p/4d31d6cddc99" target="_blank" rel="noopener">https://www.jianshu.com/p/4d31d6cddc99</a> (讲的很好)<br>链接：<a href="https://www.jianshu.com/p/dff71581b63b" target="_blank" rel="noopener">https://www.jianshu.com/p/dff71581b63b</a></p>
<p>Acknowledgement：JobManager收到ack<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/monitoring/checkpoint_monitoring.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/monitoring/checkpoint_monitoring.html</a></p>
<p>JM trigger checkpoint<br>Source 收到 trigger checkpoint 的 PRC，自己开始做 snapshot，并往下游发送 barrier<br>下游接收 barrier（需要 barrier 都到齐才会开始做 checkpoint）<br>Task 开始同步阶段 snapshot<br>Task 开始异步阶段 snapshot（异步阶段在做 rocksdb持久化）<br>Task snapshot 完成，汇报给 JM<br>如果触发checkpoint的延迟时间总是非常高时，表明<strong>checkpoint barriers</strong>需要很长时间才能从Source到达操作符。这通常表明系统在一个恒定的反压力下运行。<br>Buffered During Alignment 表示在 barrier 对齐阶段积攒了多少数据，如果这个数据过大也间接表示对齐比较慢）；<br>flink web ui上的对齐不包括收到第一个barrier的时间间隔<br><a href="https://zhuanlan.zhihu.com/p/87131964" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/87131964</a></p>
<h3 id="严格一次（Exactly-once）"><a href="#严格一次（Exactly-once）" class="headerlink" title="严格一次（Exactly-once）"></a>严格一次（Exactly-once）</h3><p>默认启用。<br>Flink Kafka Consumer和Flink的Checkpint机制进行了整合，以此提供了exactly-once处理语义。<br>为了实现这个语义，定期地将Kafka offsets信息、状态信息以及其他的操作信息进行Checkpint。所以，如果Flink作业出故障了，Flink将会从最新的Checkpint中恢复，并且从上一次偏移量开始读取Kafka中消费消息。<br>Flink仅仅支持在拥有足够的处理slots情况下才能够从Checkpint恢复。Flink on YARN模式下支持动态地重启丢失的YARN containers。</p>
<p>如果我们没有启用Checkpoint，那么Flink Kafka consumer将会定期地向Zookeeper commit偏移量。</p>
<h3 id="kafka端到端的Exactly-once"><a href="#kafka端到端的Exactly-once" class="headerlink" title="kafka端到端的Exactly-once"></a>kafka端到端的Exactly-once</h3><p>要求：flink1.4.1以上，kafka0.11以上版本(开始有事务)<br>read-process-write模式：将消息消费和生产封装在一个事务中，形成一个原子操作。<br>原理: kafka的生产者虽然仍会不断的流出数据，但是checkpoint完成才提交事务。下游的消费者通过配置<code>read-process-write</code>模式来限制 <code>只消费 已经提交事务的消息</code> 达到端到端一致性。因此数据的延迟至少是一个checkpoint的时间。<br>使用demo：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!--  生产者配置： --&gt;</span><br><span class="line">Properties sinkProperties &#x3D; new Properties();</span><br><span class="line">sinkProperties.setProperty(&quot;bootstrap.servers&quot;, &quot;*******: 9092&quot;);</span><br><span class="line">&#x2F;&#x2F; 设置transaction超时时间（必须）</span><br><span class="line">sinkProperties.setProperty(&quot;transaction.timeout.ms&quot;, &quot;60000&quot;);</span><br><span class="line">&#x2F;&#x2F; 指定Semantic.EXACTLY_ONCE，默认AT_LEAST_ONCE</span><br><span class="line">stream.addSink(new FlinkKafkaProducer&lt;String&gt;(&quot;flink-sink-test&quot;, new KeyedSerializationSchemaWrapper(new SimpleStringSchema()), sinkProperties, FlinkKafkaProducer.Semantic.EXACTLY_ONCE)).uid(&quot;kafka-sink&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;!-- 消费端配置 --&gt;</span><br><span class="line">properties.setProperty(&quot;isolation.level&quot;, &quot;read_committed&quot;);</span><br></pre></td></tr></table></figure>
<h3 id="RocksDB"><a href="#RocksDB" class="headerlink" title="RocksDB"></a>RocksDB</h3><p>增量式检查点 以 RocksDB 为基础。<br>是一个 LSM 结构的 KV 数据库。LSM不是一个具体的数据结构，是一个数据结构的概念，是一个数据结构的设计思想。将数据形成Log-Structured：在将数据写入LSM内存结构之前，先记录log。将所有磁盘上数据不组织成一个整体索引结构，而组织成有序的文件集。将数据按key排序，在合并不同file、level上的数据时类似merge-join</p>
<p>1、把所有的修改保存在内存的可变缓存中（称为 memtable），所有对 memtable 中 key 的修改，会覆盖之前的 value<br>2、当前 memtable 满了之后，RocksDB 会将所有数据以有序的写到磁盘。<br>3、当 RocksDB 将 memtable 写到磁盘后，整个文件就不再可变，称为有序字符串表（sstable）。RocksDB 的后台压缩线程会将 sstable 进行合并，就重复的键进行合并，合并后的 sstable 包含所有的键值对，RocksDB 会删除合并前的 sstable。</p>
<h3 id="流分区器"><a href="#流分区器" class="headerlink" title="流分区器"></a>流分区器</h3><p>Spark的RDD有分区的概念，Flink的DataStream同样也有，只不过没有RDD那么显式而已。Flink通过流分区器StreamPartitioner来控制DataStream中的元素往下游的流向。<br>Flink通过分区器来精确得控制数据流向。（下面讲解为1.10版本）<br><code>StreamPartitioner</code> 流分区器的基类；<br>不同分区器的核心在<code>selectChannel</code>方法上，该方法针对当前的record以及所有的channel数目，返回一个针对当前这条记录采用的output channel的索引。</p>
<ul>
<li><code>GlobalPartitioner</code> 全局分区器， 输出channel为0。全部发给下游第一个task</li>
<li><code>ForwardPartitioner</code> 输出channel为0。如果用户没有指定分区的前提下（record转发给本地下游，即在ui上多个operate被优化后显示在一个框里的情况，point-to-point转发数据），上下游算子并行度一致，那么采用的就是ForwardPartitioner， <code>如果上下游算子不一致，采用的是RebalancePartitioner</code>。在源码StreamGraph类中有体现。</li>
<li><code>ShufflePartitioner</code> 混洗分区器 输出channel为随机数</li>
<li><code>KeyGroupStreamPartitioner</code>（HashPartitioner，低版本）两次hash后与最大并行度取模！！！！</li>
<li><code>BroadcastPartitioner</code> 没有分区器，没有selectChannel，record分发给下游所有substask。由于广播流发挥作用必须靠DataStream.connect()方法与正常的数据流连接起来，所以实际上不需要BroadcastPartitioner来选择分区，没有实现selectChannel。</li>
<li><code>RebalancePartitioner</code> 重平衡分区器。先随机选择一个下游算子的实例，然后从这个实例开始 用轮转模式输出，保证负载均衡</li>
<li><code>RescalePartitioner</code> 在不同并行度不是彼此的倍数的情况下，一个或多个下游操作将具有来自上游操作的不同数量的输入。执行计划分为StreamGraph -》 JobGraph -》ExecutionGraph。 而StreamingJobGraphGenerator就是讲StreamGraph转换为JobGraph在这个类中，他把ForwardPartitioner和RescalePartitioner列为POINTWISE(点对点)分配模式，其他的为ALL_TO_ALL(多对多)分配模式。而在jobGraph -》 ExecutionGraph中，连接上下游时，会根据这两种模式，去分配上游某个分区，所对应的下游分区范围。<br>好处是可以增加taskmanager的数据本地性，减少了网络IO，taskmanager数据可以直接从本地的上游算子获取所需数据。RescalePartitioner和RebalancePartitioner相比，数据本地性比较好，减少了网络IO，但是不如 RebalancePartitioner数据均衡，因为RebalancePartitioner是ALL_TO_ALL模式的，对应下游所有分区，是真正的轮询。如果上游数据比较大，采用RebalancePartitioner，会带来不少的网络开销。（详细代码展示看下面第二、三个链接）.<br>CustomPartitionerWrapper 自定义分区器。举个栗子,根据第一个字段的长度分区：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dataStreamSource.partitionCustom(new Partitioner&lt;String&gt;() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int partition(String key, int numPartitions) &#123;</span><br><span class="line">        return key.length() % numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;, 0);</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>1、keyBy() （基于hash码重分区），是KeyGroupStreamPartitioner 两次hash后与最大并行度取模！！！！这也是为什么上面讲maxParallelism时建议要求设置，因为checkpoint、savepoint恢复的计量单位就是keyGroup，这个计算好以后修改maxParallelism 无法再重新计算。用murmurHash计算Hash值，以保证尽量的散列。<br>MathUtils.murmurHash(key.hashCode) % maxParallelism  * parallelism / maxParallelism （???? 为什么要 * parallelism / maxParallelism）<br>2、看<code>StreamingJobGraphGenerator</code>源码，可知只有ForwardPartitioner和RescalePartitioner是点对点本地模式。这种方式可节省网络传输。<br>3、上下游的算子没有指定分区器的情况下，如果上下游的算子并行度一致，则使用ForwardPartitioner，否则使用RebalancePartitioner。<br>4、对于ForwardPartitioner，必须保证上下游算子并行度一致，否则会抛出异常<br>5、看源码时的<code>Channel</code>可直接认为是下游算子的并发实例（即物理分区）。<br><a href="https://blog.csdn.net/yanghua_kobe/article/details/51736308（老版本，可看性不高）" target="_blank" rel="noopener">https://blog.csdn.net/yanghua_kobe/article/details/51736308（老版本，可看性不高）</a><br><a href="https://blog.csdn.net/lvwenyuan_1/article/details/103722226?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase（讲的更细）" target="_blank" rel="noopener">https://blog.csdn.net/lvwenyuan_1/article/details/103722226?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase（讲的更细）</a><br><a href="https://jiamaoxiang.top/2020/03/30/Flink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/" target="_blank" rel="noopener">https://jiamaoxiang.top/2020/03/30/Flink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</a><br><a href="https://cloud.tencent.com/developer/article/1559885" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1559885</a></p>
<h3 id="内存调参"><a href="#内存调参" class="headerlink" title="内存调参"></a>内存调参</h3><p><code>taskmanager.network.memory.max</code> 网络缓冲区最大内存大小。<br><code>taskmanager.network.memory.min</code> 网络缓冲区最小内存大小。<br><code>taskmanager.network.memory.fraction</code> 网络缓冲区使用的内存占据总JVM内存的比例。如果配置了taskmanager.network.memory.max和taskmanager.network.memory.min，本配置项会被覆盖。</p>
<p>链接：<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html#memory-configuration" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html#memory-configuration</a> (官网)<br><a href="https://www.jianshu.com/p/28c7722ae22f" target="_blank" rel="noopener">https://www.jianshu.com/p/28c7722ae22f</a><br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#taskmanager-memory-jvm-metaspace-size" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#taskmanager-memory-jvm-metaspace-size</a></p>
<h3 id="表转流-三种输出方式"><a href="#表转流-三种输出方式" class="headerlink" title="表转流 三种输出方式"></a>表转流 三种输出方式</h3><p>追加模式（toAppendStream）：只有在动态Table仅通过INSERT更改修改时才能使用此模式，即它仅附加并且以前发出的结果永远不会更新。<br>缩进模式（toRetractStream）：始终可以使用此模式。它用标志编码INSERT和DELETE改变boolean。<br>区别：<a href="https://zhuanlan.zhihu.com/p/65436188" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/65436188</a><br>当我们使用的sql语句包含：count() group by时，必须使用缩进模式。<br>另外一种方式Upsert stream效率更高；<a href="https://juejin.im/post/5dd7423851882573412f78d9" target="_blank" rel="noopener">https://juejin.im/post/5dd7423851882573412f78d9</a> （有例子！！讲的好）</p>
<p>Flink里面把流注册为表: tableEnv.registerDataStream(“mytable”, mystream ….) 这里的流 mystream 必须是append-only流，不能是retraction流。<br>retraction流不支持 append-only 模式的<br><a href="https://developer.aliyun.com/ask/128464?spm=a2c6h.13159736" target="_blank" rel="noopener">https://developer.aliyun.com/ask/128464?spm=a2c6h.13159736</a></p>
<p>！！！ flink-sql有一个比较坑的问题，比如group by嵌套一个group by，使用的是追加模式，里面的groupby会把DELETE也发给外层groupby,这时候如果开启了local聚合，数据就会出错，因为把delete消息也聚起来了；如果没有开启local也要保证，最终的结果是主键key可以单一覆盖，否则数据也会出错。</p>
<h3 id="背压（backpressure）监控"><a href="#背压（backpressure）监控" class="headerlink" title="背压（backpressure）监控"></a>背压（backpressure）监控</h3><p>1、如果你看到了一个任务的back pressure警告（如过高），则意味着<code>该任务产生数据的速度要高于下游Operator消化的速度。</code><br>数据沿着job的数据流图向下游流动（如从source到sink），而背压则是沿着相反的方向传播，逆流而上。<br>2、jobManager通过重复的调用<code>Task.isBackPressured()</code>方法来得到堆栈信息。<br>3、默认地，对于每一个任务，JobManager会每隔50ms触发100次堆栈轨迹。我们在web接口中看到的比例说明了这些堆栈轨迹中有多少卡在了内部方法调用中，如0.01就表示100个堆栈轨迹中有一个卡在了方法调用中。<br>4、Sampling in progress；该状态意味着JobManager出发了一个运行中的任务的堆栈轨迹取样，默认配置下，该操作需要约5秒。<br>5、配置：<br><code>web.backpressure.refresh-interval</code>，统计数据被废弃重新刷新的时间（默认值：60000，1分钟）。<br><code>web.backpressure.num-samples</code>，用于确定背压的堆栈跟踪样本数（默认值：100）。<br><code>web.backpressure.delay-between-samples</code>，堆栈跟踪样本之间的延迟以确定背压（默认值：50，50ms）</p>
<p><a href="https://www.cnblogs.com/lanyun0520/p/5676617.html（中文翻译）" target="_blank" rel="noopener">https://www.cnblogs.com/lanyun0520/p/5676617.html（中文翻译）</a><br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/monitoring/back_pressure.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/monitoring/back_pressure.html</a> （英文官网）</p>
<h3 id="AsyncFunction"><a href="#AsyncFunction" class="headerlink" title="AsyncFunction"></a>AsyncFunction</h3><p>外部查询时，异步<br><a href="https://www.jianshu.com/p/d8f99d94b761" target="_blank" rel="noopener">https://www.jianshu.com/p/d8f99d94b761</a><br><a href="https://www.cnblogs.com/dajiangtai/p/10683664.html" target="_blank" rel="noopener">https://www.cnblogs.com/dajiangtai/p/10683664.html</a></p>
<h3 id="rmq"><a href="#rmq" class="headerlink" title="rmq"></a>rmq</h3><p><a href="https://github.com/apache/rocketmq-externals/tree/master/rocketmq-flink" target="_blank" rel="noopener">https://github.com/apache/rocketmq-externals/tree/master/rocketmq-flink</a></p>
<h3 id="flink-web-ui展示有问题"><a href="#flink-web-ui展示有问题" class="headerlink" title="flink web ui展示有问题"></a>flink web ui展示有问题</h3><p>一般显示有问题都是jar包的冲突问题<br>1、version显示不出来，把下面内容加上</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-runtime_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;scope&gt;$&#123;depend-scope&#125;&lt;&#x2F;scope&gt;</span><br><span class="line">            &lt;exclusions&gt;</span><br><span class="line">                &lt;exclusion&gt;</span><br><span class="line">                    &lt;artifactId&gt;scala-library&lt;&#x2F;artifactId&gt;</span><br><span class="line">                    &lt;groupId&gt;org.scala-lang&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;&#x2F;exclusion&gt;</span><br><span class="line">            &lt;&#x2F;exclusions&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>
<p>2、taskManager等recived bytes等详细信息一直转圈出不来;下面包在集群上使用provided</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-table-planner-blink_$&#123;scala.tools.version&#125;&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;$&#123;depend-scope&#125;&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>
<h3 id="possible-cause-maybe-a-semicolon-is-missing-before-‘value-build’"><a href="#possible-cause-maybe-a-semicolon-is-missing-before-‘value-build’" class="headerlink" title="possible cause: maybe a semicolon is missing before ‘value build’?"></a>possible cause: maybe a semicolon is missing before ‘value build’?</h3><p>flink10 当使用 StreamingFileSink 的多个 with… 方法时，会提示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Error:xxx value build is not a member of ?0</span><br><span class="line">  possible cause: maybe a semicolon is missing before &#96;value build&#39;?</span><br><span class="line">  build()</span><br><span class="line">&#96;&#96;&#96;  </span><br><span class="line">这是flink的bug，bug[https:&#x2F;&#x2F;issues.apache.org&#x2F;jira&#x2F;browse&#x2F;FLINK-16684]</span><br><span class="line">解决办法是java 版本编写，或者使用 更高的 flink 版本（1.10.1, 1.11.0）。</span><br><span class="line"></span><br><span class="line">参考：https:&#x2F;&#x2F;gitee.com&#x2F;jsqf&#x2F;flink10_learn&#x2F;</span><br><span class="line">###  flink异常</span><br><span class="line">org.apache.flink.util.StateMigrationException: The new state serializer cannot be incompatible</span><br><span class="line">我们知道Flink的状态是按key组织并保存的，如果程序逻辑内改了keyBy()逻辑或者key的序列化逻辑，就会导致检查点&#x2F;保存点的数据无法正确恢复。所以如果必须要改key相关的东西，就弃用之前的状态数据吧。</span><br><span class="line">我遇到的原因是：改变了类名；state中包含了类名来反序列化</span><br><span class="line"></span><br><span class="line">链接：https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;a2302724e6d6</span><br><span class="line"></span><br><span class="line">###  java.lang.OutOfMemoryError: Metaspace</span><br><span class="line">提升参数&#96;-yDtaskmanager.memory.jvm-metaspace.size&#x3D;300M&#96; 默认是256M</span><br><span class="line">https:&#x2F;&#x2F;ci.apache.org&#x2F;projects&#x2F;flink&#x2F;flink-docs-release-1.10&#x2F;ops&#x2F;config.html#taskmanager-memory-jvm-metaspace-size</span><br><span class="line"></span><br><span class="line">### flink wordcount error: No implicits found for parameter evidence$11: TypeInformation[String]</span><br><span class="line">引包改为：</span><br><span class="line">import org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line">### Caused by: java.lang.IllegalStateException: Unable to instantiate java compiler</span><br><span class="line">org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Unable to instantiate java compiler</span><br><span class="line">解决办法：flink-table-planner-blink这个包provided不要编译到项目中去</span><br><span class="line">原因：Flink 1.10把客户端的ClassLoader解析顺序调整为了Child优先，这就导致用户的Jar包不能包含Flink框架的classes，比如常见的Calcite、Flink-Planner依赖、Hive依赖等等。用户需要把有冲突classes的jar放到flink-home&#x2F;lib下，或者调整策略为Parent优先。</span><br><span class="line"></span><br><span class="line">原文链接：https:&#x2F;&#x2F;blog.csdn.net&#x2F;woloqun&#x2F;article&#x2F;details&#x2F;105657539</span><br><span class="line"></span><br><span class="line">### kafka序列化</span><br><span class="line">依靠用户指定的解序列化器来将二进制的数据转换成Java对象。DeserializationSchema接口就是做这件事情的.</span><br><span class="line">SimpleStringSchema来反系列化message，这个类是实现了DeserializationSchema接口，并重写了T deserialize(byte[] message)函数，DeserializationSchema接口仅提供了反系列化data的接口，</span><br><span class="line">所以如果我们需要反系列化key，我们需要使用KeyedDeserializationSchema的子类。KeyedDeserializationSchema接口提供了T deserialize(byte[] messageKey, byte[] message, String topic, int partition, long offset)方法，可以饭系列化kafka消息的data和key。</span><br><span class="line"></span><br><span class="line">### sql 提示org.apache.flink.table.api.TableException: An input of GenericTypeInfo&lt;Row&gt; cannot be converted to Table. Please specify the type of the input with a RowTypeInfo.</span><br><span class="line">使用dataStream时使用Row</span><br><span class="line"></span><br><span class="line">java:</span><br><span class="line">.returns(new RowTypeInfo(Types.STRING, Types.STRING, ...));</span><br><span class="line">https:&#x2F;&#x2F;cloud.tencent.com&#x2F;developer&#x2F;article&#x2F;1406235</span><br><span class="line">scala:</span><br><span class="line">implicit val tpe: TypeInformation[Row] &#x3D; new RowTypeInfo(fieldsSchema, fields)</span><br><span class="line">https:&#x2F;&#x2F;issues.apache.org&#x2F;jira&#x2F;browse&#x2F;FLINK-6500</span><br><span class="line">### table &amp;&amp; sql</span><br><span class="line">https:&#x2F;&#x2F;mp.weixin.qq.com&#x2F;s&#x2F;oIgVt9qTETfS6_9Srws_sw（Flink SQL 工作机制）</span><br><span class="line">符号字面量 https:&#x2F;&#x2F;juejin.im&#x2F;post&#x2F;5d859395f265da03e168b330</span><br><span class="line">https:&#x2F;&#x2F;www.cnblogs.com&#x2F;leesf456&#x2F;p&#x2F;8027772.html</span><br><span class="line">https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;cf1cb33bffd9</span><br><span class="line">https:&#x2F;&#x2F;juejin.im&#x2F;post&#x2F;58d6751cac502e0058ccf01c</span><br><span class="line">https:&#x2F;&#x2F;blog.csdn.net&#x2F;baifanwudi&#x2F;article&#x2F;details&#x2F;87883278</span><br><span class="line"></span><br><span class="line">### 特殊引入</span><br><span class="line">1、因为pojo类貌似必须携带 public pojo()&#123; &#125;无参构造器。</span><br><span class="line">2、tuple要使用java包里面的（scala &#96;import org.apache.flink.api.java.tuple._&#96;）,而不是scala自带的tuple，不然也会认为是geneic类型，导致报错</span><br><span class="line">3、Scala Table API使用Scala符号，它以单个tick（&#39;）开始引用表的属性。 Table API使用Scala隐含。 确保导入&#96;org.apache.flink.api.scala._&#96;和&#96;org.apache.flink.table.api.scala._&#96;以便使用Scala隐式转换。</span><br><span class="line"></span><br><span class="line">### registerTypeWithKryoSerializer</span><br><span class="line">&#96;registerTypeWithKryoSerializer&#96; 手动指定自定义数据结构的序列化函数</span><br><span class="line"></span><br><span class="line">### Row</span><br><span class="line">1、&#96;Row&#96; is the actual record that holds the data</span><br><span class="line">2、&#96;RowTypeInfo&#96; is a schema description for Rows. It contains names and TypeInformation for each field of a Row.</span><br><span class="line">3、&#96;DataStream&#96; is a logical stream of records. A DataStream[Row] is a stream of rows. Note that this is not the actual stream but just an API concept to represent a stream in the API.</span><br><span class="line"></span><br><span class="line">### flink 数据类型</span><br><span class="line">https:&#x2F;&#x2F;segmentfault.com&#x2F;a&#x2F;1190000016350098  https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;7ec404b020c4</span><br><span class="line">TypeInformation 类是描述一切类型的公共基类，它和它的所有子类必须可序列化（Serializable），因为类型信息将会伴随 Flink 的作业提交，被传递给每个执行节点。</span><br><span class="line">由于 Flink 自己管理内存，采用了一种非常紧凑的存储格式（见官方博文），因而类型信息在整个数据处理流程中属于至关重要的元数据。</span><br><span class="line">Flink 内部实现了名为 TypeExtractror 的类，可以利用方法签名、子类信息等蛛丝马迹，自动提取和恢复类型信息。</span><br><span class="line">然而由于 Java 的类型擦除，自动提取并不是总是有效。因而一些情况下（例如通过 URLClassLoader 动态加载的类），仍需手动处理；例如上面&#96;TableException&#96;的错误章节中介绍的办法。</span><br><span class="line"></span><br><span class="line">### Error:(28, 10) ambiguous reference to overloaded definition, both method toJSONString in object JSON</span><br><span class="line">带有变长参数的方法重载使得scala在调用方法时感到“模糊”，就无法匹配参数的类型。</span><br><span class="line">解决：JSON.toJSONString(result, SerializerFeature.PrettyFormat)</span><br><span class="line">https:&#x2F;&#x2F;blog.csdn.net&#x2F;k_wzzc&#x2F;article&#x2F;details&#x2F;90032138</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### transient</span><br><span class="line">这个注解一般用于序列化的时候，标识某个字段不用被序列化。</span><br><span class="line">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;55222822&#x2F;when-to-use-transient-when-not-to-in-flink</span><br><span class="line"></span><br><span class="line">### clickhouse</span><br><span class="line">https:&#x2F;&#x2F;wchch.github.io&#x2F;2019&#x2F;11&#x2F;27&#x2F;%E4%BD%BF%E7%94%A8Flink-SQL%E8%AF%BB%E5%8F%96kafka%E6%95%B0%E6%8D%AE%E5%B9%B6%E9%80%9A%E8%BF%87JDBC%E6%96%B9%E5%BC%8F%E5%86%99%E5%85%A5Clickhouse%E5%AE%9E%E6%97%B6%E5%9C%BA%E6%99%AF%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E4%BE%8B&#x2F;</span><br><span class="line"></span><br><span class="line">使用&#96;JDBCAppendTableSink&#96;</span><br><span class="line">Flink 在 1.11.0 版本对其 JDBC connector 进行了一次较大的重构：</span><br><span class="line"></span><br><span class="line">重构之前（1.10.1 及之前版本），包名为 flink-jdbc 。</span><br><span class="line">重构之后（1.11.0 及之后版本），包名为 flink-connector-jdbc 。</span><br><span class="line"></span><br><span class="line">https:&#x2F;&#x2F;www.cnblogs.com&#x2F;qiu-hua&#x2F;p&#x2F;13871460.html (flink1.10&amp;1.11写入ch的例子)</span><br><span class="line">### date</span><br><span class="line">&#96;TIMESTAMPDIFF(DAY, TIMESTAMP time1,TIMESTAMP time2)&#96; SQL validate报错！</span><br><span class="line">&#96;TIMESTAMPDIFF(DAY, cast(time1 as timestamp), cast(time2 as timestamp))&#96;通过了</span><br><span class="line">文档中的 TIMESTAMP &#39;2003-01-02 10:00:00&#39; 代表标准SQL的时间常量(timestamp literal)</span><br><span class="line">在1.9中， 申明为TIMESTAMP类型的属性，需要是 格式化为YYYY-MM-DD&#39;T&#39;HH:mm:ss.SSS&#39;Z&#39;</span><br><span class="line">不过你可以从外部传入13位时间戳，也可以转换成TIMESTAMP，    比如DDL中定义</span><br></pre></td></tr></table></figure>
<p>CREATE TABLE <code>t</code> (<br>   ctm TIMESTAMP,<br>) WITH (<br>  ‘format.schema’ = ‘ROW<ctm LONG>‘<br>)”</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">。如果数据源也要定义为TIMESTAMP类型，则通过下面方式去定义外部数据源格式：</span><br></pre></td></tr></table></figure>
<p>DateTimeFormatter t = new DateTimeFormatterBuilder()<br>        .append(DateTimeFormatter.ISO_LOCAL_DATE)<br>        .appendLiteral(‘T’)<br>        .append(new DateTimeFormatterBuilder()<br>                .appendPattern(“HH:mm:ss”)<br>                .appendFraction(ChronoField.NANO_OF_SECOND, 0, 9, true)<br>                .appendPattern(“‘Z’”)<br>                .toFormatter())<br>        .toFormatter();</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">用这个结构去格式化时间类型</span><br><span class="line">http:&#x2F;&#x2F;apache-flink.147419.n8.nabble.com&#x2F;FlinkSQL-TIMESTAMPDIFF-E-g-td1257.html</span><br><span class="line"></span><br><span class="line">### Flink:Could not forward element to next operator</span><br><span class="line">报错原因：一般是代码里有脏数据，多翻翻日志可以真正的报错信息。</span><br><span class="line">watermark中的eventTime为null或格式不对</span><br><span class="line"></span><br><span class="line">### No Implicit Value for Evidence Parameter Error</span><br><span class="line">import org.apache.flink.api.scala._</span><br><span class="line">https:&#x2F;&#x2F;blog.csdn.net&#x2F;Dax1n&#x2F;article&#x2F;details&#x2F;70211035</span><br><span class="line"></span><br><span class="line">### OutputTag类型匹配错误</span><br><span class="line">[ERROR]  found   : org.apache.flink.util.OutputTag[]</span><br><span class="line">[ERROR]  required: org.apache.flink.streaming.api.scala.OutputTag[?]</span><br><span class="line">把全部引入包改成第二个。</span><br><span class="line"></span><br><span class="line">### No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)</span><br><span class="line">krb5.keytab 出问题了</span><br></pre></td></tr></table></figure>
<p>Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Could not flush and close the file system output stream to null in order to obtain the stream state handle<br>    at java.util.concurrent.FutureTask.report(FutureTask.java:122)<br>    at java.util.concurrent.FutureTask.get(FutureTask.java:192)<br>    at org.apache.flink.runtime.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:461)<br>    at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:47)<br>    at org.apache.flink.streaming.runtime.tasks.StreamTask$AsyncCheckpointRunnable.run(StreamTask.java:1143)<br>    … 3 more<br>Caused by: java.io.IOException: Could not flush and close the file system output stream to null in order to obtain the stream state handle<br>    at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.closeAndGetHandle(FsCheckpointStreamFactory.java:334)<br>    at org.apache.flink.runtime.state.CheckpointStreamWithResultProvider$PrimaryStreamOnly.closeAndFinalizeCheckpointStreamResult(CheckpointStreamWithResultProvider.java:77)<br>    at org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy$RocksDBIncrementalSnapshotOperation.materializeMetaData(RocksIncrementalSnapshotStrategy.java:495)<br>    at org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy$RocksDBIncrementalSnapshotOperation.callInternal(RocksIncrementalSnapshotStrategy.java:313)<br>    at org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy$RocksDBIncrementalSnapshotOperation.callInternal(RocksIncrementalSnapshotStrategy.java:263)<br>    at org.apache.flink.runtime.state.AsyncSnapshotCallable.call(AsyncSnapshotCallable.java:75)<br>    at java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>    at org.apache.flink.runtime.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:458)<br>    … 5 more<br>Caused by: java.io.IOException: Could not open output stream for state backend<br>    at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.createStream(FsCheckpointStreamFactory.java:367)<br>    at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.flush(FsCheckpointStreamFactory.java:234)<br>    at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.closeAndGetHandle(FsCheckpointStreamFactory.java:309)<br>    … 12 more<br>Caused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn’t setup connection for user to host; Host Details : local host is: ; destination host is: “”;<br>    at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)<br>    at org.apache.hadoop.ipc.Client.call(Client.java:1476)<br>    at org.apache.hadoop.ipc.Client.call(Client.java:1409)<br>    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)<br>    at com.sun.proxy.$Proxy20.create(Unknown Source)<br>    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:301)<br>    at sun.reflect.GeneratedMethodAccessor35.invoke(Unknown Source)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)<br>    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)<br>    at com.sun.proxy.$Proxy21.create(Unknown Source)<br>    at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:2047)<br>    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1759)<br>    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1683)<br>    at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:425)<br>    at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:421)<br>    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)<br>    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:421)<br>    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:362)<br>    at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:180)<br>    at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.create(ChRootedFileSystem.java:181)<br>    at org.apache.hadoop.fs.viewfs.ViewFileSystem.create(ViewFileSystem.java:299)<br>    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925)<br>    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)<br>    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:803)<br>    at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:141)<br>    at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:37)<br>    at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.create(SafetyNetWrapperFileSystem.java:126)<br>    at org.apache.flink.core.fs.EntropyInjector.createEntropyAware(EntropyInjector.java:61)<br>    at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.createStream(FsCheckpointStreamFactory.java:356)<br>    … 14 more<br>Caused by: java.io.IOException: Couldn’t setup connection for user to namenode04<br>    at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:680)<br>    at java.security.AccessController.doPrivileged(Native Method)<br>    at javax.security.auth.Subject.doAs(Subject.java:422)<br>    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)<br>    at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:651)<br>    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:739)<br>    at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:376)<br>    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1525)<br>    at org.apache.hadoop.ipc.Client.call(Client.java:1448)<br>    … 43 more<br>Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]<br>    at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211)<br>    at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:413)<br>    at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:561)<br>    at org.apache.hadoop.ipc.Client$Connection.access$1900(Client.java:376)<br>    at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:731)<br>    at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:727)<br>    at java.security.AccessController.doPrivileged(Native Method)<br>    at javax.security.auth.Subject.doAs(Subject.java:422)<br>    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)<br>    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:726)<br>    … 46 more<br>Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)<br>    at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)<br>    at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122)<br>    at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)<br>    at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224)<br>    at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)<br>    at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)<br>    at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192)<br>    … 55 more</p>
<pre><code>### 文档
中文：https://flink-docs-cn.gitbook.io/project/05-ying-yong-kai-fa/04-table-api-and-sql/00-gai-shu
英文：https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/table/streaming/time_attributes.html
中文（非官方）：https://cloud.tencent.com/developer/article/1032586
demo-java：https://www.jianshu.com/p/43259ca7a9be
demo-scala：https://blog.csdn.net/ddxygq/article/details/87887114
https://developer.aliyun.com/article/746408?spm=a2c6h.14242504.J_8926434010.2.3fca361fvEl7PM&amp;groupCode=sc
https://gitee.com/jsqf/flink10_learn （使用实例）
https://blog.csdn.net/weixin_36630761/article/details/108753562 (基础)</code></pre>]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>habse</title>
    <url>/2020/05/11/hbase/</url>
    <content><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>建立在HDFS之上，列存储、实时读写，列式NoSQL数据库，仅支持单行事务。<br>NoSQL数据库，不支持SQL作为查询语言。<br>逻辑视图中的空cell在物理上是不存储的，因此可稀疏。<br>与hadoop一样，Hbase目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加计算和存储能力（比关系型数据库好的一点）。</p>
<h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><p>数据量大。数据库量要足够多，如果有十亿及百亿行数据，那么Hbase是一个很好的选项，如果只有几百万行甚至不到的数据量，RDBMS是一个很好的选择。因为数据量小的话，真正能工作的机器量少，剩余的机器都处于空闲的状态。<br>不需要辅助索引。静态类型的列，事务等特性，一个已经用RDBMS的系统想要切换到Hbase，则需要重新设计系统。</p>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><h4 id="row-key"><a href="#row-key" class="headerlink" title="row key"></a>row key</h4><p>  用来检索记录的主键，任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)，在hbase内部，row key保存为字节数组。<br>  存储时，数据按照Row key的字典序(byte order)排序存储。设计key时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)<br>注意：<br>字典序对int排序的结果是1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。要保持整形的自然序，行键必须用0作左填充。<br>行的一次读写是原子操作 (不论一次读写多少列)。这个设计决策能够使用户很容易的理解程序在对同一个行进行并发更新操作时的行为。</p>
<h4 id="column-family"><a href="#column-family" class="headerlink" title="column family"></a>column family</h4><p>  列族。一个表可以有多个列族column family，每个列族在hdfs上都是单独的一个文件（即每个列都保存到某一个文件中，但每个文件不只一个列）。<br>  列族是表的schema的一部 分(而列不是)，必须在使用表之前定义。<br>  访问控制、磁盘和内存的使用统计都是在列族层面进行的。</p>
<h4 id="Column"><a href="#Column" class="headerlink" title="Column"></a>Column</h4><p>Column: 列。属于某一个列族，列名定义为family:qualifier，其中qualifier可以是任意的字符串.列族可以有很多列,列名为family:qualifier。例如 courses:history，courses:math都属于courses 这个列族。列中 的数据是没有类型的，全部是字节码形式存储。</p>
<h4 id="Value-Cell"><a href="#Value-Cell" class="headerlink" title="Value(Cell)"></a>Value(Cell)</h4><p>由{row key, column( =<family> + <qualifier>), version} 唯一确定的单元。cell中的数据是没有类型的，全部是字节码形式存贮。<br>通过row和columns确定的为一个存贮单元称为cell。每个 cell都保存着同一份数据的多个版本。版本通过时间戳来索引。</p>
<h4 id="时间戳（Version-Number）"><a href="#时间戳（Version-Number）" class="headerlink" title="时间戳（Version Number）"></a>时间戳（Version Number）</h4><p>1、通过row和columns确定的为一个存贮单元称为cell。每个 cell都保存着同一份数据的多个版本。版本通过时间戳来索引。<br>2、类型为Long 64位整型，时间戳可以由hbase(在数据写入时自动 )赋值，此时时间戳是精确到毫秒的当前系统时间。时间戳也可以由客户显式赋值。<br>3、如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。<br>4、为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，hbase提供了两种数据版本回收方式。一是保存数据的最后n个版本，二是保存最近一段时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。<br>5、如果不指明时间，将会返回每列最新时间的行。</p>
<h4 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h4><p><img src="cun.png" alt=""><br>Region：Table按row key区间分割为多个region，即每个region包含一定范围的row key。每个表一开始只有一个region，随着数据量的增加，region会进行自动的split。<br>HBase建议一个Region的合理大小是500M到3G之间.<br>region是Hbase中分布式存储和负载均衡的最小单元。最小单元就表示不同的Hregion可以分布在不同的HRegion server上。但一个Hregion是不会拆分到多个server上的。<br>事实上，HRegion由一个或者多个Store组成，每个store保存一个columns family。<br>每个Strore又由一个memStore和0至多个StoreFile组成。如图：<br>StoreFile以HFile格式保存在HDFS上。</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>Client<br>1 包含访问hbase的接口，client维护着一些cache来加快对hbase的访问，比如regione的位置信息。</p>
<p>Zookeeper<br>1 保证任何时候，集群中只有一个master<br>2 存贮所有Region的寻址入口。<br>3 实时监控Region Server的状态，将Region server的上线和下线信息实时通知给Master<br>4 存储Hbase的schema,包括有哪些table，每个table有哪些column family</p>
<p>Master<br>1 为Region server分配region<br>2 负责region server的负载均衡<br>3 发现失效的region server并重新分配其上的region<br>4 GFS上的垃圾文件回收<br>5 处理schema更新请求</p>
<p>Region Server<br>1 Region server维护Master分配给它的region，处理对这些region的IO请求<br>2 Region server负责切分在运行过程中变得过大的region<br>！！！注，client访问hbase上数据的过程并不需要master参与（寻址访问zookeeper和region server，数据读写访问regione server），master仅仅维护者table和region的元数据信息，负载很低。</p>
<p>读：<br><img src="read.webp" alt=""><br>二层架构的定位步骤如下：<br>（1）用户通过查找zk（zookeeper）的/hbase/meta-region-server节点查询哪台RegionServer上有hbase:meta表。<br>（2）客户端连接含有hbase:meta表的RegionServer。Hbase:meta表存储了所有Region的行健范围信息，通过这个表就可以查询出你要存取的rowkey属于哪个Region的范围里面，以及这个Region又是属于哪个RegionServer。<br>（3）获取这些信息后，客户端就可以直连其中一台拥有你要存取的rowkey的RegionServer，并直接对其操作。<br>（4）客户端会把meta信息缓存起来，下次操作就不需要进行以上加载HBase:meta的步骤了。</p>
<p>链接：<a href="https://www.jianshu.com/p/928e139fa450" target="_blank" rel="noopener">https://www.jianshu.com/p/928e139fa450</a></p>
<h3 id="写过程"><a href="#写过程" class="headerlink" title="写过程"></a>写过程</h3><p>数据在更新时首先写入Log(WAL log)和内存(MemStore)中，MemStore中的数据是排序的，当MemStore累计到一定阈值时，就会创建一个新的MemStore，并 且将老的MemStore添加到flush队列，由单独的线程flush到磁盘上，成为一个StoreFile。于此同时，系统会在zookeeper中 记录一个redo point，表示这个时刻之前的变更已经持久化了。(minor compact)<br>当系统出现意外时，可能导致内存(MemStore)中的数据丢失，此时使用Log(WAL log)来恢复checkpoint之后的数据。<br>前面提到过StoreFile是只读的，一旦创建后就不可以再修改。因此Hbase的更新其实是不断追加的操作。当一个Store中的StoreFile达到一定的阈值后，就会进行一次合并(major compact),将对同一个key的修改合并到一起，形成一个大的StoreFile，当StoreFile的大小达到一定阈值后，又会对 StoreFile进行split，等分为两个StoreFile。<br>由于对表的更新是不断追加的，处理读请求时，需要访问Store中全部的 StoreFile和MemStore，将他们按照row key进行合并，由于StoreFile和MemStore都是经过排序的，并且StoreFile带有内存中索引，合并的过程还是比较快。</p>
<h3 id="访问方式"><a href="#访问方式" class="headerlink" title="访问方式"></a>访问方式</h3><p>访问hbase table中的行，只有三种方式：<br>1 通过单个row key访问<br>2 通过row key的range<br>3 全表扫描</p>
<h3 id="rowKey的设计原则"><a href="#rowKey的设计原则" class="headerlink" title="rowKey的设计原则"></a>rowKey的设计原则</h3><p>（1）Rowkey长度原则<br>Rowkey 是一个二进制码流，Rowkey 的长度被很多开发者建议说设计在10~100 个字节，不过建议是越短越好，不要超过16 个字节。<br>原因如下：<br>①数据的持久化文件HFile 中是按照KeyValue 存储的，如果Rowkey 过长比如100 个字节，1000 万列数据光Rowkey 就要占用100*1000 万=10 亿个字节，将近1G 数据，这会极大影响HFile 的存储效率；<br>②MemStore 将缓存部分数据到内存，如果Rowkey 字段过长内存的有效利用率会降低，系统将无法缓存更多的数据，这会降低检索效率。因此Rowkey 的字节长度越短越好。<br>③目前操作系统是都是64 位系统，内存8 字节对齐。控制在16 个字节，8 字节的整数倍利用操作系统的最佳特性。<br>（2）Rowkey散列原则<br>如果Rowkey是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver 实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个 RegionServer 上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。<br>（3） Rowkey唯一原则<br>必须在设计上保证其唯一性。</p>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>预分区<br>在建表的时候就定义好拆分点的算法，使用org.apache.hadoop.hbase.util.RegionSplitter类来创建表，并传入拆分点算法，就可以在建表同事定义拆分点算法<br>方式：shell、JAVA程序</p>
<p>查询优化<br>setCaching()，用于设置缓存，即设置一次RPC请求可以获取多行数据。这样可以有效的减少服务端与客户端的交互，更有效的提升扫描查询的性能。<br>setBatch 用于设置批量处理，批量可以让用户选择每一次ResultScanner实例的next操作要取回多少列。<br>Cache设置了服务器一次返回的行数，而Batch设置了服务器一次返回的列数。</p>
<p>HFile是数据底层存储文件，在每个memstore进行刷新时会生成一个HFile，当HFile增加到一定程度时，会将属于一个region的HFile进行合并，这个步骤会带来开销但不可避免，但是合并后HFile大小如果大于设定的值，那么HFile会重新分裂。为了减少这样的无谓的I/O开销，建议估计项目数据量大小，给HFile设定一个合适的值。</p>
<h3 id="compact"><a href="#compact" class="headerlink" title="compact"></a>compact</h3><p>在hbase中每当有memstore数据flush到磁盘之后，就形成一个storefile，当storeFile的数量达到一定程度后，就需要将 storefile 文件来进行 compaction 操作。</p>
<p>Compact 的作用：<br>① 合并文件<br>② 清除过期，多余版本的数据<br>③ 提高读写数据的效率</p>
<p>HBase 中实现了两种 compaction 的方式：minor and major. 这两种 compaction 方式的区别是：</p>
<p>1）Minor 操作只用来做部分文件的合并操作以及包括 minVersion=0 并且设置 ttl 的过期版本清理，不做任何删除数据、多版本数据的清理工作。<br>2）Major 操作是对 Region 下的HStore下的所有StoreFile执行合并操作，最终的结果是整理合并出一个文件。</p>
<h3 id="BulkLoad"><a href="#BulkLoad" class="headerlink" title="BulkLoad"></a>BulkLoad</h3><p>使用MapReduce作业以HBase的内部数据格式输出表数据，然后直接将生成的存储文件加载到一个正在运行的集群中。使用批量加载将比简单地使用HBase API消耗更少的CPU和网络资源。<br>原理是使用Mapreduce直接生成HFile格式文件后，RegionServers再将HFile文件移动到相应的Region目录下。<br>有限制，仅适合初次数据导入，以及HBase与Hadoop为同一集群。</p>
<h3 id="HLog-WAL-log"><a href="#HLog-WAL-log" class="headerlink" title="HLog(WAL log)"></a>HLog(WAL log)</h3><p>WAL 意为Write ahead log，类似mysql中的binlog,用来 做灾难恢复只用，Hlog记录数据的所有变更,一旦数据修改，就可以从log中进行恢复。<br>每个Region Server维护一个Hlog,而不是每个Region一个。这样不同region(来自不同table)的日志会混在一起，这样做的目的是不断追加单个 文件相对于同时写多个文件而言，可以减少磁盘寻址次数，因此可以提高对table的写性能。带来的麻烦是，如果一台region server下线，为了恢复其上的region，需要将region server上的log进行拆分，然后分发到其它region server上进行恢复。</p>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><p>1、服务器中输入“hbase shell ” ，连接hbase<br>2、查询所有表 list<br>3、查看表结构 describe ‘user_action_table’<br>4、 查询指定列的数据<br>scan ‘user_action_table’ ,{COLUMNS =&gt;[‘c:action’], LIMIT =&gt;3}<br>5、通过指定列查询 get <table>,<rowkey>,[<a href="family:column">family:column</a>,….] 或 get <table>,<rowkey>,[COLUMN=&gt;<a href="family:column">family:column</a>,….]<br>get ‘qiso_user_action_table’, ‘0000|863739035063055|1575279551000|V_WATCH’, ‘c:action’</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blog.csdn.net/lukabruce/article/details/80624619" target="_blank" rel="noopener">https://blog.csdn.net/lukabruce/article/details/80624619</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>subprocess</title>
    <url>/2020/04/13/subprocess/</url>
    <content><![CDATA[<p>python调用shell比较常用的是subprocess，其他可参看<a href="http://www.cnblogs.com/thinker-lj/p/3860123.html" target="_blank" rel="noopener">http://www.cnblogs.com/thinker-lj/p/3860123.html</a></p>
<p>subprocess的参数：</p>
<table>
<thead>
<tr>
<th align="center">名字</th>
<th align="left">意义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">bufsize</td>
<td align="left">设置缓冲，负数表示系统默认缓冲，0表示无缓冲，正数表示自定义缓冲行数</td>
</tr>
<tr>
<td align="center">stdin</td>
<td align="left">程序的标准输入句柄，NONE表示不进行重定向，继承父进程，PIPE表示创建管道</td>
</tr>
<tr>
<td align="center">stdout</td>
<td align="left">程序的标准输出句柄，参数意义同上</td>
</tr>
<tr>
<td align="center">stderr</td>
<td align="left">程序的标准错误句柄，参数意义同上，特殊，可以设置成STDOUT，表示与标准输出一致</td>
</tr>
<tr>
<td align="center">shell</td>
<td align="left">为True时，表示将通过shell来执行</td>
</tr>
<tr>
<td align="center">cwd</td>
<td align="left">用来设置当前子进程的目录</td>
</tr>
<tr>
<td align="center">env</td>
<td align="left">设置环境变量，为NONE表示继承自父进程的</td>
</tr>
<tr>
<td align="center">universal_newlines</td>
<td align="left">将此参数设置为True，Python统一把这些换行符当作’/n’来处理。</td>
</tr>
</tbody></table>
<p>例子，打开qtalk：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># __author__='chenliclchen'</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute</span><span class="params">(cmd)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> cmd, <span class="string">"is ready executing...."</span></span><br><span class="line">    result = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=<span class="literal">True</span>)</span><br><span class="line">    stderrinfo, stdinfo = result.communicate()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"errinfo:"</span>, stderrinfo</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"info: "</span>, stdinfo</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"code: "</span>, result.returncode</span><br><span class="line"> </span><br><span class="line">cmd = <span class="string">"~/install/qtalk/run.sh &amp;"</span></span><br><span class="line">execute(cmd)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>subprocess</tag>
      </tags>
  </entry>
  <entry>
    <title>定时任务APScheduler</title>
    <url>/2020/04/13/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1APScheduler/</url>
    <content><![CDATA[<h1 id="定时任务APScheduler"><a href="#定时任务APScheduler" class="headerlink" title="定时任务APScheduler"></a>定时任务APScheduler</h1><p>两种使用方法，注解和代码，参考：<a href="http://www.jb51.net/article/117989.htm" target="_blank" rel="noopener">http://www.jb51.net/article/117989.htm</a><br>三种定时，参考：<a href="https://www.cnblogs.com/luxiaojun/p/6567132.html" target="_blank" rel="noopener">https://www.cnblogs.com/luxiaojun/p/6567132.html</a></p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>pip install APScheduler</p>
<h4 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h4><p>（1）、触发器(trigger)<br>　　包含调度逻辑，每一个作业有它自己的触发器，用于决定接下来哪一个作业会运行，根据trigger中定义的时间点，频率，时间区间等等参数设置。除了他们自己初始配置以外，触发器完全是无状态的。<br>（2）、作业存储(job store)<br>　　存储被调度的作业，默认的作业存储是简单地把作业保存在内存中，其他的作业存储是将作业保存在数据库中。一个作业的数据讲在保存在持久化作业存储时被序列化，并在加载时被反序列化。调度器不能分享同一个作业存储。job store支持主流的存储机制：redis, mongodb, 关系型数据库,　内存等等<br>（3）、执行器(executor)<br>　　处理作业的运行，他们通常通过在作业中提交制定的可调用对象到一个线程或者进城池来进行。当作业完成时，执行器将会通知调度器。基于池化的操作，可以针对不同类型的作业任务，更为高效地使用cpu的计算资源。<br>（4）、调度器(scheduler)<br>　　通常在应用只有一个调度器，调度器提供了处理这些的合适的接口。配置作业存储和执行器可以在调度器中完成，例如添加、修改和移除作业。</p>
<p>我们在代码里只关注调度器。<br>这里简单列一下常用的若干调度器：</p>
<p><code>BlockingScheduler</code>：仅可用在当前你的进程之内，与当前的进行共享计算资源<br><code>BackgroundScheduler</code>:　在后台运行调度，不影响当前的系统计算运行<br><code>AsyncIOScheduler</code>:　如果当前系统中使用了async module，则需要使用异步的调度器<br><code>GeventScheduler</code>:　如果使用了gevent，则需要使用该调度<br><code>TornadoScheduler</code>:　如果使用了Tornado, 则使用当前的调度器<br><code>TwistedScheduler</code>:Twister应用的调度器<br><code>QtScheduler</code>:　Qt的调度器</p>
<h4 id="注解使用"><a href="#注解使用" class="headerlink" title="注解使用"></a>注解使用</h4><p>两种使用方法。只介绍注解，另一种可参考上面的链接。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> apscheduler.schedulers.blocking <span class="keyword">import</span> BlockingScheduler</span><br><span class="line">sched = BlockingScheduler()</span><br><span class="line"></span><br><span class="line"><span class="meta">@sched.scheduled_job('cron', day_of_week='mon', hour=10)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scheduled</span><span class="params">()</span>:</span></span><br><span class="line">     do_something......</span><br><span class="line">sched.start()</span><br></pre></td></tr></table></figure>
<p>上面实现的是每周一<code>（day_of_week=&#39;mon&#39;）</code>的10点<code>（hour=10）</code>执行scheduled方法。</p>
<h4 id="三种定时方法"><a href="#三种定时方法" class="headerlink" title="三种定时方法"></a>三种定时方法</h4><p>一共有三种定时方法（只简单介绍，深入了解参考上面第2个链接）。<br>注解的第一个参数是trigger，它管理着作业的调度方式。它可以为date, interval或者cron。对于不同的trigger，对应的参数也相同。<br>(1)interval 是每*都执行。如每分钟执行一次:<br><code>@sched.scheduled_job(&#39;interval&#39;, minutes=1)</code><br>(2)cron一般是每个定时时刻发生，比如3中例子。也可以实现（1）中的那种：<br><code>@sched.scheduled_job(&#39;cron&#39;, minute=&#39;*/1&#39;)</code><br>（3）date是定时只发生一个。</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>APScheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>python+crontab（os）</title>
    <url>/2020/04/13/python+crontab%EF%BC%88os%EF%BC%89/</url>
    <content><![CDATA[<p>使用crontab定时执行python时，os.getcwd() 的返回结果是‘/root’(网上说还可能是’/home’)，而不是python当前的目录。（网上说原因是cwd方法返回的是当前线程的路径，而线程是由crontab在执行的，因此，crontab里的文件都是绝对路径。）<br>如果要使用python脚本当前目录，可以用<code>os.path.abspath(os.path.dirname(__file__))</code></p>
<p><code>os.path.dirname(__file__)</code>  ：<br>它返回的是脚本执行的路径，所以如果就是在脚本当前路径下执行，返回的结果是空。</p>
<p><code>os.path.abspath(file)</code>:<br>返回的是py脚本的当前绝对路径。注意返回结果是包括了filename的，因此一般和上面的命令一起只用，这样就真正的得到了它的绝对路径。</p>
<p><code>os.chdir（）</code>可以切换目录</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line">path1 = os.path.dirname(__file__)</span><br><span class="line">path2 = os.path.abspath(path1)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">print</span> <span class="string">"path1: "</span>, path1</span><br><span class="line"><span class="keyword">print</span> <span class="string">"path2: "</span>, path2</span><br><span class="line">执行：python ./base/test_os_path.py </span><br><span class="line">结果：</span><br><span class="line">path1:  ./base</span><br><span class="line">path2:  /home/chenliclchen/pycharmProject/register/base</span><br><span class="line"> </span><br><span class="line">执行：python test_os_path.py</span><br><span class="line">结果：</span><br><span class="line">path1: </span><br><span class="line">path2:  /home/chenliclchen/pycharmProject/register/base</span><br></pre></td></tr></table></figure>
<p>如果把abspath的参数也换成‘__file__’， 那两次的执行结果都是”path2:  /home/chenliclchen/pycharmProject/register/base/test_os_path.py“</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>os</tag>
      </tags>
  </entry>
  <entry>
    <title>开发还是算法，一直困惑我的问题</title>
    <url>/2020/04/13/%E5%BC%80%E5%8F%91%E8%BF%98%E6%98%AF%E7%AE%97%E6%B3%95%EF%BC%8C%E4%B8%80%E7%9B%B4%E5%9B%B0%E6%83%91%E6%88%91%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>研究生妹纸一枚，方向是深度学习。马上要面临紧张的秋招。</p>
<p>我原本这个项目很多人都不看好，都说不好找工作。那时我是真头疼，马上要找工作了，没有项目经验。后来”人机大战“，阿尔法狗获得胜利。我也觉得没啥，但身边做这个项<br>目的都特别高兴，好像机器人赢了，就是我们程序员的胜利。我自己本身很喜欢编程，喜欢各种的技术。</p>
<p>前一个月是实习生招聘，发现我的方向出奇的好找工作，呵呵（猜测是阿尔法狗对很多企业敲响了警钟）。我心中虽然高兴，但是我仍然迷茫。有次面试，面试官问我”你将来想<br>做算法方向还是开发“，后来我一笔带过，但其实我知道，就算我的方向好找工作，但是就我自己而言，我适合做算法吗，我喜欢做算法吗？什么样的人适合做算法呢？</p>
<p>毕竟是妹纸，身边最多的声音都是”找个银行吧，多好，福利好工资高假期多“，”算法多难啊，我连开发都不想搞，我就想去个银行什么的最好了，还稳定“。我一直在困惑”<br>我呢？我想做什么？“，真是个蛋疼的问题，在这个马上就要找工作的季节。</p>
<p>我一直觉得我虽然喜欢编程，但并不太适合研究。搞算法难免要了解数学，我以前觉得数学枯燥无味与实际编程木有多大关系，所以就没怎么认真学过。。。所以我担心将来搞算<br>法搞的深了是否会运用到很多数学。面试某公司时，也曾问过”如果我做这个岗位，是否需要学习很多算法“，面试官说”其实算法是跟我们的业务相关“。╮(╯▽╰)╭想想<br>都头疼啊。</p>
<p>还有今天老师跟我说”你不要沉迷与python，你的重点应该是算法“。。。。</p>
<p>心烦意乱啊。。。。现在还抱着一本《利用python做数据分析》的书。</p>
]]></content>
      <categories>
        <category>生活杂记</category>
      </categories>
      <tags>
        <tag>生活杂记</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/04/12/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>WordCount与MapReduce计数器</title>
    <url>/2018/11/20/WordCount%E4%B8%8EMapReduce%E8%AE%A1%E6%95%B0%E5%99%A8/</url>
    <content><![CDATA[<p>代码地址：<a href="https://github.com/meihuakaile/mr_test" target="_blank" rel="noopener">https://github.com/meihuakaile/mr_test</a></p>
<h1 id="map实现"><a href="#map实现" class="headerlink" title="map实现"></a>map实现</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.mr.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Counter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span></span></span><br><span class="line"><span class="class">        , <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * map方法是提供给map task进程来调用的，map task进程是每读取一行文本来调用一次我们自定义的map方法</span></span><br><span class="line"><span class="comment">     * map task在调用map方法时，传递的参数：</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key 一行的起始偏移量LongWritable作为key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value 一行的文本内容Text作为value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String[] words = value.toString().split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">if</span> (words.length == <span class="number">1</span>)&#123;</span><br><span class="line"><span class="comment">//            自定义计数器</span></span><br><span class="line">            context.getCounter(<span class="string">"lineCounter"</span>, <span class="string">"one"</span>).increment(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//            枚举计数器</span></span><br><span class="line">            context.getCounter(LOG_LINE_COUNTER.ONE).increment(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (words.length == <span class="number">2</span>)&#123;</span><br><span class="line">            Counter two = context.getCounter(<span class="string">"lineCounter"</span>, <span class="string">"two"</span>);</span><br><span class="line">            two.setValue(<span class="number">100</span>);</span><br><span class="line">            two.increment(<span class="number">1</span>);</span><br><span class="line">            context.getCounter(LOG_LINE_COUNTER.TWO).increment(<span class="number">1</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (words.length == <span class="number">3</span>)&#123;</span><br><span class="line">            context.getCounter(<span class="string">"lineCounter"</span>, <span class="string">"three"</span>).increment(<span class="number">1</span>);</span><br><span class="line">            context.getCounter(LOG_LINE_COUNTER.THREE).increment(<span class="number">1</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            context.getCounter(<span class="string">"lineCounter"</span>, <span class="string">"too long"</span>).increment(<span class="number">1</span>);</span><br><span class="line">            context.getCounter(LOG_LINE_COUNTER.TOO_LONG).increment(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(String word: words)&#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">enum</span> LOG_LINE_COUNTER&#123;</span><br><span class="line">        ONE, TWO, THREE, TOO_LONG</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="reduce实现"><a href="#reduce实现" class="headerlink" title="reduce实现"></a>reduce实现</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.mr.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * reduce方法提供给reduce task进程来调用</span></span><br><span class="line"><span class="comment">     * reduce task会将shuffle阶段分发过来的大量kv数据对进行聚合，聚合的机制是相同key的kv对聚合为一组</span></span><br><span class="line"><span class="comment">     * 然后reduce task对每一组聚合kv调用一次我们自定义的reduce方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key 一组kv中的key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> values</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value: values)&#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="job"><a href="#job" class="headerlink" title="job"></a>job</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.mr.wordcount;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Counter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Counters;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * WordCountJob</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 18-11-9 下午8:46</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountJob</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> Logger logger = Logger.getLogger(WordCountJob<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(configuration, <span class="string">"test"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        指定本job所在的jar包</span></span><br><span class="line">        job.setJarByClass(WordCountJob<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//        设置wordCountJob所用的mapper逻辑类为哪个类</span></span><br><span class="line">        job.setMapperClass(WordCountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//        设置wordCountJob所用的reducer逻辑类为哪个类</span></span><br><span class="line">        job.setReducerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//        设置map阶段输出的kv数据类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//        设置最终输出的kv数据类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//        设置要处理的文本数据所存放的路径</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"><span class="comment">//        自定义计数器用的比较广泛，特别是统计无效数据条数的时候，我们就会用到计数器来记录错误日志的条数。</span></span><br><span class="line">        Counters counters = job.getCounters();</span><br><span class="line">        Counter one = counters.findCounter(WordCountMapper.LOG_LINE_COUNTER.ONE);</span><br><span class="line">        Counter three = counters.findCounter(<span class="string">"lineCounter"</span>, <span class="string">"three"</span>);</span><br><span class="line">        logger.info(<span class="string">"枚举, name: "</span> + one.getName() + <span class="string">" value: "</span> + one.getValue());</span><br><span class="line">        logger.info(<span class="string">"自定义, name: "</span> + three.getName() + <span class="string">" value: "</span> + three.getValue());</span><br><span class="line"></span><br><span class="line"><span class="comment">//        与shell是否成功返回结果保持一致</span></span><br><span class="line">        <span class="keyword">return</span> result? <span class="number">0</span>: <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h1><p>计数器参考：<a href="https://www.cnblogs.com/codeOfLife/p/5521356.html" target="_blank" rel="noopener">https://www.cnblogs.com/codeOfLife/p/5521356.html</a></p>
<p>MapReduce 的计数器Counter用以观察mr执行的详细信息，如进度等。mr自带了很多计数器，如job个数、map个数、<br>map输入行数等（可以参看上面链接）。<br>我主要用的是自定义计数器，计数器从另一方便面来说就是日志。特别是在我们记录无效数据时特别有用（比如解析多个日志文件时，统计解析失败的条数。毕竟，如果数据量很大，不能就throw掉不管吧）。</p>
<p>自定义计数器经常用的几个方法：<br>1、定义计数器<br>   1)枚举声明计数器</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 自定义枚举变量Enum </span></span><br><span class="line">Counter counter = context.getCounter(Enum <span class="keyword">enum</span>)</span><br></pre></td></tr></table></figure>
<p>  2)自定义计数器</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 自己命名groupName和counterName </span></span><br><span class="line">Counter counter = context.getCounter(String groupName,String counterName)</span><br></pre></td></tr></table></figure>

<p>2、为计数器赋值<br>   1)初始化计数器</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">counter.setValue(<span class="keyword">long</span> value);<span class="comment">// 设置初始值</span></span><br></pre></td></tr></table></figure>
<p>   2)计数器自增<br><code>counter.increment(long incr);// 增加计数</code></p>
<p>3、获取计数器的值</p>
<p>   1) 获取自定义/枚举计数器的值</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Counters counters = job.getCounters();</span><br><span class="line">Counter one = counters.findCounter(WordCountMapper.LOG_LINE_COUNTER.ONE);</span><br><span class="line">Counter three = counters.findCounter(<span class="string">"lineCounter"</span>, <span class="string">"three"</span>);</span><br><span class="line">logger.info(<span class="string">"枚举, name: "</span> + one.getName() + <span class="string">" value: "</span> + one.getValue());</span><br><span class="line">logger.info(<span class="string">"自定义, name: "</span> + three.getName() + <span class="string">" value: "</span> + three.getValue());</span><br></pre></td></tr></table></figure>
<p>  2) 获取内置计数器的值</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.waitForCompletion(<span class="keyword">true</span>); </span><br><span class="line">Counters counters=job.getCounters(); </span><br><span class="line"><span class="comment">// 查找作业运行启动的reduce个数的计数器，groupName和counterName可以从内置计数器表格查询（前面已经列举有） </span></span><br><span class="line">Counter counter=counters.findCounter(<span class="string">"org.apache.hadoop.mapreduce.JobCounter"</span>,<span class="string">"TOTAL_LAUNCHED_REDUCES"</span>);<span class="comment">// 假如groupName为org.apache.hadoop.mapreduce.JobCounter，counterName为TOTAL_LAUNCHED_REDUCES </span></span><br><span class="line"><span class="keyword">long</span> value=counter.getValue();<span class="comment">// 获取计数值</span></span><br></pre></td></tr></table></figure>
<p>  3) 获取所有计数器的值</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Counters counters = job.getCounters(); </span><br><span class="line"><span class="keyword">for</span> (CounterGroup group : counters) &#123; </span><br><span class="line">  <span class="keyword">for</span> (Counter counter : group) &#123; </span><br><span class="line">    System.out.println(counter.getDisplayName() + <span class="string">": "</span> + counter.getName() + <span class="string">": "</span>+ counter.getValue()); </span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>hdfs</title>
    <url>/2018/10/28/hdfs/</url>
    <content><![CDATA[<p><img src="1.png" alt=""></p>
<h3 id="Namenode"><a href="#Namenode" class="headerlink" title="Namenode"></a>Namenode</h3><ul>
<li>EditLog:对于 任何对文件系统元数据产生修改 的操作， Namenode 都会使用一种称为 EditLog 的事务日志记录下来。</li>
<li>FsImage:整个文件系统的命名空间 ，包括数据块到文件的映射、文件的属性等，都存储在一个称为 FsImage 的文件中</li>
</ul>
<h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>Datanode 将 HDFS 数据以文件的形式存储在本地的文件系统中，它并不知道有关 HDFS 文件的信息。它把每个 HDFS 数据块存储在本地文件系统的一个单独的文件中。</p>
<p>当一个 Datanode 启动时，它会扫描本地文件系统，产生一个这些本地文件对应的所有 HDFS 数据块的列表，然后作为报告发送到 Namenode ，这个报告就是块状态报告。</p>
<h3 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h3><p>Secondary NameNode 定期合并 fsimage 和 edits 日志，将 edits 日志文件大小控制在一个限度下。<br>Secondary NameNode处理流程</p>
<p>(1) 、 namenode 响应 Secondary namenode 请求，将 edit log 推送给 Secondary namenode ， 开始重新写一个新的 edit log 。<br>(2) 、 Secondary namenode 收到来自 namenode 的 fsimage 文件和 edit log 。<br>(3) 、 Secondary namenode 将 fsimage 加载到内存，应用 edit log ， 并生成一 个新的 fsimage 文件。<br>(4) 、 Secondary namenode 将新的 fsimage 推送给 Namenode 。<br>(5) 、 Namenode 用新的 fsimage 取代旧的 fsimage ， 在 fstime 文件中记下检查 点发生的时</p>
<h3 id="HDFS的安全模式"><a href="#HDFS的安全模式" class="headerlink" title="HDFS的安全模式"></a>HDFS的安全模式</h3><p>Namenode 启动后会进入一个称为安全模式的特殊状态。处于安全模式 的 Namenode 是不会进行数据块的复制的。 Namenode 从所有的 Datanode 接收心跳信号和块状态报告。块状态报告包括了某个 Datanode 所有的数据 块列表。每个数据块都有一个指定的最小副本数。当 Namenode 检测确认某 个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全 (safely replicated) 的;在一定百分比(这个参数可配置)的数据块被 Namenode 检测确认是安全之后(加上一个额外的 30 秒等待时间)， Namenode 将退出安全模式状态。接下来它会确定还有哪些数据块的副本没 有达到指定数目，并将这些数据块复制到其他 Datanode 上。</p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka</title>
    <url>/2018/10/28/kafka/</url>
    <content><![CDATA[<p>一个主题topic有多个分区。<br>一个消费者可以消费多个 分区。但是一个分区只能有一个消费者。<br>当以下事件发生时，Kafka将会进行一次分区分配：</p>
<ul>
<li>同一个ConsumerGroup内新增消费者；</li>
<li>消费者离开当前所属的ConsumerGroup，包括shutsdown或crashes；</li>
<li>订阅的主题新增分区；</li>
</ul>
<p>如果你的分区数是N，那么最好线程数也保持为N，这样通常能够达到最大的吞吐量。超过N的配置只是浪费系统资源，因为多出的线程不会被分配到任何分区。<br>–num.streams N 消费者线程数量</p>
<h2 id="Producer分区机制"><a href="#Producer分区机制" class="headerlink" title="Producer分区机制"></a>Producer分区机制</h2><p>当指定partition key的时候，分配partition的策略：</p>
<ul>
<li>hash：由消息所提供的key来进行hash，然后分发到对应的partition。这是默认使用的partition机制。</li>
<li>自定义：自己实现partition接口，并在配置中用参数<code>partitioner.class</code>指定这个实现。</li>
</ul>
<p>当没有指定partition key的时候，分配partition的策略：<br>随机：把每个消息随机分发到一个partition中。在10分钟内，该partition不会切换。所以，当producer数目小于partition时，在一定时间内会有部分partition没有收到数据。</p>
<h2 id="Consumer消费策略"><a href="#Consumer消费策略" class="headerlink" title="Consumer消费策略"></a>Consumer消费策略</h2><p>Kafka提供的两种Consumer消费Partition的分配策略：range和roundrobin，由参数partition.assignment.strategy指定，默认是range策略。</p>
<h3 id="range策略"><a href="#range策略" class="headerlink" title="range策略"></a>range策略</h3><p>Range策略是<strong><em>对每个主题而言</em></strong>的，首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。在我们的例子里面，排完序的分区将会是0, 1, 2, 3, 4, 5, 6, 7, 8, 9；消费者线程排完序将会是C1-0, C2-0, C2-1。然后将partitions的个数除于消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。在我们的例子里面，我们有10个分区，3个消费者线程， 10 / 3 = 3，而且除不尽，那么消费者线程 C1-0 将会多消费一个分区，所以最后分区分配的结果看起来是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C1-0 将消费 0, 1, 2, 3 分区</span><br><span class="line">C2-0 将消费 4, 5, 6 分区</span><br><span class="line">C2-1 将消费 7, 8, 9 分区</span><br></pre></td></tr></table></figure>
<p>假如我们有11个分区，那么最后分区分配的结果看起来是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C1-0 将消费 0, 1, 2, 3 分区</span><br><span class="line">C2-0 将消费 4, 5, 6, 7 分区</span><br><span class="line">C2-1 将消费 8, 9, 10 分区</span><br></pre></td></tr></table></figure>
<p>假如我们有2个主题(T1和T2)，分别有10个分区，那么最后分区分配的结果看起来是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C1-0 将消费 T1主题的 0, 1, 2, 3 分区以及 T2主题的 0, 1, 2, 3分区</span><br><span class="line">C2-0 将消费 T1主题的 4, 5, 6 分区以及 T2主题的 4, 5, 6分区</span><br><span class="line">C2-1 将消费 T1主题的 7, 8, 9 分区以及 T2主题的 7, 8, 9分区</span><br></pre></td></tr></table></figure>
<p>Range策略中一个很明显的短板就是对于多topic的情况，可能会存在某一个消费线程压力过大的问题，无法做到真正的均衡。</p>
<h3 id="roundrobin策略"><a href="#roundrobin策略" class="headerlink" title="roundrobin策略"></a>roundrobin策略</h3><p>使用RoundRobin策略有两个前提条件必须满足：</p>
<ul>
<li>同一个Consumer Group里面的所有消费者的num.streams必须相等；</li>
<li>每个消费者订阅的主题必须相同。</li>
</ul>
<p>所以这里假设前面提到的2个消费者的num.streams = 2。<br>RoundRobin策略的工作原理：将<strong><em>所有主题的分区</em></strong>组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序。在例子里面，加入按照 hashCode 排序完的topic-partitions组依次为T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9，我们的消费者线程排序为C1-0, C1-1, C2-0, C2-1，最后分区分配的结果为：最后按照round-robin风格将分区分别分配给不同的消费者线程。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C1-0 将消费 T1-5, T1-2, T1-6 分区；</span><br><span class="line">C1-1 将消费 T1-3, T1-1, T1-9 分区；</span><br><span class="line">C2-0 将消费 T1-0, T1-4 分区；</span><br><span class="line">C2-1 将消费 T1-8, T1-7 分区；</span><br></pre></td></tr></table></figure>
<p>如果所有consumer实例有相同的consumer group，那么这个就像传统的队列，负载均衡到所有consumer上。假如多个consumer实例都有多个线程，且属于同一个group，那么一个topic的所有partition会均匀分配给所有线程。<br>接收消息的顺序只能保证一个partition之内是有序的，一个consumer接收多个partition的话是无法保证消息全局有序的，即consumer接收的消息的顺序可能跟producer发送的顺序不同。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/WangQYoho/article/details/76169514" target="_blank" rel="noopener">https://blog.csdn.net/WangQYoho/article/details/76169514</a><br><a href="http://lsr1991.github.io/2015/07/09/kafka-partition-mechanism/" target="_blank" rel="noopener">http://lsr1991.github.io/2015/07/09/kafka-partition-mechanism/</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>spark</title>
    <url>/2018/10/28/spark/</url>
    <content><![CDATA[<p>来自：<br><a href="https://blog.csdn.net/djd1234567/article/details/51785900" target="_blank" rel="noopener">美团点评 spark性能优化指南-基础篇</a><br><a href="https://blog.csdn.net/l18930738887/article/details/51781313" target="_blank" rel="noopener">美团点评 spark性能优化指南-高级篇</a></p>
<p>spark本身并没有提供分布式文件系统，因此spark的分析大多依赖于Hadoop的分布式文件系统HDFS.<br>Hadoop的Mapreduce与spark都可以进行数据计算，而相比于Mapreduce，spark的速度更快并且提供的功能更加丰富.<br>spark在没有shuffle时比hadoop快，因为不用中间落地，但在有shuffle时，效率一样。<br>Spark相对与MapReduce的优势有：低延迟、支持DAG和分布式内存计算。</p>
<h3 id="提交作业流程"><a href="#提交作业流程" class="headerlink" title="提交作业流程"></a>提交作业流程</h3><p><img src="1.jpg" alt=""><br>spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。<br>根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。<br>Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。<br>Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，<strong><em>这里的资源指的就是Executor进程</em></strong>。<br>YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p>
<p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了：<br>Driver进程会将我们编写的Spark作业代码分拆为多个stage，<br>每个stage执行一部分代码片段，并为每个stage创建一批task，<br>然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。<br>一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，<br>然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。</p>
<p>如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p>
<p>一个作业 —&gt; 多个stage 通过shuffle划分<br>一个stage —&gt; 执行部分代码<br>一个stage —&gt; 多个task  每个task执行相同逻辑代码，处理数据不同。</p>
<p>一个stage的所有task都执行完毕之后，会在<strong><em>各个节点本地的磁盘文件</em></strong>中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止<br>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。</p>
<p>shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。<br>（注：hadoop只有一种shuffle，Spark有两种，hash和sort。Spark1.6之前用hash shuffle，在spark1.6之后使用sort shuffle）</p>
<p>Executor的内存主要分为三块：</p>
<ul>
<li>第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；</li>
<li>第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；</li>
<li>第三块是让RDD持久化时使用，默认占Executor总内存的60%。</li>
</ul>
<p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p>
<h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming:"></a>Spark Streaming:</h3><p><img src="2.png" alt=""><br>1、Spark Streaming用于处理流式计算问题。能够和Spark的其他模块无缝集成。<br>2、Spark Streaming是一个粗粒度的框架【也就是只能对一批数据指定处理方法】，核心是采用微批次架构。和Storm采用的以条处理的不同。<br>3、Spark Streaming会运行接收器来不断的接收输入的数据流，然后根据程序配置的时间，将时间范围内的所有数据打成一个RDD，发送给Spark Core去进行处理。依次来打成对数据流的计算。<br>4、Spark Streaming有它自己的抽象，叫DStream Discretized Stream离散化流<br>5、如果入水口的速度大于出水口的速度，那么势必导致水管爆裂，Spark Streaming也存在这个问题，内部采用背压机制来进行处理，会通过ReceiverRateController来不断计算RDD的处理速度和RDD的生成速度，来通过令牌桶机制进行速度控制。只要是控制令牌的生成周期。</p>
<h3 id="rdd"><a href="#rdd" class="headerlink" title="rdd"></a>rdd</h3><p>RDD（Resilient Distributed Datasets弹性分布式数据集）:spark的数据集，相当于在hdfs上包装了一层。<br>1、RDD是Spark提供的核心抽象，全称为Resillient Distributed Dataset，即弹性分布式数据集。<br>2、RDD在抽象上来说是一种元素集合，包含了数据。它是<strong><em>被分区</em></strong>的，分为多个分区，每个分区分布在集群中的不同节点上，从而让RDD中的数据可以被并行操作。（分布式数据集）<br>3、RDD通常通过Hadoop上的文件，即<strong><em>HDFS文件或者Hive表(spark_sql可直接读)</em></strong>，来进行创建；有时也可以通过应用程序中的集合来创建。<br>4、RDD最重要的特性就是，提供了<strong><em>容错性</em></strong>，可以自动从节点失败中恢复过来。即如果某个节点上的RDD partition，因为节点故障，导致数据丢了，那么RDD会自动通过自己的数据来源重新计算该partition。这一切对使用者是透明的。<br>5、RDD的数据默认情况下存放在内存中的，但是在内存资源不足时，Spark会自动将RDD数据写入磁盘。（<strong><em>弹性</em></strong>）</p>
<p><del>在Spark编程时，不仅仅只有reduce才会产生shuffle过程，RDD提供的groupByKey，countApproxDistinctByKey等操作都会生成shuffle。Spark中shuffle的实现与MapReduce的shuffle有比较大的差别，首先是map阶段，map的输出不再需要排序，直接写到文件中，一个map会把属于不同reduce的数据分别输出到不同的文体中，而reduce则通过aggregator处理所有shuffle fetch获取的partition。</del><br>开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p>
<h3 id="持久化调优"><a href="#持久化调优" class="headerlink" title="持久化调优"></a>持久化调优</h3><p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。<br>就上面的问题，建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span><br><span class="line"> </span><br><span class="line">// 正确的做法。</span><br><span class="line">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span><br><span class="line">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span><br><span class="line">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span><br><span class="line">val rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"> </span><br><span class="line">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span><br><span class="line">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span><br><span class="line">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span><br><span class="line">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span><br><span class="line">val rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).persist(StorageLevel.MEMORY_AND_DISK_SER)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure>
<p>如何选择一种最合适的持久化策略</p>
<ul>
<li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li>
<li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li>
<li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li>
<li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li>
</ul>
<h3 id="避免shuffle"><a href="#避免shuffle" class="headerlink" title="避免shuffle"></a>避免shuffle</h3><p>如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。<br>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。<br>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p>
<p>Broadcast与map代替join，代码示例（本身理解，和hive的mapjoin一样）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">// 传统的join操作会导致shuffle操作。</span><br><span class="line">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span><br><span class="line">val rdd3 = rdd1.join(rdd2)</span><br><span class="line"> </span><br><span class="line">// Broadcast+map的join操作，不会导致shuffle操作。</span><br><span class="line">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span><br><span class="line">val rdd2Data = rdd2.collect()</span><br><span class="line">val rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"> </span><br><span class="line">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span><br><span class="line">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span><br><span class="line">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span><br><span class="line">val rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"> </span><br><span class="line">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span><br><span class="line">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span><br></pre></td></tr></table></figure>
<p>Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。<br>遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p>
<p>使用map-side预聚合的shuffle操作：</p>
<p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。<br>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，<strong><em>类似于MapReduce中的本地combiner</em></strong>。map-side预聚合之后，<strong><em>每个节点本地</em></strong>就只会<strong><em>有一条相同的key</em></strong>，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，<strong><em>建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子</em></strong>。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p>
<h3 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h3><p>在Spark中，主要有三个地方涉及到了序列化：</p>
<ul>
<li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li>
<li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li>
<li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。<br>Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多.<br>官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</li>
</ul>
<p>参考：<br>（Spark Streaming）<a href="https://blog.csdn.net/liangzelei/article/details/80661963" target="_blank" rel="noopener">https://blog.csdn.net/liangzelei/article/details/80661963</a><br>（RDD）<a href="https://blog.csdn.net/dong515299210/article/details/81584256" target="_blank" rel="noopener">https://blog.csdn.net/dong515299210/article/details/81584256</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>azkaban</title>
    <url>/2018/10/19/azkaban%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h2 id="出错"><a href="#出错" class="headerlink" title="出错"></a>出错</h2><p>1、<code>JCE报错</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">testV1_1 FAILED</span><br><span class="line">    java.lang.RuntimeException: java.lang.RuntimeException: org.jasypt.exceptions.EncryptionOperationNotPossibleException: Encryption raised an exception. A possible cause is you are using strong encryption algorithms and you have not installed the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files <span class="keyword">in</span> this Java Virtual Machine</span><br><span class="line">        at azkaban.crypto.Crypto.decrypt(Crypto.java:76)</span><br><span class="line">        at azkaban.crypto.DecryptionTest.testV1_1(DecryptionTest.java:35)</span><br><span class="line"></span><br><span class="line">        Caused by:</span><br><span class="line">        java.lang.RuntimeException: org.jasypt.exceptions.EncryptionOperationNotPossibleException: Encryption raised an exception. A possible cause is you are using strong encryption algorithms and you have not installed the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files <span class="keyword">in</span> this Java Virtual Machine</span><br><span class="line">            at azkaban.crypto.CryptoV1_1.decrypt(CryptoV1_1.java:57)</span><br><span class="line">            at azkaban.crypto.Crypto.decrypt(Crypto.java:74)</span><br><span class="line">            ... 1 more</span><br><span class="line"></span><br><span class="line">            Caused by:</span><br><span class="line">            org.jasypt.exceptions.EncryptionOperationNotPossibleException: Encryption raised an exception. A possible cause is you are using strong encryption algorithms and you have not installed the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files <span class="keyword">in</span> this Java Virtual Machine</span><br><span class="line">                at org.jasypt.encryption.pbe.StandardPBEByteEncryptor.handleInvalidKeyException(StandardPBEByteEncryptor.java:1073)</span><br><span class="line">                at org.jasypt.encryption.pbe.StandardPBEByteEncryptor.decrypt(StandardPBEByteEncryptor.java:1050)</span><br><span class="line">                at org.jasypt.encryption.pbe.StandardPBEStringEncryptor.decrypt(StandardPBEStringEncryptor.java:725)</span><br><span class="line">                at azkaban.crypto.CryptoV1_1.decrypt(CryptoV1_1.java:55)</span><br><span class="line">                ... 2 more</span><br><span class="line"></span><br><span class="line">azkaban.crypto.EncryptionTest &gt; testEncryption FAILED</span><br><span class="line">    org.jasypt.exceptions.EncryptionOperationNotPossibleException: Encryption raised an exception. A possible cause is you are using strong encryption algorithms and you have not installed the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files <span class="keyword">in</span> this Java Virtual Machine</span><br><span class="line">        at org.jasypt.encryption.pbe.StandardPBEByteEncryptor.handleInvalidKeyException(StandardPBEByteEncryptor.java:1073)</span><br><span class="line">        at org.jasypt.encryption.pbe.StandardPBEByteEncryptor.encrypt(StandardPBEByteEncryptor.java:924)</span><br><span class="line">        at org.jasypt.encryption.pbe.StandardPBEStringEncryptor.encrypt(StandardPBEStringEncryptor.java:642)</span><br><span class="line">        at azkaban.crypto.CryptoV1_1.encrypt(CryptoV1_1.java:42)</span><br><span class="line">        at azkaban.crypto.Crypto.encrypt(Crypto.java:58)</span><br><span class="line">        at azkaban.crypto.EncryptionTest.testEncryption(EncryptionTest.java:28)</span><br><span class="line">        </span><br><span class="line">5 tests completed, 2 failed  </span><br><span class="line">&gt; Task :azkaban-common:compileJava </span><br><span class="line">注: 某些输入文件使用或覆盖了已过时的 API。</span><br><span class="line">注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。</span><br><span class="line">注: 某些输入文件使用了未经检查或不安全的操作。</span><br><span class="line">注: 有关详细信息, 请使用 -Xlint:unchecked 重新编译。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">FAILURE: Build failed with an exception.</span><br><span class="line"></span><br><span class="line">* What went wrong:</span><br><span class="line">Execution failed <span class="keyword">for</span> task <span class="string">':az-crypto:test'</span>.</span><br><span class="line">&gt; There were failing tests. See the report at: file:///opt/azkaban/az-crypto/build/reports/tests/<span class="built_in">test</span>/index.html</span><br><span class="line"></span><br><span class="line">* Try:</span><br><span class="line">Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more <span class="built_in">log</span> output. Run with --scan to get full insights.</span><br><span class="line"></span><br><span class="line">* Get more <span class="built_in">help</span> at https://help.gradle.org</span><br></pre></td></tr></table></figure>
<p>解决：<br>(下载JCE,使用的JDK8，包含了JCE所需要的jre8)[<a href="https://www.oracle.com/technetwork/cn/java/javase/downloads/jce8-download-2133166-zhs.html]" target="_blank" rel="noopener">https://www.oracle.com/technetwork/cn/java/javase/downloads/jce8-download-2133166-zhs.html]</a><br>解压后放置到jdk所在目录下（使用<code>which java</code>找java目录）的：<br><code>cp UnlimitedJCEPolicyJDK8/* /usr/local/jdk1.8.0_74/jre/lib/security</code></p>
<p>参考：<a href="https://juejin.im/entry/5bbd92e9e51d4539701e85ab" target="_blank" rel="noopener">https://juejin.im/entry/5bbd92e9e51d4539701e85ab</a></p>
<p>2、<code>keytool 错误: java.io.FileNotFoundException: keystore (权限不够)</code><br>执行<code>keytool -keystore keystore -alias jetty -genkey -keyalg RSA</code>出错。<br>解决：<code>-keystore keystore</code>这个参数的原因，当前目录没有写权限。只需把换成有权限的目录，再把文件拷到当前目录即可。</p>
<p>参考：<a href="http://www.cnblogs.com/mengshu-lbq/archive/2012/05/16/2503564.html" target="_blank" rel="noopener">http://www.cnblogs.com/mengshu-lbq/archive/2012/05/16/2503564.html</a></p>
<p>3、<code>azkaban.executor.ExecutorManagerException: No active executors found</code><br>解决：<br>数据库 <code>INSERT INTO executors(host,port, active) VALUES(&quot;executor_ip&quot;, 12321, true)</code><br>配置文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">executor.port=12321</span><br><span class="line">azkaban.use.multiple.executors=<span class="literal">true</span></span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,MinimumFreeMemory,CpuStatus</span><br><span class="line">azkaban.executorselector.comparator.NumberOfAssignedFlowComparator=1</span><br><span class="line">azkaban.executorselector.comparator.Memory=1</span><br><span class="line">azkaban.executorselector.comparator.LastDispatched=1</span><br><span class="line">azkaban.executorselector.comparator.CpuUsage=1</span><br></pre></td></tr></table></figure>

<p>参考：<a href="https://zzyongx.github.io/blogs/azkaban.html" target="_blank" rel="noopener">https://zzyongx.github.io/blogs/azkaban.html</a></p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>配置文件<br>注：先配置好服务器节点上的时区<br>1、先生成时区配置文件Asia/Shanghai，用交互式命令 tzselect 即可<br>2、拷贝该时区文件，覆盖系统本地时区配置<br>cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</p>
<h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p><code>git clone https://github.com/azkaban/azkaban.git</code></p>
<h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>进入到Azkaban的根目录下面，执行<code>./gradlew build</code>，进行编译，如出错请参考上面<br>编译好的文件都放在build/distributions/目录下<br>执行<code>cp –r azkaban-*/build/distributions/*.tar.gz /opt/</code> 拷贝编译好的tar.gz包</p>
<p>参考：<a href="https://blog.csdn.net/ProfoundOx/article/details/78888183（比较靠谱的安装）" target="_blank" rel="noopener">https://blog.csdn.net/ProfoundOx/article/details/78888183（比较靠谱的安装）</a><br><a href="https://blog.csdn.net/weixin_35852328/article/details/79327996" target="_blank" rel="noopener">https://blog.csdn.net/weixin_35852328/article/details/79327996</a></p>
<h2 id="redash安装"><a href="#redash安装" class="headerlink" title="redash安装"></a>redash安装</h2><p><a href="http://172.18.0.1:5000/" target="_blank" rel="noopener">http://172.18.0.1:5000/</a><br>参考：<a href="https://blog.csdn.net/diantun00/article/details/80968604" target="_blank" rel="noopener">https://blog.csdn.net/diantun00/article/details/80968604</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>hive出错</title>
    <url>/2018/10/19/hive%E5%87%BA%E9%94%99/</url>
    <content><![CDATA[<h3 id="易错"><a href="#易错" class="headerlink" title="易错"></a>易错</h3><p>hive cli 有tab补全的功能，因此，如果hql里有tab时，会出现<code>Display all 479 possibilities? (y or n)</code>的询问。<br><code>left/right join on where</code>时注意条件放在on之后还是where之后，结果会不同。 </p>
<h3 id="读orc格式数据"><a href="#读orc格式数据" class="headerlink" title="读orc格式数据"></a>读orc格式数据</h3><p>hive-0.11版本中的使用方法为：<code>hive --orcfiledump &lt;location-of-orc-file&gt;</code>，其他版本的使用方法可以去官方文档中查找。</p>
<p>出错场景：用hive读orc时出错，使用hive版本是0.11，参考hive官网<a href="http://www.bieryun.com/2447.html" target="_blank" rel="noopener">http://www.bieryun.com/2447.html</a> 讲解Hive version 0.11 through 0.14都可以使用上面的方法读orc文件。<br>之后使用1.2.2的版本使用上面命令不会出错，可以select读数据。</p>
<p>使用时出错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; com.google.protobuf.InvalidProtocolBufferException: Message missing required fields: columns[1].kind, columns[2].kind, columns[3].kind</span><br><span class="line">at com.google.protobuf.UninitializedMessageException.asInvalidProtocolBufferException(UninitializedMessageException.java:81)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter$Builder.buildParsed(OrcProto.java:5908)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter$Builder.access$10700(OrcProto.java:5834)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter.parseFrom(OrcProto.java:5779)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripeFooter(RecordReaderImpl.java:1108)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:1114)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.&lt;init&gt;(RecordReaderImpl.java:94)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rows(ReaderImpl.java:242)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rows(ReaderImpl.java:236)</span><br><span class="line">at org.apache.hadoop.hive.ql.io.orc.FileDump.main(FileDump.java:37)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke(Method.java:606)</span><br><span class="line">at org.apache.hadoop.util.RunJar.main(RunJar.java:212)</span><br></pre></td></tr></table></figure>
<p>分析：orc的表是别人建的，无法确定当初建表的hive的版本。google.protobuf是一个作为协议的包，类似于序列化。因此猜测是不同版本的hive的orc不一样导致压缩数据和解压数据无法连起来。</p>
<h3 id="使用SQL2011保留字出错"><a href="#使用SQL2011保留字出错" class="headerlink" title="使用SQL2011保留字出错"></a>使用SQL2011保留字出错</h3><p>报错：<code>Failed to recognize predicate &#39;xxx&#39;. Failed rule: &#39;identifier&#39; in column specification</code><br>原因：HQL中出现<code>row</code>作为字段名。在Hive1.2.0版本开始增加了如下配置选项，默认值为true：<br><code>hive.support.sql11.reserved.keywords</code>该选项的目的是：是否启用对SQL2011保留关键字的支持。 启用后，将支持部分SQL2011保留关键字。<br>解决方法（1）：放弃<code>row</code>，换一个关键字。<br>解决方法（2）：弃用对保留关键字的支持。<code>set hive.support.sql11.reserved.keywords = false ;</code><br>解决方法（3）：弃用对保留关键字的支持。在conf下的hive-site.xml配置文件中修改配置选项：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.support.sql11.reserved.keywords<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>总结自：<a href="https://blog.csdn.net/SJF0115/article/details/73244762" target="_blank" rel="noopener">https://blog.csdn.net/SJF0115/article/details/73244762</a></p>
<h3 id="join-on中比较大小报错"><a href="#join-on中比较大小报错" class="headerlink" title="join on中比较大小报错"></a>join on中比较大小报错</h3><p>报错：<code>Both left and right aliases encountered in JOIN &#39;1&#39;</code><br>原因：两个表join的时候，不支持两个表的字段 非相等 操作。Hive 不支持所有非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。<br>解决：可以把不相等条件拿到 where语句中。</p>
<h3 id="使用distinct报错"><a href="#使用distinct报错" class="headerlink" title="使用distinct报错"></a>使用distinct报错</h3><p>报错<code>HIVE: cannot recognize input near &#39;distinct&#39; &#39;(&#39;</code><br>原因：hive的语法中select的字段分两种，all和distinct，默认是all，加distinct的字段必须放在所有查询字段的最前面（mysql也是）。<br>总结自：<a href="https://stackoverflow.com/questions/38794766/hive-cannot-recognize-input-near-distinct" target="_blank" rel="noopener">https://stackoverflow.com/questions/38794766/hive-cannot-recognize-input-near-distinct</a></p>
<h3 id="memory-limits"><a href="#memory-limits" class="headerlink" title="memory limits"></a>memory limits</h3><p>报错：<code>is running beyond physical memory limits. Current usage: 2.0 GB of 2 GB physical memory used; 9.9 GB of 40 GB virtual memory used.</code><br>分析：<code>2.0 GB of 2 GB physical memory used</code> 物理内存溢出 OOM为out of memory<br>解决：设置mapper和reducer 物理内存和虚拟内存<br><code>set mapreduce.map.memory.mb=10240;</code>  container的内存 运行mapper的容器的物理内存，1024M = 1G<br><code>set mapreduce.map.java.opts=&#39;-Xmx7680M&#39;;</code>  jvm堆内存<br><code>set mapreduce.reduce.memory.mb=10240;</code><br><code>set mapreduce.reduce.java.opts=&#39;-Xmx7680M&#39;;</code><br>在yarn container这种模式下，map/reduce task是运行在Container之中的，所以上面提到的mapreduce.map(reduce).memory.mb大小<strong><em>都大于</em></strong>mapreduce.map(reduce).java.opts值的大小。mapreduce.{map|reduce}.java.opts能够通过Xmx设置JVM最大的heap的使用，<strong><em>一般设置为0.75倍的memory.mb，因为需要为java code等预留些空间</em></strong>。</p>
<p>来源于网络：虚拟内存的计算由 物理内存 和 yarn-site.xml中的yarn.nodemanager.vmem-pmem-ratio制定。<br><code>yarn.nodemanager.vmem-pmem-ratio</code>是 一个比例，默认是2.1   虚拟内存 = 物理内存 × 这个比例<br>yarn.nodemanager.vmem-pmem-ratio 的比率，默认是2.1.这个比率的控制影响着虚拟内存的使用，当yarn计算出来的虚拟内存，比在mapred-site.xml里的mapreduce.map.memory.mb或mapreduce.reduce.memory.mb的2.1倍还要多时，会被kill掉。</p>
<p>参考：<a href="https://blog.csdn.net/yisun123456/article/details/81327372" target="_blank" rel="noopener">https://blog.csdn.net/yisun123456/article/details/81327372</a></p>
<h3 id="dynamic-partitions"><a href="#dynamic-partitions" class="headerlink" title="dynamic partitions"></a>dynamic partitions</h3><p><code>The maximum number of dynamic partitions is controlled by hive.exec.max.dynamic.partitions and hive.exec.max.dynamic.partitions.pernode. Maximum was set to: 100</code><br>原因看报错，显而易见，动态分区超过最大动态分区，默认100.<br>解决：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET hive.exec.max.dynamic.partitions&#x3D;2048;</span><br><span class="line">SET hive.exec.max.dynamic.partitions.pernode&#x3D;256;</span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://blog.csdn.net/oDaiLiDong/article/details/49884571" target="_blank" rel="noopener">https://blog.csdn.net/oDaiLiDong/article/details/49884571</a></p>
<p><del>### join出错</del><br><del>报错<code>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask</code></del><br>一个join操作导致的，原因不明。<br>网上查到解决办法：<code>SET hive.auto.convert.join=false;</code> 此操作的原理看上面“join原理、调优”章节里的“map端的join”。<br>参考：<a href="https://www.cnblogs.com/MOBIN/p/5702580.html" target="_blank" rel="noopener">https://www.cnblogs.com/MOBIN/p/5702580.html</a></p>
<h3 id="jobname过长"><a href="#jobname过长" class="headerlink" title="jobname过长"></a>jobname过长</h3><p>报错<code>hive java.io.IOException: Could not find status of job:job_15416269223_3720383</code><br>hive的运行原理：当任务执行结束后，对于每一个jobid，会根据一定规则，生成两个文件，一个是<em>.jhist,另一个是</em>.conf.这两个文件记录了这个job的所有执行信息，这两个文件是要写入到jobhistory所监控的路径的。<br>这部分是根据hive的jobname来决定的，默认是从hql中的开头和结尾截取一部分，如果sql开头或结尾有中文注释的话，会被截取进来，并进行url编码。导致作业的信息名变的非常长，超过了namenode所允许的最大的文件命名长度。导致任务无法写入historyserver。hive在historyserver无法获得这个job的状态，报开头的错误。<br>这里提供一个简单的解决办法：<code>set  hive.jobname.length=10;</code></p>
<p>原文：<a href="https://blog.csdn.net/zhoudetiankong/article/details/52126760" target="_blank" rel="noopener">https://blog.csdn.net/zhoudetiankong/article/details/52126760</a></p>
<h3 id="hive启动出错-HADOOP-HOME"><a href="#hive启动出错-HADOOP-HOME" class="headerlink" title="hive启动出错 $HADOOP_HOME"></a>hive启动出错 $HADOOP_HOME</h3><p><code>Cannot find hadoop installation: $HADOOP_HOME or $HADOOP_PREFIX must be set or hadoop must be in the</code><br>解决办法：<br>在hive-env.sh（conf目录下）中添加hadoop的目录：<br><code>export HADOOP_HOME=/home/work/hadoop/hadoop-2.6.0/</code><br>之后<code>source hive-env.sh</code> 就ok了</p>
<h3 id="配置spark后hive启动出错"><a href="#配置spark后hive启动出错" class="headerlink" title="配置spark后hive启动出错"></a>配置spark后hive启动出错</h3><p><code>ls: cannot access /usr/local/spark/lib/spark-assembly-*.jar: No such file or directory</code><br><a href="https://blog.csdn.net/Gpwner/article/details/73457108" target="_blank" rel="noopener">https://blog.csdn.net/Gpwner/article/details/73457108</a></p>
<h3 id="缺分号"><a href="#缺分号" class="headerlink" title="缺分号"></a>缺分号</h3><p><code>Hive Warning: Value had a \n character in it</code><br>遇到这个错误一般是hive query的格式问题， 哪里缺了个分号  <code>；</code><br>我这次是几个set语句后面忘记加分号了。</p>
<h3 id="join只允许等值操作"><a href="#join只允许等值操作" class="headerlink" title="join只允许等值操作"></a>join只允许等值操作</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c.*, t.* <span class="keyword">FROM</span> c <span class="keyword">JOIN</span> t </span><br><span class="line"><span class="keyword">ON</span> (t.area1= c.cname <span class="keyword">OR</span> t.area2 =c.cname <span class="keyword">OR</span> t.area3 = c.cname)</span><br><span class="line"><span class="keyword">WHERE</span> t.time&gt;=<span class="string">'20140818'</span> <span class="keyword">and</span> t.time&lt;=<span class="string">'20140824'</span> <span class="keyword">AND</span> platform=<span class="string">'pc'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> t.time;</span><br></pre></td></tr></table></figure>
<p>报错<code>FAILED: SemanticException [Error 10019]: Line 5:32 OR not supported in JOIN currently &#39;cname&#39;</code><br>hive 受限于 MapReduce 算法模型，只支持 equi-joins（等值 join），要实现上述的非等值 join，可以选择采用笛卡儿积（ full Cartesian product ）来实现。<br>笛卡尔积：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c.*, t.* <span class="keyword">FROM</span> c <span class="keyword">JOIN</span> t </span><br><span class="line"><span class="keyword">WHERE</span> t.area1= c.cname <span class="keyword">OR</span> t.area2 =c.cname <span class="keyword">OR</span> t.area3 = c.cname <span class="keyword">and</span> </span><br><span class="line">      t.time&gt;=<span class="string">'20140818'</span> <span class="keyword">and</span> t.time&lt;=<span class="string">'20140824'</span> <span class="keyword">AND</span> platform=<span class="string">'pc'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> t.time;</span><br></pre></td></tr></table></figure>
<p>笛卡尔积是m×n的映射，耗时且非内存。<br><code>or</code>时，采用<code>union all</code>代替：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line">    <span class="keyword">SELECT</span> c.*, t.* <span class="keyword">FROM</span> c <span class="keyword">JOIN</span> t <span class="keyword">on</span> t.area1= c.cname</span><br><span class="line">    <span class="keyword">WHERE</span> t.time&gt;=<span class="string">'20140818'</span> <span class="keyword">and</span> t.time&lt;=<span class="string">'20140824'</span> <span class="keyword">AND</span> platform=<span class="string">'pc'</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">ALL</span> </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">SELECT</span> c.*, t.* <span class="keyword">FROM</span> c <span class="keyword">JOIN</span> t <span class="keyword">on</span> t.area2= c.cname</span><br><span class="line">    <span class="keyword">WHERE</span> t.time&gt;=<span class="string">'20140818'</span> <span class="keyword">and</span> t.time&lt;=<span class="string">'20140824'</span> <span class="keyword">AND</span> platform=<span class="string">'pc'</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">union</span> <span class="keyword">ALL</span> </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">SELECT</span> c.*, t.* <span class="keyword">FROM</span> c <span class="keyword">JOIN</span> t <span class="keyword">on</span> t.area3= c.cname</span><br><span class="line">    <span class="keyword">WHERE</span> t.time&gt;=<span class="string">'20140818'</span> <span class="keyword">and</span> t.time&lt;=<span class="string">'20140824'</span> <span class="keyword">AND</span> platform=<span class="string">'pc'</span></span><br><span class="line">)tmp</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> tmp.time;</span><br></pre></td></tr></table></figure>
<p>此时使用<code>union all</code>，可以再上上面章节的并发优化<code>set hive.exec.parallel=true;</code> </p>
<p>总结自：<a href="https://cloud.tencent.com/developer/article/1043838" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1043838</a></p>
<h3 id="堆内存出错-Java-heap-space"><a href="#堆内存出错-Java-heap-space" class="headerlink" title="堆内存出错 Java heap space"></a>堆内存出错 Java heap space</h3><p><code>FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. Java heap space</code><br>设置 <code>set io.sort.mb=10;</code> 默认值是100<br>需要与<code>mapred.child.java.opts</code>相配 默认：-Xmx200m；不能超过<code>mapred.child.java.opt</code>设置，否则会OOM。<br>一般来说，都是reduce耗费内存比较大，这个选项是用来设置JVM堆的最大可用内存，但不要设置过大，如果超过2G(来自网络)，就应该考虑一下优化程序。<br>Input Split的大小，决定了一个Job拥有多少个map，默认128M每个Split（版本之间不同，低版本是64M），如果输入的数据量巨大，那么默认的128M的block会有特别多Map Task，集群的网络传输会很大，给Job Tracker的调度、队列、内存都会带来很大压力。</p>
<h3 id="Error-java-io-IOException-invalid-block-type"><a href="#Error-java-io-IOException-invalid-block-type" class="headerlink" title="Error: java.io.IOException: invalid block type"></a>Error: java.io.IOException: invalid block type</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">19/01/16 07:19:43 INFO mapreduce.Job: Task Id : attempt_1546194685436_1728349_m_000140_1, Status : FAILED</span><br><span class="line">Error: java.io.IOException: invalid block <span class="built_in">type</span></span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.inflateBytesDirect(Native Method)</span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.decompress(ZlibDecompressor.java:227)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)</span><br><span class="line">	at java.io.InputStream.read(InputStream.java:101)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.moveToNext(MapTask.java:197)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.next(MapTask.java:183)</span><br><span class="line">	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)</span><br><span class="line">	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild<span class="variable">$2</span>.run(YarnChild.java:162)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)</span><br><span class="line"></span><br><span class="line">Container killed by the ApplicationMaster.</span><br><span class="line">Container killed on request. Exit code is 143</span><br><span class="line"></span><br><span class="line">19/01/16 07:19:43 INFO mapreduce.Job: Task Id : attempt_1546194685436_1728349_m_000143_1, Status : FAILED</span><br><span class="line">Error: java.io.IOException: incorrect data check</span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.inflateBytesDirect(Native Method)</span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.decompress(ZlibDecompressor.java:227)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)</span><br><span class="line">	at java.io.InputStream.read(InputStream.java:101)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.moveToNext(MapTask.java:197)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.next(MapTask.java:183)</span><br><span class="line">	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)</span><br><span class="line">	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild<span class="variable">$2</span>.run(YarnChild.java:162)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)</span><br><span class="line"></span><br><span class="line">Container killed by the ApplicationMaster.</span><br><span class="line">Container killed on request. Exit code is 143</span><br><span class="line"></span><br><span class="line">19/01/16 07:19:43 INFO mapreduce.Job: Task Id : attempt_1546194685436_1728349_m_000131_1, Status : FAILED</span><br><span class="line">Error: java.io.IOException: invalid block <span class="built_in">type</span></span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.inflateBytesDirect(Native Method)</span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.decompress(ZlibDecompressor.java:227)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)</span><br><span class="line">	at java.io.InputStream.read(InputStream.java:101)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.moveToNext(MapTask.java:197)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.next(MapTask.java:183)</span><br><span class="line">	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)</span><br><span class="line">	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild<span class="variable">$2</span>.run(YarnChild.java:162)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)</span><br><span class="line"></span><br><span class="line">Container killed by the ApplicationMaster.</span><br><span class="line">Container killed on request. Exit code is 143</span><br><span class="line"></span><br><span class="line">19/01/16 07:19:43 INFO mapreduce.Job: Task Id : attempt_1546194685436_1728349_m_000145_1, Status : FAILED</span><br><span class="line">Error: java.io.IOException: incorrect data check</span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.inflateBytesDirect(Native Method)</span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.decompress(ZlibDecompressor.java:227)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)</span><br><span class="line">	at java.io.InputStream.read(InputStream.java:101)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.moveToNext(MapTask.java:197)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.next(MapTask.java:183)</span><br><span class="line">	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)</span><br><span class="line">	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild<span class="variable">$2</span>.run(YarnChild.java:162)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)</span><br><span class="line"></span><br><span class="line">Container killed by the ApplicationMaster.</span><br><span class="line">Container killed on request. Exit code is 143</span><br><span class="line"></span><br><span class="line">19/01/16 07:19:43 INFO mapreduce.Job: Task Id : attempt_1546194685436_1728349_m_000128_1, Status : FAILED</span><br><span class="line">Error: java.io.IOException: invalid block <span class="built_in">type</span></span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.inflateBytesDirect(Native Method)</span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.decompress(ZlibDecompressor.java:227)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)</span><br><span class="line">	at java.io.InputStream.read(InputStream.java:101)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.moveToNext(MapTask.java:197)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.next(MapTask.java:183)</span><br><span class="line">	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)</span><br><span class="line">	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild<span class="variable">$2</span>.run(YarnChild.java:162)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)</span><br><span class="line"></span><br><span class="line">Container killed by the ApplicationMaster.</span><br><span class="line">Container killed on request. Exit code is 143</span><br><span class="line"></span><br><span class="line">19/01/16 07:19:43 INFO mapreduce.Job: Task Id : attempt_1546194685436_1728349_m_000124_1, Status : FAILED</span><br><span class="line">Error: java.io.IOException: invalid stored block lengths</span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.inflateBytesDirect(Native Method)</span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.decompress(ZlibDecompressor.java:227)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)</span><br><span class="line">	at java.io.InputStream.read(InputStream.java:101)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.moveToNext(MapTask.java:197)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.next(MapTask.java:183)</span><br><span class="line">	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)</span><br><span class="line">	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild<span class="variable">$2</span>.run(YarnChild.java:162)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)</span><br><span class="line"></span><br><span class="line">Container killed by the ApplicationMaster.</span><br><span class="line">Container killed on request. Exit code is 143</span><br><span class="line"></span><br><span class="line">19/01/16 07:19:43 INFO mapreduce.Job: Task Id : attempt_1546194685436_1728349_m_000142_1, Status : FAILED</span><br><span class="line">Error: java.io.IOException: invalid block <span class="built_in">type</span></span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.inflateBytesDirect(Native Method)</span><br><span class="line">	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.decompress(ZlibDecompressor.java:227)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)</span><br><span class="line">	at java.io.InputStream.read(InputStream.java:101)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.moveToNext(MapTask.java:197)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.next(MapTask.java:183)</span><br><span class="line">	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)</span><br><span class="line">	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild<span class="variable">$2</span>.run(YarnChild.java:162)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)</span><br><span class="line"></span><br><span class="line">Container killed by the ApplicationMaster.</span><br><span class="line">Container killed on request. Exit code is 143</span><br></pre></td></tr></table></figure>
<p>解决：加参数，<code>-D mapred.skip.map.max.skip.records=1</code> map task最多允许的跳过记录数，默认值0。<br>参考：<a href="https://forums.aws.amazon.com/thread.jspa?threadID=92747" target="_blank" rel="noopener">https://forums.aws.amazon.com/thread.jspa?threadID=92747</a></p>
<h3 id="Unexpected-end-of-input-stream"><a href="#Unexpected-end-of-input-stream" class="headerlink" title="Unexpected end of input stream"></a>Unexpected end of input stream</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error: java.io.EOFException: Unexpected end of input stream</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:145)</span><br><span class="line">	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)</span><br><span class="line">	at java.io.InputStream.read(InputStream.java:101)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)</span><br><span class="line">	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.moveToNext(MapTask.java:197)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask<span class="variable">$TrackedRecordReader</span>.next(MapTask.java:183)</span><br><span class="line">	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)</span><br><span class="line">	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild<span class="variable">$2</span>.run(YarnChild.java:162)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)</span><br></pre></td></tr></table></figure>
<p>这个主要是hdfs文件的问题。文件的压缩有问题，现在解压失败报错。<br>从日志中可以找到出错的文件名，然后get下来后，尝试手动解压失败，确定问题。</p>
<h3 id="Unsupported-major-minor-version-52-0"><a href="#Unsupported-major-minor-version-52-0" class="headerlink" title="Unsupported major.minor version 52.0"></a>Unsupported major.minor version 52.0</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">java.lang.UnsupportedClassVersionError: com/mr/udf/UDTFTest : Unsupported major.minor version 52.0</span><br><span class="line">	at java.lang.ClassLoader.defineClass1(Native Method)</span><br><span class="line">	at java.lang.ClassLoader.defineClass(ClassLoader.java:800)</span><br><span class="line">	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)</span><br><span class="line">	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)</span><br><span class="line">	at java.net.URLClassLoader.access<span class="variable">$100</span>(URLClassLoader.java:71)</span><br><span class="line">	at java.net.URLClassLoader<span class="variable">$1</span>.run(URLClassLoader.java:361)</span><br><span class="line">	at java.net.URLClassLoader<span class="variable">$1</span>.run(URLClassLoader.java:355)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)</span><br><span class="line">	at java.lang.Class.forName0(Native Method)</span><br><span class="line">	at java.lang.Class.forName(Class.java:270)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.FunctionTask.getUdfClass(FunctionTask.java:309)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.FunctionTask.createTemporaryFunction(FunctionTask.java:165)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.FunctionTask.execute(FunctionTask.java:72)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1676)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1435)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1218)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1082)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1072)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:606)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)</span><br><span class="line">FAILED: Execution Error, <span class="built_in">return</span> code -101 from org.apache.hadoop.hive.ql.exec.FunctionTask. com/mr/udf/UDTFTest : Unsupported major.minor version 52.0</span><br></pre></td></tr></table></figure>
<p>原因：hive自定义jar包和hive里的jdk版本不兼容。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://www.voidcn.com/article/p-zfiukxcn-hx.html" target="_blank" rel="noopener">http://www.voidcn.com/article/p-zfiukxcn-hx.html</a><br><a href="https://www.cnblogs.com/cfox/p/3849407.html" target="_blank" rel="noopener">https://www.cnblogs.com/cfox/p/3849407.html</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>hive压缩</title>
    <url>/2018/10/19/hive%E5%8E%8B%E7%BC%A9/</url>
    <content><![CDATA[<p><img src="yasuo.png" alt=""><br><img src="yasuo1.png" alt=""><br><code>set hive.exec.compress.intermediate=true;</code> 中间数据map压缩，不影响最终结果。但是job中间数据输出要写在硬盘并通过网络传输到reduce，传送数据量变小，因为shuffle sort（混洗排序）数据被压缩了。<br><code>set mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;</code> 为中间数据配置压锁编解码器 ，通常配置Snappy更好。<br>压缩Map的输出，这样做有两个好处：<br>a)压缩是在内存中进行，所以写入map本地磁盘的数据就会变小，大大减少了本地IO次数<br>b) Reduce从每个map节点copy数据，也会明显降低网络传输的时间<br>注：数据序列化其实效果会更好，无论是磁盘IO还是数据大小，都会明显的降低。</p>
<p><code>set hive.exec.compress.output=true;</code>  打开job最终输出压缩的开关，设置之后必须设置下面这行，否则还是没有压缩效果<br><code>set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;</code>  设置压缩类型<br><code>set mapred.output.compression.type=BLOCK;</code> 大文件压缩仍然会耗时，而且影响mapper并行（mapper并行和文件的个数有关），这个设置，使大的文件可以分割成小文件进行压缩</p>
<p>gzip是一种数据格式，默认且目前仅使用deflate算法压缩data部分；deflate是一种压缩算法。<br><strong><em>gzip不支持切片，切片参数都不管用。如果文件太大，会导致map非常慢。如果要压缩成gzip格式，做好控制在170M，mr的效果是最好的。</em></strong><br>这种处理文件压缩的能力并非是hive特有的，实际上，使用了hadoop的TextInputFormat进行处理，它可以识别后缀名是.deflate或.gz的压缩文件，并可以轻松处理。<br>hive无需关心底层文件是否是压缩的，以及如何压缩的。</p>
<p><a href="https://m.2cto.com/kf/201611/566909.html" target="_blank" rel="noopener">https://m.2cto.com/kf/201611/566909.html</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习</title>
    <url>/2018/10/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>1、线性回归和逻辑回归在计算之前先对数据处理，把无关、想似的属性去除，效果会好。</p>
<p>2、朴素贝叶斯有个强假设，假设每个属性都是独立的，不相关的。</p>
<p>3、KNN算法当输入数据维度很高时（也可以理解成属性很多），效果会变差。主要是因为输入变量的数量对于算法性能有着很大的负面影响。</p>
<p>4、支持向量机可能是最受欢迎、讨论最为广泛的机器学习算法之一。</p>
<p>5、如果用方差较高的算法（如决策树）能够获得较好的结果，那么通过bagging算法通常可以获得更好的结果。</p>
<p>6、通常，AdaBoost算法与决策树一起工作。第一个决策树创建后，决策树在每个训练实例上的性能，都被用来衡量下一个决策树针对该实例所应分配的关注程度。难以预测的训练数据被赋予更大的权重，而容易预测的数据则被赋予更小的权重。模型依次被创建，每次更新训练实例的权重，都会影响到序列中下一个决策树学习性能。所有决策树完成后，即可对新输入的数据进行预测，而每个决策树的性能将由它在训练数据上的准确度所决定。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>yarn</title>
    <url>/2018/09/19/Yarn/</url>
    <content><![CDATA[<p>应用提交过程分析：<br><img src="1.png" alt=""></p>
<ul>
<li>客户端程序向ResourceManager提交应用并请求一个ApplicationMaster实例</li>
<li>ResourceManager找到可以运行一个Container的NodeManager，并在这个Container中启动ApplicationMaster实例</li>
<li>ApplicationMaster向ResourceManager进行注册，注册之后客户端就可以查询ResourceManager获得自己ApplicationMaster的详细信息，以后就可以和自己的ApplicationMaster直接交互了</li>
<li>在平常的操作过程中，ApplicationMaster根据resource-request协议向ResourceManager发送resource-request请求</li>
<li>当Container被成功分配之后，ApplicationMaster通过向NodeManager发送container-launch-specification信息来启动Container， container-launch-specification信息包含了能够让Container和ApplicationMaster交流所需要的资料</li>
<li>应用程序的代码在启动的Container中运行，并把运行的进度、状态等信息通过application-specific协议发送给ApplicationMaster</li>
<li>在应用程序运行期间，提交应用的客户端主动和ApplicationMaster交流获得应用的运行状态、进度更新等信息，交流的协议也是application-specific协议</li>
<li>但应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster向ResourceManager取消注册然后关闭，用到所有的Container也归还给系统</li>
</ul>
<p>ApplicationMaster + ResourceManager = JobTracker<br>ResourceManager：Scheduler和ApplicationManager。<br>   Scheduler的角色是一个纯调度器，主要负责协调集群中各个应用的资源分配，保障整个集群的运行效率，只负责调度Containers，不会关心应用程序监控及其运行状态等信息。同样，它也不能重启因应用失败或者硬件错误而运行失败的任务。<br>   ApplicationManager主要负责接收job的提交请求，为应用分配第一个Container来运行ApplicationMaster，还有就是负责监控ApplicationMaster，在遇到失败时重启ApplicationMaster运行的Container。</p>
<p>ApplicationMaster：是向ResourceManager申请资源并和NodeManager协同工作来运行应用的各个任务然后跟踪它们状态及监控各个任务的执行，遇到失败的任务还负责重启它。</p>
<p>NodeManager：接收ResourceManager的资源分配请求，分配具体的Container给应用。Nodemanager定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态（cpu和内存等资源）</p>
<p>Container：YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。每个任务分配一个Container，如有100个mapper，就会有对应的100个container。</p>
<p>来源：<a href="https://blog.csdn.net/suifeng3051/article/details/49486927" target="_blank" rel="noopener">https://blog.csdn.net/suifeng3051/article/details/49486927</a><br>内存参数：<a href="https://www.cnblogs.com/wcwen1990/p/6737985.html" target="_blank" rel="noopener">https://www.cnblogs.com/wcwen1990/p/6737985.html</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop知识记录</title>
    <url>/2018/09/19/hadoop%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<p>当一些查询翻译到MapReduce任务时，往往会产生多个Stage，而这些串联的Stage又依赖于底层文件系统（如HDFS）来存储每一个Stage的输出结果。<br>Hadoop 包含以下两个主要组件：Hadoop Distributed File System (HDFS) 和一个分布式计算引擎，该引擎支持以 MapReduce 作业的形式实现和运行程序。<br><strong><em>新版本：</em></strong><br><img src="4.png" alt=""></p>
<p><strong><em>新旧版本：</em></strong><br>主要是把Jobtracker拆成主从的RM + AM</p>
<p>Jobtracker是hadoop1.x中的组件，它的功能相当于：<br>　　Resourcemanager + MRAppMaster</p>
<p>TaskTracker 相当于：<br>　　Nodemanager + yarnchild</p>
<p><strong><em>旧版本：</em></strong><br>hdfs由nameNode管理。<br><img src="1.png" alt=""><br>MapReduce由jobtracker管理。<br><img src="2.png" alt=""><br>集群分布：<br>DataNode和TaskTracker部署在同一台节点上。<br>但是NameNode和JobTracker最好分开部署，为了性能和稳定。<br><img src="3.png" alt=""></p>
<h2 id="设置reduce为0"><a href="#设置reduce为0" class="headerlink" title="设置reduce为0"></a>设置reduce为0</h2><p>mr不写reduce时，系统为了架构的完成性，仍然自动生成了一个reduce，这个reduce并不做shuffle和数据拖取，生成的结果文件就是map的输出文件。<br>但是可以直接通过<code>job.setNumReduceTasks(0)</code>实现将reduce设置成0个。</p>
<h2 id="HDFS常用操作"><a href="#HDFS常用操作" class="headerlink" title="HDFS常用操作"></a>HDFS常用操作</h2><p>hadoop dfs -ls 列出HDFS下的文件<br>hadoop dfs -ls in 列出HDFS下某个文档中的文件<br>hadoop dfs -put test1.txt test 上传文件到指定目录并且重新命名，只有所有的DataNode都接收完数据才算成功<br>hadoop dfs -get in getin 从HDFS获取文件并且重新命名为getin，同put一样可操作文件也可操作目录<br>hadoop dfs -rmr out 删除指定文件从HDFS上<br>hadoop dfs -cat in/* 查看HDFS上in目录的内容<br>hadoop dfsadmin -report 查看HDFS的基本统计信息<br>hadoop dfsadmin -safemode leave 退出安全模式<br>hadoop dfsadmin -safemode enter 进入安全模式  </p>
<h2 id="test"><a href="#test" class="headerlink" title="test"></a>test</h2><p>使用方法：hadoop fs -test -[ezd] URI<br>选项：<br>-e 检查目录/文件是否存在。如果存在则返回0。<br>-z 检查文件是否是0字节。如果是则返回0。<br>-d 如果路径是个目录，则返回1，否则返回0。<br>示例1：<code>hadoop fs -test -e filename</code><br>示例2：shell判断hdfs目录是否存在:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">test</span> -e /hdfs_dir</span><br><span class="line"><span class="keyword">if</span> [ $? -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"Directory not exists!"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<h2 id="hadoop-fs-ls-d"><a href="#hadoop-fs-ls-d" class="headerlink" title="hadoop fs -ls -d"></a>hadoop fs -ls -d</h2><p>使用帮助命令<code>hadoop fs -help ls</code> 看到-d对应的是<code>Directories are listed as plain files.</code></p>
<p>遇到的情况是<code>hadoop fs -ls -d /user/hive/warehouse/test.db/t/*/*</code> 假设表test.t是分区表，而且有两个分区，这条命令就可以输出该表所有的分区值。  </p>
<h2 id="hadoop-fs-count-lt-hdfs-path-gt"><a href="#hadoop-fs-count-lt-hdfs-path-gt" class="headerlink" title="hadoop fs -count &lt; hdfs path &gt;"></a>hadoop fs -count &lt; hdfs path &gt;</h2><p>输出 目录个数，文件个数，文件总计大小，输入路径<br>例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop fs -count /data/dltb3yi/</span><br><span class="line"> 1        24000       253953854502 /data/dltb3yi/      <span class="comment"># 获得24000个文件</span></span><br></pre></td></tr></table></figure>
<h2 id="hadoop-fs-rm-r"><a href="#hadoop-fs-rm-r" class="headerlink" title="hadoop fs -rm -r"></a>hadoop fs -rm -r</h2><p>删除目录</p>
<h2 id="job-waitForCompletion"><a href="#job-waitForCompletion" class="headerlink" title="job.waitForCompletion"></a>job.waitForCompletion</h2><p>Job运行是通过job.waitForCompletion(true)，true表示将运行进度等信息及时输出给用户，false的话只是等待作业结束</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>hadoop常用命令：<a href="https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#test" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html#test</a><br><a href="https://www.cnblogs.com/zhaosk/p/4391294.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhaosk/p/4391294.html</a><br><a href="https://blog.csdn.net/litianxiang_kaola/article/details/71154302" target="_blank" rel="noopener">https://blog.csdn.net/litianxiang_kaola/article/details/71154302</a>  （wordcount例子）</p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>hive map、reduce计算</title>
    <url>/2018/09/19/hive%20map%E3%80%81reduce%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<h3 id="map个数计算"><a href="#map个数计算" class="headerlink" title="map个数计算"></a>map个数计算</h3><p><code>dfs.block.size</code><br>splitsize= Math.max(minSize, Math.min(goalSize, blockSize)),通常这个值=blockSize，输入的文件较小，文件字节数小于blocksize时，splitsize=输入文件字节数之和。</p>
<p><strong><em>gzip不支持切片，因此一个gzip压缩文件不能通过切片 由多个map执行，只能是有多少个文件，对应有多少个map。</em></strong><br><code>minSize=max{minSplitSize, mapred.min.split.size}</code>（minSplitSize大小默认为1B）<br><code>maxSize=mapred.max.split.size</code>（不在配置文件中指定时大小为<code>Long.MAX_VALUE=3G</code>）<br><code>splitSize=max{minSize, min{maxSize, blockSize}}</code><br>一般来说，一个map不能处理多个文件。<br>在一个map不涉及到多文件处理时，用上面的参数。想增加map个数时，把<code>maxSize</code>调小，小于<code>blockSize</code>（不同版本默认大小不同，2.X一般是128M）时可以取代<code>blocksize</code>变成新的切片大小<code>splitsize</code>。</p>
<p><strong><em>map计算方式:</em></strong><br>文件大小/splitSize&gt;1.1，创建一个split0，文件剩余大小=文件大小-splitSize<br>　…..<br>剩余文件大小/splitSize&lt;=1.1 将剩余的部分作为一个split<br>每一个分片对应一个map任务，这样map任务的数目也就显而易见啦。 </p>
<p>但其实<strong><em>一个map可以跨文件处理</em></strong>：<br>通过实验，gzip、orc、lzo都支持文件合并。<br>一般在想减少map个数，但是文件大小都小于<code>blockSize</code>时，上面已经使不上劲时使用下面参数。<br><code>set hive.hadoop.supports.splittable.combineinputformat=true;</code> 开关<br><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</code> 执行Map前进行小文件合并<br><code>set mapred.max.split.size=2048000000;</code> 2G 每个Map最大输入大小<br><code>set mapred.min.split.size.per.node=2048000000;</code> 一个节点上split的至少的大小 ，决定了多个data node上的文件是否需要合并<br><code>set mapred.min.split.size.per.rack=2048000000;</code> 一个交换机下split的至少的大小，决定了多个交换机上的文件是否需要合并<br><strong><em>此时，map计算方式:</em></strong><br>a、根据输入目录下的每个文件,如果其长度超过mapred.max.split.size,以block为单位分成多个split(一个split是一个map的输入),每个split的长度都大于mapred.max.split.size, 因为以block为单位, 因此也会大于blockSize, 此文件剩下的长度如果大于mapred.min.split.size.per.node, 则生成一个split, 否则先暂时保留.<br>b、现在剩下的都是一些长度效短的碎片,把每个rack下碎片合并, 只要长度超过mapred.max.split.size就合并成一个split, 最后如果剩下的碎片比mapred.min.split.size.per.rack大, 就合并成一个split, 否则暂时保留.<br>c、把不同rack下的碎片合并, 只要长度超过mapred.max.split.size就合并成一个split, 剩下的碎片无论长度, 合并成一个split.<br>举例: mapred.max.split.size=1000<br>mapred.min.split.size.per.node=300<br>mapred.min.split.size.per.rack=100<br>输入目录下五个文件,rack1下三个文件,长度为2050,1499,10, rack2下两个文件,长度为1010,80. 另外blockSize为500.<br>经过第一步, 生成五个split: 1000,1000,1000,499,1000. 剩下的碎片为rack1下:50,10; rack2下10:80<br>由于两个rack下的碎片和都不超过100, 所以经过第二步, split和碎片都没有变化.<br>第三步,合并四个碎片成一个split, 长度为150.<br>如果要减少map数量, 可以调大mapred.max.split.size, 否则调小即可.<br>其特点是: 一个块至多作为一个map的输入，一个文件可能有多个块，一个文件可能因为块多分给做为不同map的输入， 一个map可能处理多个块，可能处理多个文件。<br>详细看：<a href="/2018/10/19/hive小文件合并/">小文件合并</a></p>
<h3 id="reduce数量"><a href="#reduce数量" class="headerlink" title="reduce数量"></a>reduce数量</h3><p>一个reduce生成一个文件。<br>可以在hive运行sql的时，打印出来，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer&#x3D;&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max&#x3D;&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapred.reduce.tasks&#x3D;&lt;number&gt;</span><br></pre></td></tr></table></figure>
<p>reduce数量由以下三个参数决定，<br><code>mapred.reduce.tasks</code> (强制指定reduce的任务数量)<br><code>hive.exec.reducers.bytes.per.reducer</code> （每个reduce任务处理的数据量，默认为1000^3=1G,一般可能会不管用。）<br><code>hive.exec.reducers.max</code> （每个任务最大的reduce数，默认1009）</p>
<p>没有强制指定reduce个数,计算reducer数的公式很简单N=min(<code>hive.exec.reducers.max</code>, 总输入数据量/<code>hive.exec.reducers.bytes.per.reducer</code>)<br>在hql最后加上<code>distribute by rand()</code>，可强制使hql有reduce过程。</p>
<p> 只有一个reduce的场景：<br>  a、没有group by 的汇总，比如把select pt,count(1) from popt_tbaccountcopy_mes where pt = ‘2012-07-04’ group by pt; 写成 select count(1) from popt_tbaccountcopy_mes where pt = ‘2012-07-04’;<br>  b、order by<br>  c、笛卡尔积</p>
<p><a href="https://www.iteblog.com/archives/1697.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/1697.html</a></p>
<h3 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h3><p>Combiner会优化MapReduce的中间结果，所以它在整个模型中会多次使用。那么哪些场景才能使用Combiner呢？从这里分析，Combiner的输出是Reducer的输入，Combiner绝不能改变最终的计算结果。所以从我的想法来看，Combiner只应该用于那种Reduce的输入key/value与输出key/value类型完全一致，且不影响最终结果的场景。比如累加，最大值等。Combiner的使用一定得慎重，如果用好，它对job执行效率有帮助，反之会影响reduce的最终结果。</p>
<p>讲的很棒：(shuffle过程)<a href="https://www.iteblog.com/archives/1119.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/1119.html</a><br>（过程）<a href="https://www.cnblogs.com/ljy2013/articles/4435657.html" target="_blank" rel="noopener">https://www.cnblogs.com/ljy2013/articles/4435657.html</a><br>（shuffle优化）<a href="https://blog.csdn.net/z_xiaozhut/article/details/82353905" target="_blank" rel="noopener">https://blog.csdn.net/z_xiaozhut/article/details/82353905</a></p>
<h1 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h1><p>定义：某一个或几个key的数据相比于其他key特别多，导致他们对应的reduce非常慢，其他数据量少的reduce早就执行完了，但是还要等待。<br>最容易的原因：（1）大量的key为空join连接的情况，空的key都hash到一个reduce上去了.</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.cnblogs.com/xd502djj/p/3799432.html" target="_blank" rel="noopener">https://www.cnblogs.com/xd502djj/p/3799432.html</a><br><a href="https://www.cnblogs.com/yueliming/p/3251285.html" target="_blank" rel="noopener">https://www.cnblogs.com/yueliming/p/3251285.html</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>caffe 数据库LMDB的读写</title>
    <url>/2018/09/08/caffe%20%E6%95%B0%E6%8D%AE%E5%BA%93LMDB%E7%9A%84%E8%AF%BB%E5%86%99/</url>
    <content><![CDATA[<p>读写的图片都是灰度图，rgb图类似  </p>
<h3 id="读数据库（图片的channel是2，其实是两张图片）"><a href="#读数据库（图片的channel是2，其实是两张图片）" class="headerlink" title="读数据库（图片的channel是2，其实是两张图片）"></a>读数据库（图片的channel是2，其实是两张图片）</h3><p>Datum是caffe里定义的一种存数据的结构。所以使用它时必须在开头import caffe。</p>
<p>它的属性有：</p>
<ul>
<li>channels：图片的通道。如彩色图用3，灰度图用1.但是也许你想把它定义中其他数字，让它每个通道都是一个单张的图，这个例子就是2，每个通道是一张灰度图。</li>
<li>height：图片（即data）的高</li>
<li>width：图片（即data）的宽</li>
<li>data：图片的数据（像素值）</li>
<li>label：图片的label。如caffe的mnist里label是0~9的数字。</li>
</ul>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>,<span class="string">"../../python"</span>)<span class="comment">#为了import caffe</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> lmdb</span><br><span class="line"><span class="keyword">import</span> caffe</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    parse = argparse.ArgumentParser()</span><br><span class="line">    parse.add_argument(<span class="string">'--lmdbpath'</span>)</span><br><span class="line">    args = parse.parse_args()</span><br><span class="line"> </span><br><span class="line">    env = lmdb.open(args.lmdbpath, readonly=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> env.begin() <span class="keyword">as</span> txn:</span><br><span class="line">        cursor = txn.cursor()</span><br><span class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> cursor:</span><br><span class="line">            <span class="comment">#print(key,len(value))#value是string类型</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">'key: '</span>,key</span><br><span class="line">               datum = caffe.proto.caffe_pb2.Datum()<span class="comment">#datum类型</span></span><br><span class="line">               datum.ParseFromString(value)<span class="comment">#转成datum</span></span><br><span class="line">               flat_x = np.fromstring(datum.data, dtype=np.uint8)<span class="comment">#转成numpy类型</span></span><br><span class="line">               x = flat_x.reshape(datum.channels, datum.height, datum.width)<span class="comment">#reshape大小</span></span><br><span class="line">               y = datum.label<span class="comment">#图片的label</span></span><br><span class="line">            fig = pyplot.figure()<span class="comment">#把两张图片显示出来</span></span><br><span class="line">            ax = fig.add_subplot(<span class="number">121</span>)</span><br><span class="line">            ax.imshow(x[<span class="number">0</span>], cmap=<span class="string">'gray'</span>)</span><br><span class="line">            ax = fig.add_subplot(<span class="number">122</span>)</span><br><span class="line">            ax.imshow(x[<span class="number">1</span>], cmap=<span class="string">'gray'</span>)</span><br><span class="line">            pyplot.show()</span><br></pre></td></tr></table></figure>
<h3 id="写数据库（例子中把两张图片作为一张图的两个channel）"><a href="#写数据库（例子中把两张图片作为一张图的两个channel）" class="headerlink" title="写数据库（例子中把两张图片作为一张图的两个channel）"></a>写数据库（例子中把两张图片作为一张图的两个channel）</h3><p>caffe，以及faster rcnn写lmdb时都习惯把图片的名字写到txt文件中，通过txt去加载图片。思路大概如此。</p>
<p>我这个例子txt存的是：图片1名字   图片2名字    label</p>
<p>例如mnist的例子可以是：图片名字   label（代表这张图是0~9的哪个数字）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> lmdb</span><br><span class="line"><span class="keyword">import</span> Image <span class="keyword">as</span> img</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> sys,os</span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="string">'../../python'</span>)</span><br><span class="line"><span class="keyword">import</span> caffe</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_txt</span><span class="params">(txt, shuffle)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> txt == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"txtpath!!!"</span></span><br><span class="line">        exit(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(txt):</span><br><span class="line">         <span class="keyword">print</span> <span class="string">"the txt is't exists"</span></span><br><span class="line">         exit(<span class="number">0</span>)</span><br><span class="line">    flag = <span class="number">0</span></span><br><span class="line">    file_content = []</span><br><span class="line">    txt_file = open(txt, <span class="string">'r'</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> open(txt, <span class="string">'r'</span>):</span><br><span class="line">        line = txt_file.readline()</span><br><span class="line">        list = line.split()</span><br><span class="line">        file_content.append([list[<span class="number">0</span>], list[<span class="number">1</span>], list[<span class="number">2</span>]])</span><br><span class="line">        flag += <span class="number">1</span> </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> shuffle == <span class="literal">None</span>: <span class="comment">#为了打乱数据顺序</span></span><br><span class="line">        random.shuffle(file_content)</span><br><span class="line">    <span class="keyword">return</span> file_content</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_argu</span><span class="params">(parse)</span>:</span></span><br><span class="line">    parse.add_argument(<span class="string">'--txt'</span>)</span><br><span class="line">    parse.add_argument(<span class="string">'--lmdb'</span>)</span><br><span class="line">    parse.add_argument(<span class="string">'--shuffle'</span>)<span class="comment">#为了打乱数据顺序</span></span><br><span class="line">    parse.add_argument(<span class="string">'--picpath'</span>)</span><br><span class="line">    <span class="keyword">return</span> parse.parse_args() </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    parse = argparse.ArgumentParser()</span><br><span class="line">    args = add_argu(parse)</span><br><span class="line"></span><br><span class="line">    content = []  </span><br><span class="line">    content = load_txt(args.txt, args.shuffle)<span class="comment">#加载图片名字和label</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'total: '</span>, len(content)</span><br><span class="line">    env = lmdb.Environment(args.lmdb, map_size=int(<span class="number">1e12</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> env.begin(write=<span class="literal">True</span>) <span class="keyword">as</span> txn:</span><br><span class="line">        <span class="comment"># txn is a Transaction object</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(content)):</span><br><span class="line">            datum = caffe.proto.caffe_pb2.Datum()</span><br><span class="line">            pic_path1 = args.picpath + <span class="string">'/'</span> +  content[i][<span class="number">0</span>]</span><br><span class="line">            pic_path2 = args.picpath + <span class="string">'/'</span> + content[i][<span class="number">1</span>]</span><br><span class="line">            label = int(content[i][<span class="number">2</span>])</span><br><span class="line">            img_file1 = io.imread(pic_path1)</span><br><span class="line">            img_file2 = io.imread(pic_path2)</span><br><span class="line">            datum.channels = <span class="number">2</span><span class="comment">#channels</span></span><br><span class="line">            datum.height = img_file1.shape[<span class="number">0</span>]<span class="comment">#height</span></span><br><span class="line">            datum.width = img_file1.shape[<span class="number">1</span>]<span class="comment">#width</span></span><br><span class="line">            data = np.zeros((<span class="number">2</span>,  img_file1.shape[<span class="number">0</span>],  img_file2.shape[<span class="number">1</span>]), dtype=np.uint8)<span class="comment">#初始化data</span></span><br><span class="line">            data[<span class="number">0</span>] = img_file1</span><br><span class="line">            data[<span class="number">1</span>] = img_file2</span><br><span class="line">            datum.data = data.tostring() <span class="comment">#data</span></span><br><span class="line">            datum.label = int(label)<span class="comment">#label</span></span><br><span class="line">            str_id = <span class="string">"%08d"</span> %(i) + <span class="string">"_"</span> + content[i][<span class="number">0</span>] <span class="comment">#'&#123;:08&#125;'.format(i) #顺序+图片名字作为key</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># The encode is only essential in Python 3</span></span><br><span class="line">            txn.put(str_id.encode(<span class="string">'ascii'</span>), datum.SerializeToString())</span><br></pre></td></tr></table></figure>
<p>注意：数据库的读是按照key的字典序读的，而不是按照写的顺序，所以写数据库时key必须重新写。<br>如果把图片名字作为key，读出来的图片仍是按照图片名字的字典序（不是写的顺序），因此之前的对图片名字打乱后再存入txt的操作就失去了意义。str_id是key  </p>
]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>lmdb</tag>
      </tags>
  </entry>
  <entry>
    <title>caffe用python时可能需要的模块安装</title>
    <url>/2018/09/08/caffe%E7%94%A8python%E6%97%B6%E5%8F%AF%E8%83%BD%E9%9C%80%E8%A6%81%E7%9A%84%E6%A8%A1%E5%9D%97%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p><strong>!!!经验 [ 换源 ](/2018/04/08/ubuntu 安装numpy的烂问题libgfortran3依赖) 之后再装。  </strong></p>
<h1 id="Cython-Distutils"><a href="#Cython-Distutils" class="headerlink" title="Cython.Distutils"></a>Cython.Distutils</h1><p>报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ImportError: No module named Cython.Distutils</span><br></pre></td></tr></table></figure>
<p>解决：进入python后，输入命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from Cython.Distutils import build_ext</span><br></pre></td></tr></table></figure>
<p>仍然输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):  </span><br><span class="line">File &quot;&quot;, line 1, in   </span><br><span class="line">ImportError: No module named Cython.Distutils</span><br></pre></td></tr></table></figure>
<p>则表示你的确没有Cython.Distutils。此时输入命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install cython</span><br></pre></td></tr></table></figure>
<p>安装它时可能会提示你还没有安装pip，那就安它提示的命令安装pip后再安装即可。<br>！！！！！！<br>但实际上这个方法还是没有解决问题。换另一种方法：<br>Just install Cython from <a href="http://cython.org/#download" target="_blank" rel="noopener"> http://cython.org/#download
</a> and install it using this command : </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo python setup.py install</span><br><span class="line">sudo python -c &#39;import Cython.Distutils&#39;</span><br></pre></td></tr></table></figure>
<p> and it will be installed and the error message will disappear.  </p>
<p>(晚上有遇到一个类似的问题，用pip安装仍然是fail，仔细看是权限问题，不知道上面那个问题是不是也是权限问题。)</p>
<p>今天再次遇到，简单解决：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install Cython</span><br></pre></td></tr></table></figure>

<p>(但是简单解决的前提不知道是不是因为我提前换了源，换源参考： [换源](/2018/04/08/ubuntu 安装numpy的烂问题libgfortran3依赖) )  </p>
<h1 id="easydict"><a href="#easydict" class="headerlink" title="easydict"></a>easydict</h1><p>报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ImportError: No module named easydict  （类似与上面的错误）</span><br></pre></td></tr></table></figure>
<p>解决：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo pip install easydict</span><br></pre></td></tr></table></figure>

<h1 id="sklearn-utils"><a href="#sklearn-utils" class="headerlink" title="sklearn.utils"></a>sklearn.utils</h1><p>解决： if you have Python 2 you can install all these requirements by issuing:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install build-essential python-dev python-setuptools python-numpy python-scipy libatlas-dev libatlas3gf-base pip install --user --install-option&#x3D;&quot;--prefix&#x3D;&quot; -U scikit-learn</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-sklearn</span><br></pre></td></tr></table></figure>
<p>使用下面命令测试sklearn是否安装正确：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nosetests -v sklearn</span><br></pre></td></tr></table></figure>
<h1 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h1><p>解决：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-pandas</span><br></pre></td></tr></table></figure>

<h1 id="cv2"><a href="#cv2" class="headerlink" title="cv2"></a>cv2</h1><p>报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ImportError: No module named cv2</span><br></pre></td></tr></table></figure>
<p>解决：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-opencv</span><br></pre></td></tr></table></figure>
<h1 id="yaml"><a href="#yaml" class="headerlink" title="yaml"></a>yaml</h1><p>报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ImportError: No module named yaml</span><br></pre></td></tr></table></figure>
<p>解决：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-yaml</span><br></pre></td></tr></table></figure>
<h1 id="lmdb"><a href="#lmdb" class="headerlink" title="lmdb"></a>lmdb</h1><p>报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo pip install lmdb</span><br></pre></td></tr></table></figure>

<h1 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h1><p>[ numpy ](/2018/04/08/ubuntu 安装numpy的烂问题libgfortran3依赖)  </p>
<h1 id="skimage-io-google-protobuf"><a href="#skimage-io-google-protobuf" class="headerlink" title="skimage.io | google.protobuf"></a>skimage.io | google.protobuf</h1><p><a href="/2018/04/08/fast-rcnn安装及例子执行中的问题（一）"> skimage.io | google.protobuf.internal </a></p>
]]></content>
      <categories>
        <category>caffe安装&amp;&amp;问题&amp;&amp;解决</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn的源码理解（一）SmoothL1LossLayer论文与代码的结合理解</title>
    <url>/2018/09/08/faster%20rcnn%E7%9A%84%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3%EF%BC%88%E4%B8%80%EF%BC%89SmoothL1LossLayer%E8%AE%BA%E6%96%87%E4%B8%8E%E4%BB%A3%E7%A0%81%E7%9A%84%E7%BB%93%E5%90%88%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">// Fast R-CNN</span></span><br><span class="line"><span class="comment">// Copyright (c) 2015 Microsoft</span></span><br><span class="line"><span class="comment">// Licensed under The MIT License [see fast-rcnn/LICENSE for details]</span></span><br><span class="line"><span class="comment">// Written by Ross Girshick</span></span><br><span class="line"><span class="comment">// ------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"caffe/fast_rcnn_layers.hpp"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> caffe &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">SmoothL1Forward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* in, Dtype* out,</span></span></span><br><span class="line"><span class="function"><span class="params">    Dtype sigma2)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// f(x) = 0.5 * (sigma * x)^2          if |x| &lt; 1 / sigma / sigma</span></span><br><span class="line">  <span class="comment">//        |x| - 0.5 / sigma / sigma    otherwise</span></span><br><span class="line">  CUDA_KERNEL_LOOP(index, n) &#123;</span><br><span class="line">    Dtype val = in[index];</span><br><span class="line">    Dtype abs_val = <span class="built_in">abs</span>(val);</span><br><span class="line">    <span class="keyword">if</span> (abs_val &lt; <span class="number">1.0</span> / sigma2) &#123;</span><br><span class="line">      out[index] = <span class="number">0.5</span> * val * val * sigma2;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      out[index] = abs_val - <span class="number">0.5</span> / sigma2;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> SmoothL1LossLayer&lt;Dtype&gt;::Forward_gpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">  <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line">  caffe_gpu_sub(</span><br><span class="line">      count,</span><br><span class="line">      bottom[<span class="number">0</span>]-&gt;gpu_data(),</span><br><span class="line">      bottom[<span class="number">1</span>]-&gt;gpu_data(),</span><br><span class="line">      diff_.mutable_gpu_data());    <span class="comment">// d := b0 - b1</span></span><br><span class="line">  <span class="keyword">if</span> (has_weights_) &#123;</span><br><span class="line">    <span class="comment">// apply "inside" weights</span></span><br><span class="line">    caffe_gpu_mul(</span><br><span class="line">        count,</span><br><span class="line">        bottom[<span class="number">2</span>]-&gt;gpu_data(),</span><br><span class="line">        diff_.gpu_data(),</span><br><span class="line">        diff_.mutable_gpu_data());  <span class="comment">// d := w_in * (b0 - b1)</span></span><br><span class="line">  &#125;</span><br><span class="line">  SmoothL1Forward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(</span><br><span class="line">      count, diff_.gpu_data(), errors_.mutable_gpu_data(), sigma2_);</span><br><span class="line">  CUDA_POST_KERNEL_CHECK;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (has_weights_) &#123;</span><br><span class="line">    <span class="comment">// apply "outside" weights</span></span><br><span class="line">    caffe_gpu_mul(</span><br><span class="line">        count,</span><br><span class="line">        bottom[<span class="number">3</span>]-&gt;gpu_data(),</span><br><span class="line">        errors_.gpu_data(),</span><br><span class="line">        errors_.mutable_gpu_data());  <span class="comment">// d := w_out * SmoothL1(w_in * (b0 - b1))</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  Dtype loss;</span><br><span class="line">  caffe_gpu_dot(count, ones_.gpu_data(), errors_.gpu_data(), &amp;loss);</span><br><span class="line">  top[<span class="number">0</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] = loss / bottom[<span class="number">0</span>]-&gt;num();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">SmoothL1Backward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* in, Dtype* out,</span></span></span><br><span class="line"><span class="function"><span class="params">    Dtype sigma2)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// f'(x) = sigma * sigma * x         if |x| &lt; 1 / sigma / sigma</span></span><br><span class="line">  <span class="comment">//       = sign(x)                   otherwise</span></span><br><span class="line">  CUDA_KERNEL_LOOP(index, n) &#123;</span><br><span class="line">    Dtype val = in[index];</span><br><span class="line">    Dtype abs_val = <span class="built_in">abs</span>(val);</span><br><span class="line">    <span class="keyword">if</span> (abs_val &lt; <span class="number">1.0</span> / sigma2) &#123;</span><br><span class="line">      out[index] = sigma2 * val;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      out[index] = (Dtype(<span class="number">0</span>) &lt; val) - (val &lt; Dtype(<span class="number">0</span>));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> SmoothL1LossLayer&lt;Dtype&gt;::Backward_gpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</span><br><span class="line">  <span class="comment">// after forwards, diff_ holds w_in * (b0 - b1)</span></span><br><span class="line">  <span class="keyword">int</span> count = diff_.count();</span><br><span class="line">  SmoothL1Backward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(</span><br><span class="line">      count, diff_.gpu_data(), diff_.mutable_gpu_data(), sigma2_);</span><br><span class="line">  CUDA_POST_KERNEL_CHECK;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span> (propagate_down[i]) &#123;</span><br><span class="line">      <span class="keyword">const</span> Dtype sign = (i == <span class="number">0</span>) ? <span class="number">1</span> : <span class="number">-1</span>;</span><br><span class="line">      <span class="keyword">const</span> Dtype alpha = sign * top[<span class="number">0</span>]-&gt;cpu_diff()[<span class="number">0</span>] / bottom[i]-&gt;num();</span><br><span class="line">      caffe_gpu_axpby(</span><br><span class="line">          count,                           <span class="comment">// count</span></span><br><span class="line">          alpha,                           <span class="comment">// alpha</span></span><br><span class="line">          diff_.gpu_data(),                <span class="comment">// x</span></span><br><span class="line">          Dtype(<span class="number">0</span>),                        <span class="comment">// beta</span></span><br><span class="line">          bottom[i]-&gt;mutable_gpu_diff());  <span class="comment">// y</span></span><br><span class="line">      <span class="keyword">if</span> (has_weights_) &#123;</span><br><span class="line">        <span class="comment">// Scale by "inside" weight</span></span><br><span class="line">        caffe_gpu_mul(</span><br><span class="line">            count,</span><br><span class="line">            bottom[<span class="number">2</span>]-&gt;gpu_data(),</span><br><span class="line">            bottom[i]-&gt;gpu_diff(),</span><br><span class="line">            bottom[i]-&gt;mutable_gpu_diff());</span><br><span class="line">        <span class="comment">// Scale by "outside" weight</span></span><br><span class="line">        caffe_gpu_mul(</span><br><span class="line">            count,</span><br><span class="line">            bottom[<span class="number">3</span>]-&gt;gpu_data(),</span><br><span class="line">            bottom[i]-&gt;gpu_diff(),</span><br><span class="line">            bottom[i]-&gt;mutable_gpu_diff());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">INSTANTIATE_LAYER_GPU_FUNCS(SmoothL1LossLayer);</span><br><span class="line"></span><br><span class="line">&#125;  <span class="comment">// namespace caffe</span></span><br></pre></td></tr></table></figure>
<h3 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h3><p>SmoothL1LossLayer  计算一张图片的损失函数，对应于下图的加号右边部分</p>
<p><img src="21.png" alt=""></p>
<p>i  是mini-batch的anchor的索引。<br>Pi  是目标的预测概率。<br>有物体时pi*为1，否则为  0<br>ti  是一个向量，预测坐标<br>ti*  是一个向量，是gt包围盒的坐标</p>
<p><img src="22.png" alt="">  </p>
<p>bottom[0]  预测坐标，对应于下图的ti<br>bottom[1]target  坐标，对应于下图的ti*<br>bottom[2]inside  有物体(fg)时为1，否则为0，对应于下图的pi*<br>bottom[3]outside 没有前景（fg）也没有后景（bg）的为0，其他为1/（bg+fg），对应于加号右边的系数部分（但其实这个地方我本人还是不懂，因为论文上说的系数都是一些固定的值，如 入 =10。初始代码一直在更新，估计又换了别的方法。不论如何，在现在的代码中  outside  是乘以了后面的结果）</p>
<p>Lreg的公式就是下图，另x=ti - ti*</p>
<p><img src="23.png" alt=""></p>
<p>Pi × Leg(ti, ti*)  表明只有有fg（20个物体类别）的才有回归损失</p>
<p><img src="24.png" alt="">  </p>
]]></content>
      <categories>
        <category>faster rcnn</category>
        <category>faster cnn源码理解</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>faster rcnn源码理解</tag>
      </tags>
  </entry>
  <entry>
    <title>numpy常用方法</title>
    <url>/2018/09/08/numpy%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>一般参数有axis时，<strong>0是列； 1是行<strong><br>假设 ：  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> scio</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<h5 id="求平均值-mean"><a href="#求平均值-mean" class="headerlink" title="求平均值 mean"></a>求平均值 mean</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">array1.mean(axis=<span class="number">0</span>) == numpy.mean(array1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">Examples</span><br><span class="line">--------</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.mean(a)</span><br><span class="line"><span class="number">2.5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.mean(a, axis=<span class="number">0</span>)</span><br><span class="line">array([ <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.mean(a, axis=<span class="number">1</span>)</span><br><span class="line">array([ <span class="number">1.5</span>, <span class="number">3.5</span>])</span><br></pre></td></tr></table></figure>
<p>axis=0 对 列求平均值。  </p>
<h5 id="求方差-std"><a href="#求方差-std" class="headerlink" title="求方差 std"></a>求方差 std</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">array1.std(axis=<span class="number">0</span>) == numpy.std(array1, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>axis的意义同1，求方差。  </p>
<h5 id="numpy-scipy-pandas-区别"><a href="#numpy-scipy-pandas-区别" class="headerlink" title="numpy scipy pandas 区别"></a>numpy scipy pandas 区别</h5><p>NumPy 是基础的 数学 计算库，包括 基本的四则运行，方程式 计算，微积分 什么的，还有很多其他数学方面的计算，我也不是很清楚<br>SciPy 是在NumPy基础上，封装了一层，没有那么纯数学，提供方法直接计算结果<br>Pandas 是上层做数据分析用的，主要是做表格数据呈现<br>如果不是纯数学专业还是从 Pandas 入手比较好。  </p>
<h5 id="读-mat文件-loadmat"><a href="#读-mat文件-loadmat" class="headerlink" title="读.mat文件 loadmat"></a>读.mat文件 loadmat</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scio.loadmat(train_data_file) </span><br><span class="line"><span class="comment"># return is a dict</span></span><br></pre></td></tr></table></figure>
<h5 id="求几次方"><a href="#求几次方" class="headerlink" title="求几次方 **"></a>求几次方 **</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">diff ** <span class="number">2</span> </span><br><span class="line"><span class="comment"># 2次方，2换成0.5就是开方 diff 可数字和数组</span></span><br></pre></td></tr></table></figure>
<h5 id="数组求和-sum"><a href="#数组求和-sum" class="headerlink" title="数组求和 sum"></a>数组求和 sum</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">diff.sum(axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 求数组的和， axis的值同上 0是列； 1是行； diff 数组</span></span><br></pre></td></tr></table></figure>
<h5 id="排序下标-argsort"><a href="#排序下标-argsort" class="headerlink" title="排序下标 argsort"></a>排序下标 argsort</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">diff.argsort(axis=<span class="number">1</span>)</span><br><span class="line"> <span class="comment"># diff数组从小到大数据的下标 axis同上，默认是1</span></span><br><span class="line">examples：</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.argsort()</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="comment"># 输出数组行从小到大排序的原始数据坐标；</span></span><br><span class="line"><span class="comment"># 如，index=0行的1,2 从小到大排序还是 1 2 ，1原来数组的下标是0; 2原来数组的下标是1 ； 因此返回数据的第一行是0 1</span></span><br></pre></td></tr></table></figure>
<h5 id="dict-get"><a href="#dict-get" class="headerlink" title="dict get"></a>dict get</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">class_count.get(label, <span class="number">0</span>) </span><br><span class="line"><span class="comment"># class_count 是一个 dict 。 从class_count找key = label对应的value，找到就返回value，找不到就返回0。</span></span><br></pre></td></tr></table></figure>
<h5 id="dict排序-sorted"><a href="#dict排序-sorted" class="headerlink" title="dict排序 sorted"></a>dict排序 sorted</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sorted_class_count = sorted(class_count.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>）</span><br><span class="line"><span class="comment"># 对字典的value排序，返回一个list，list里包含很多元组，这些元组就是之前dict的key-value对儿。</span></span><br><span class="line"><span class="comment"># class_count 是一个 dict 。 reverse=True致使value是从大到小的顺序。</span></span><br><span class="line"><span class="comment"># sorted_class_count[][]</span></span><br></pre></td></tr></table></figure>
<h5 id="统计次数-bincount"><a href="#统计次数-bincount" class="headerlink" title="统计次数 bincount"></a>统计次数 bincount</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.bincount(a)</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># 对数组中数据统计次数；返回数组下标是原始数组的值，返回数组的数据值是次数。例如上面数组a值1出现的次数是1次，因此返回数组index=1的值为1，index=2的值为2</span></span><br><span class="line"><span class="comment"># 输入数组使用中发现的约束条件：一维数组、 必须是int整形。（个人理解，更适用于数组数据比较集中且从0/1开始的数）</span></span><br></pre></td></tr></table></figure>
<h5 id="np-random-uniform（start，-end-，-size-）"><a href="#np-random-uniform（start，-end-，-size-）" class="headerlink" title="np.random.uniform（start， end[， size]）"></a>np.random.uniform（start， end[， size]）</h5><p>生成一个数值<strong>均匀分布</strong>在start，end间的长度大小是size的数组（ndarray类型）。 注意是&gt;= start &amp;&amp; &lt; end ，前闭后开。 size默认是1 </p>
<h5 id="np的flatten-vs-ravel"><a href="#np的flatten-vs-ravel" class="headerlink" title="np的flatten() vs ravel()"></a>np的flatten() vs ravel()</h5><p>两者都是把多维矩阵铺平，以行为主。区别是flatten返回的是原矩阵的拷贝；ravel是返回的是原矩阵的一种变换视图，如果对返回值修改原矩阵也会跟着变化。  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a.flatten()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a <span class="comment">#原矩阵没有变化</span></span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = a.ravel()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a <span class="comment">#原矩阵变化</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<h5 id="np-mgrid-生成网格"><a href="#np-mgrid-生成网格" class="headerlink" title="np.mgrid 生成网格"></a>np.mgrid 生成网格</h5><p>np.mgrid[start: end: size/seperate]原本是生成一个[start, end)的表格，分割大小是seperate；第三个参数后面有j时表示生成一个[start, end]的表格，表格大小是size：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.mgrid[<span class="number">-10</span>:<span class="number">10</span>:<span class="number">5j</span>] <span class="comment"># 有‘j’。生成大小是5的分布在[-10, 10]的表格</span></span><br><span class="line">array([<span class="number">-10.</span>, <span class="number">-5.</span>, <span class="number">0.</span>, <span class="number">5.</span>, <span class="number">10.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.mgrid[<span class="number">-10</span>:<span class="number">10</span>:<span class="number">5</span>] <span class="comment"># 用5分割[-10, 10)， 生成一个表格。</span></span><br><span class="line">array([<span class="number">-10</span>, <span class="number">-5</span>, <span class="number">0</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>arr1, arr2 = np.mgrid[<span class="number">-10</span>:<span class="number">10</span>:<span class="number">5</span>, <span class="number">-10</span>:<span class="number">10</span>:<span class="number">5j</span>] <span class="comment"># arr1，arr2铺平 再stack ，可以很好的作为二位坐标数据。</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>arr1</span><br><span class="line">array([[<span class="number">-10.</span>, <span class="number">-10.</span>, <span class="number">-10.</span>, <span class="number">-10.</span>, <span class="number">-10.</span>],</span><br><span class="line">[ <span class="number">-5.</span>, <span class="number">-5.</span>, <span class="number">-5.</span>, <span class="number">-5.</span>, <span class="number">-5.</span>],</span><br><span class="line">[ <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">[ <span class="number">5.</span>, <span class="number">5.</span>, <span class="number">5.</span>, <span class="number">5.</span>, <span class="number">5.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>arr2</span><br><span class="line">array([[<span class="number">-10.</span>, <span class="number">-5.</span>, <span class="number">0.</span>, <span class="number">5.</span>, <span class="number">10.</span>],</span><br><span class="line">[<span class="number">-10.</span>, <span class="number">-5.</span>, <span class="number">0.</span>, <span class="number">5.</span>, <span class="number">10.</span>],</span><br><span class="line">[<span class="number">-10.</span>, <span class="number">-5.</span>, <span class="number">0.</span>, <span class="number">5.</span>, <span class="number">10.</span>],</span><br><span class="line">[<span class="number">-10.</span>, <span class="number">-5.</span>, <span class="number">0.</span>, <span class="number">5.</span>, <span class="number">10.</span>]])</span><br></pre></td></tr></table></figure>
<h5 id="np-amin-amax-np-min-max"><a href="#np-amin-amax-np-min-max" class="headerlink" title="np.amin/amax == np.min/max"></a>np.amin/amax == np.min/max</h5><h5 id="矩阵拼接np-vstack-tuple-np-hstack-tuple-np-concatenate-tuple-np-stack"><a href="#矩阵拼接np-vstack-tuple-np-hstack-tuple-np-concatenate-tuple-np-stack" class="headerlink" title="矩阵拼接np.vstack(tuple) np.hstack(tuple) np.concatenate(tuple) np.stack()"></a>矩阵拼接np.vstack(tuple) np.hstack(tuple) np.concatenate(tuple) np.stack()</h5><p>tuple是一个arrays，就是由多个矩阵组成。<br>hstack(tuple) 是把多个矩阵以行拼接，等同于np.concatenate(tuple, axis=1)<br>vstack(tuple) 是把多个矩阵以列拼接，等同于np.concatenate(tuple, axis=0)  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = np.array([[<span class="number">2</span>,<span class="number">3</span>,<span class="number">9</span>], [<span class="number">2</span>,<span class="number">6</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.hstack((a, b))</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.concatenate((a,b), axis = <span class="number">1</span>)</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">4</span>]])</span><br><span class="line">    </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.vstack((a, b))</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">6</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.concatenate((a,b), axis = <span class="number">0</span>)</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">6</span>, <span class="number">4</span>]])</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.stack((a, b), axis=<span class="number">0</span>)</span><br><span class="line">array([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]],</span><br><span class="line">    </span><br><span class="line">[[<span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">6</span>, <span class="number">4</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.stack((a, b), axis=<span class="number">1</span>)</span><br><span class="line">array([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>]],</span><br><span class="line">    </span><br><span class="line">[[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">[<span class="number">2</span>, <span class="number">6</span>, <span class="number">4</span>]]])</span><br></pre></td></tr></table></figure>
<h5 id="np-transpose-元组-和-T"><a href="#np-transpose-元组-和-T" class="headerlink" title="np.transpose(元组) 和 T"></a>np.transpose(元组) 和 T</h5><p>transpose适用于多维数组，它依赖与参数 元组 ，元组依赖与 shape。<br>T用于一/二维数组。  </p>
<h5 id="代表一个维度的切片"><a href="#代表一个维度的切片" class="headerlink" title=":, 代表一个维度的切片"></a>:, 代表一个维度的切片</h5><p>eg：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = io.imread(img_path)</span><br><span class="line">data = np.zeros((<span class="number">3</span>, img.shape[<span class="number">0</span>], img.shape[<span class="number">1</span>]), dtype=np.uint8)</span><br><span class="line">data[<span class="number">0</span>] = img[:, :, <span class="number">0</span>]</span><br><span class="line">data[<span class="number">1</span>] = img[:, :, <span class="number">1</span>]</span><br><span class="line">data[<span class="number">2</span>] = img[:, :, <span class="number">2</span>]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>numpy</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title>python的Image和skimage处理图片</title>
    <url>/2018/09/08/python%E7%9A%84Image%E5%92%8Cskimage%E5%A4%84%E7%90%86%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<p>前提</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  Image</span><br></pre></td></tr></table></figure>
<h3 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h3><h4 id="转灰度"><a href="#转灰度" class="headerlink" title="转灰度"></a>转灰度</h4><pre><code>img = Image.open(path)#打开图片 
img.getpixel((height, width))#得到(height, width)处的像素值（可能是一个list，3通道）
img.convert(&quot;L&quot;)#转灰度图</code></pre><p><img src="2.png" alt=""></p>
<h4 id="改变尺寸"><a href="#改变尺寸" class="headerlink" title="改变尺寸"></a>改变尺寸</h4><pre><code>size = (64, 64)
img.resize(size, Image.ANTIALIAS)#改变尺寸</code></pre><p><img src="3.png" alt="">  </p>
<h4 id="截图"><a href="#截图" class="headerlink" title="截图"></a>截图</h4><pre><code>box = (10, 10, 100, 100)
img.crop(box)#在img上的box处截图</code></pre><p><img src="4.png" alt="">  </p>
<h4 id="加噪音"><a href="#加噪音" class="headerlink" title="加噪音"></a>加噪音</h4><pre><code>img_data = np.array(img)
for i in xrange(300):
  x = random.randint(0, img_data.shape[0]-1)
  y = random.randint(0, img_data.shape[1]-1)
  img_data[x][y][0] = 255
img = Image.fromarray(img_data)#加300个噪音,转来转去麻烦可以直接用skimage度图片就不用转了</code></pre><p><img src="5.png" alt="">  </p>
<h4 id="图片旋转"><a href="#图片旋转" class="headerlink" title="图片旋转"></a>图片旋转</h4><pre><code>img.rotate(90)#图片旋转90</code></pre><p><img src="6.png" alt="">  </p>
<h4 id="图片镜像"><a href="#图片镜像" class="headerlink" title="图片镜像"></a>图片镜像</h4><pre><code>img.transpose(Image.FLIP_LEFT_RIGHT)#图片镜像</code></pre><p><img src="7.png" alt=""></p>
<h3 id="skimage"><a href="#skimage" class="headerlink" title="skimage"></a>skimage</h3><h4 id="改变尺寸-1"><a href="#改变尺寸-1" class="headerlink" title="改变尺寸"></a>改变尺寸</h4><pre><code>from skimage import io,transform
img_data = io.imread(img_path)
transform.resize(img_data, (64, 64))#改变尺寸</code></pre><p><img src="8.png" alt="">  </p>
<h4 id="缩小-放大图片"><a href="#缩小-放大图片" class="headerlink" title="缩小/放大图片"></a>缩小/放大图片</h4><pre><code>transform.rescale(img_data, 0.5)#缩小/放大图片</code></pre><p><img src="9.png" alt="">  </p>
]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>图片处理</tag>
      </tags>
  </entry>
  <entry>
    <title>spring遇到问题</title>
    <url>/2018/09/05/spring%E9%81%87%E5%88%B0%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1 id="javax-management-RuntimeMBeanException-java-lang-UnsupportedOperationException-Usage-threshold-is-not-supported"><a href="#javax-management-RuntimeMBeanException-java-lang-UnsupportedOperationException-Usage-threshold-is-not-supported" class="headerlink" title="javax.management.RuntimeMBeanException: java.lang.UnsupportedOperationException: Usage threshold is not supported"></a>javax.management.RuntimeMBeanException: java.lang.UnsupportedOperationException: Usage threshold is not supported</h1><p>使用spring，启动tomcat时报错：<code>javax.management.RuntimeMBeanException: java.lang.UnsupportedOperationException: Usage threshold is not supported</code></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">javax.management.RuntimeMBeanException: java.lang.UnsupportedOperationException: Usage threshold is not supported</span><br><span class="line">        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839) ~[na:1.8.0_144]</span><br><span class="line">        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852) ~[na:1.8.0_144]</span><br><span class="line">        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651) ~[na:1.8.0_144]</span><br><span class="line">        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678) ~[na:1.8.0_144]</span><br><span class="line">        at servlet.server.TomcatServer.extractContext(TomcatServer.java:40) [common-core-8.3.2.jar:na]</span><br><span class="line">        at servlet.server.ServerWrapper.<span class="tag">&lt;<span class="name">init</span>&gt;</span>(ServerWrapper.java:26) [common-core-8.3.2.jar:na]</span><br><span class="line">        at servlet.server.TomcatServer.<span class="tag">&lt;<span class="name">init</span>&gt;</span>(TomcatServer.java:14) [common-core-8.3.2.jar:na]</span><br><span class="line">        at ServletWatcher.portOf(ServletWatcher.java:240) [common-core-8.3.2.jar:na]</span><br><span class="line">        at ServletWatcher.fixPort(ServletWatcher.java:232) [common-core-8.3.2.jar:na]</span><br><span class="line">        at ServletWatcher.init(ServletWatcher.java:91) [common-core-8.3.2.jar:na]</span><br><span class="line">        at ServletWatcher.contextInitialized(ServletWatcher.java:58) [common-core-8.3.2.jar:na]</span><br><span class="line">        at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:5016) [tomcat-embed-core-7.0.59.jar:7.0.59]</span><br><span class="line">        at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5524) [tomcat-embed-core-7.0.59.jar:7.0.59]</span><br><span class="line">        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150) [tomcat-embed-core-7.0.59.jar:7.0.59]</span><br><span class="line">        at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1575) [tomcat-embed-core-7.0.59.jar:7.0.59]</span><br><span class="line">        at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1565) [tomcat-embed-core-7.0.59.jar:7.0.59]</span><br><span class="line">        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_144]</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_144]</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144]</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]</span><br><span class="line">Caused by: java.lang.UnsupportedOperationException: Usage threshold is not supported</span><br><span class="line">        at sun.management.MemoryPoolImpl.getUsageThreshold(MemoryPoolImpl.java:106) ~[na:1.8.0_144]</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]</span><br><span class="line">        at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71) ~[na:1.8.0_144]</span><br><span class="line">        at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source) ~[na:na]</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]</span><br><span class="line">        at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275) ~[na:1.8.0_144]</span><br><span class="line">        at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193) ~[na:1.8.0_144]</span><br><span class="line">        at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175) ~[na:1.8.0_144]</span><br><span class="line">        at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117) ~[na:1.8.0_144]</span><br><span class="line">        at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54) ~[na:1.8.0_144]</span><br><span class="line">        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237) ~[na:1.8.0_144]</span><br><span class="line">        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83) ~[na:1.8.0_144]</span><br><span class="line">        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206) ~[na:1.8.0_144]</span><br><span class="line">        at javax.management.StandardMBean.getAttribute(StandardMBean.java:372) ~[na:1.8.0_144]</span><br><span class="line">        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647) ~[na:1.8.0_144]</span><br><span class="line">        ... 17 common frames omitted</span><br></pre></td></tr></table></figure>
<p>原因：jmxmp的端口被占了</p>
<p>参考：<a href="https://stackoverflow.com/questions/12283207/unable-to-connect-with-jmxmp-in-tomcat/12337393#12337393?s=d4f50b7dc0164ed298b008d97968de5b" target="_blank" rel="noopener">https://stackoverflow.com/questions/12283207/unable-to-connect-with-jmxmp-in-tomcat/12337393#12337393?s=d4f50b7dc0164ed298b008d97968de5b</a></p>
<h1 id="Error-instantiating-class-com-market-model-with-invalid-types-or-values-Cause-java-lang-NoSuchMethodException-com-market-model-MaterialDeliverModel"><a href="#Error-instantiating-class-com-market-model-with-invalid-types-or-values-Cause-java-lang-NoSuchMethodException-com-market-model-MaterialDeliverModel" class="headerlink" title="Error instantiating class com.market.model. with invalid types () or values (). Cause: java.lang.NoSuchMethodException: com.market.model.MaterialDeliverModel"></a>Error instantiating class com.market.model. with invalid types () or values (). Cause: java.lang.NoSuchMethodException: com.market.model.MaterialDeliverModel</h1><p>原因：构造函数被重载过，但是没有空的构造函数。<br>参考：<a href="https://blog.csdn.net/qq_25821067/article/details/54811165" target="_blank" rel="noopener">https://blog.csdn.net/qq_25821067/article/details/54811165</a></p>
]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux脚本开头#!/bin/bash和#!/bin/sh是什么意思以及区别</title>
    <url>/2018/08/14/Linux%E8%84%9A%E6%9C%AC%E5%BC%80%E5%A4%B4bash%E5%92%8Csh%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D%E4%BB%A5%E5%8F%8A%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>转载自：<a href="https://www.cnblogs.com/EasonJim/p/6850319.html" target="_blank" rel="noopener">https://www.cnblogs.com/EasonJim/p/6850319.html</a></p>
<h3 id="意思"><a href="#意思" class="headerlink" title="意思"></a>意思</h3><p><code>#!/bin/sh</code>是指此脚本使用<code>/bin/sh</code>来解释执行，<code>#!</code>是特殊的表示符，其后面根的是此解释此脚本的shell的路径。<br>其实第一句的<code>#!</code>是对脚本的解释器程序路径，脚本的内容是由解释器解释的，我们可以用各种各样的解释器来写对应的脚本。<br>比如说<code>/bin/csh</code>脚本，<code>/bin/perl</code>脚本，<code>/bin/awk</code>脚本，<code>/bin/sed</code>脚本，甚至<code>/bin/echo</code>等等。<br><code>#!/bin/bash</code>同理。</p>
<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p><img src="1.png" alt=""><br>GNU/Linux操作系统中的/bin/sh本是bash (Bourne-Again Shell) 的符号链接，但鉴于bash过于复杂，有人把bash从NetBSD移植到Linux并更名为dash (Debian Almquist Shell)，并建议将/bin/sh指向它，以获得更快的脚本执行速度。Dash Shell 比Bash Shell小的多，符合POSIX标准。<br>Ubuntu继承了Debian，所以从Ubuntu 6.10开始默认是Dash Shell。<br>应该说，/bin/sh与/bin/bash虽然大体上没什么区别，但仍存在不同的标准。标记为#!/bin/sh的脚本不应使用任何POSIX没有规定的特性 (如let等命令, 但#!/bin/bash可以)。Debian曾经采用/bin/bash更改/bin/dash，目的使用更少的磁盘空间、提供较少的功能、获取更快的速度。但是后来经过shell脚本测试存在运行问题。因为原先在bash shell下可以运行的shell script (shell 脚本)，在/bin/sh下还是会出现一些意想不到的问题，不是100%的兼用。<br>上面可以这样理解，使用man sh命令和man bash命令去观察，可以发现sh本身就是dash，也就更好的说明集成Debian系统之后的更改。</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>shell编写总结</title>
    <url>/2018/08/10/shell%E7%BC%96%E5%86%99%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"></span><br><span class="line">a=<span class="string">"ni n"</span></span><br><span class="line">b=<span class="string">"ni n"</span></span><br><span class="line">c=<span class="string">"nin"</span></span><br><span class="line">d=<span class="string">"ni"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$a</span>"</span> = <span class="string">"<span class="variable">$b</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">'a = b'</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$a</span>"</span> != <span class="string">"<span class="variable">$c</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">'a != c'</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$c</span>"</span> \&gt; <span class="string">"<span class="variable">$d</span>"</span> -o <span class="string">"<span class="variable">$c</span>"</span> = <span class="string">"<span class="variable">$d</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">'c &gt;= d '</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$a</span>"</span> \&lt; <span class="string">"<span class="variable">$b</span>"</span> -o <span class="string">"<span class="variable">$a</span>"</span> = <span class="string">"<span class="variable">$b</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">'a &lt;= b'</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<p><code>/bin/sh</code> 没有<code>[[]]</code><br>字符串等于： <code>[ &quot;$a&quot; = &quot;$b&quot; ]</code> 不能用 ==，等号前后的空格不能省<br>字符串不等： <code>[ &quot;$a&quot; != &quot;$b&quot; ]</code><br>字符串小于： <code>[ &quot;$a&quot; \&lt; &quot;$b&quot; ]</code><br>字符串大于： <code>[ &quot;$a&quot; \&gt; &quot;$b&quot; ]</code><br>报错<code>“integer expression expected”</code> 时一般都是因为<code>[ &quot;qw&quot; -eq &quot;as&quot; ]</code> 等的操作。<code>-eq -ne -lt -gt</code>等只能是数字比较，不能用于字符串。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">q=`expr 1 / 1`</span><br><span class="line">w=$((1/1))</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$q</span>, <span class="variable">$w</span></span><br><span class="line"><span class="comment"># 赋值操作等号前后不能有空格</span></span><br><span class="line"><span class="comment"># 加减乘除： （1）`expr 1 + 1` 加号前后必须有空格;乘法时。乘法符号前必须有\:`expr 1 \* 1`  (2)`$((1+1))` 加号前后可以不用空格</span></span><br><span class="line"><span class="comment"># 只要括号中的运算符、表达式符合C语言运算规则，都可用在$((exp))中，甚至是三目运算符。</span></span><br><span class="line"><span class="comment"># $() 和 `` 效果一样，执行命令</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yesterday=`date -d <span class="string">"2018-08-08 1 days ago"</span> +<span class="string">"%Y-%m-%d"</span>`</span><br><span class="line">tomorrow=`date -d <span class="string">"2018-08-08 1 days"</span> +<span class="string">"%Y-%m-%d"</span>`</span><br><span class="line">yesterday1=`date -d <span class="string">"-1 days 2018-08-08"</span> +<span class="string">"%Y-%m-%d"</span>`</span><br><span class="line">tomorrow1=`date -d <span class="string">"1 days 2018-08-08"</span> +<span class="string">"%Y-%m-%d"</span>`</span><br><span class="line">today=`date +<span class="string">"%Y-%m-%d"</span>`</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$yesterday</span>, <span class="variable">$tomorrow</span>, <span class="variable">$yesterday1</span>, <span class="variable">$tomorrow1</span>, <span class="variable">$today</span></span><br><span class="line"><span class="comment"># 日期加减1： `date -d "2018-08-08 1 days ago" +"%Y-%m-%d"` 减时间的效果；去掉`ago`就是加时间的效果</span></span><br><span class="line"><span class="comment"># 日期加减2：`date -d "-1 days 2018-08-08" +"%Y-%m-%d"`  减时间的效果；把`-1`改成`1`/`+1`就是加时间的效果</span></span><br><span class="line"><span class="comment"># date是现在时间 ；上面的`2018-08-08`可以用`now`代替今天</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">z=1</span><br><span class="line">x=1</span><br><span class="line">v=2</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$z</span> = <span class="variable">$x</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"z = x"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$z</span> != <span class="variable">$v</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"z != v"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数字等于： [ $z = $v ] 不能用==，等号前后的空格不能省</span></span><br><span class="line"><span class="comment"># 数字不等： [ $z != $v ]等号前后的空格不能省</span></span><br></pre></td></tr></table></figure>

<p>多线程：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># run processes and store pids in array</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="variable">$n_procs</span>; <span class="keyword">do</span></span><br><span class="line">    ./procs[<span class="variable">$&#123;i&#125;</span>] &amp;</span><br><span class="line">    pids[<span class="variable">$&#123;i&#125;</span>]=$!</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># wait for all pids</span></span><br><span class="line"><span class="keyword">for</span> pid <span class="keyword">in</span> <span class="variable">$&#123;pids[*]&#125;</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">wait</span> <span class="variable">$pid</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>进程与线程</title>
    <url>/2018/08/06/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/</url>
    <content><![CDATA[<p>进程：进程是资源（CPU、内存等）分配的基本单位，它是程序执行时的一个实例。程序运行时系统就会创建一个进程，并为它分配资源，然后把该进程放入进程就绪队列，进程调度器选中它的时候就会为它分配CPU时间，程序开始真正运行。<br>Linux系统函数fork()可以在父进程中创建一个子进程，这样的话，在一个进程接到来自客户端新的请求时就可以复制出一个子进程让其来处理，父进程只需负责监控请求的到来，然后创建子进程让其去处理，这样就能做到并发处理。</p>
<p>线程：线程是程序执行时的最小单位，它是进程的一个执行流，是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。同样多线程也可以实现并发操作，每个请求分配一个线程来处理。</p>
<p>进程和线程各自有什么区别和优劣呢？</p>
<ul>
<li>进程是资源分配的最小单位，线程是程序执行的最小单位。</li>
<li>进程有自己的独立地址空间，每启动一个进程，系统就会为它分配地址空间，建立数据表来维护代码段、堆栈段和数据段，这种操作非常昂贵。而线程是共享进程中的数据的，使用相同的地址空间，因此CPU切换一个线程的花费远比进程要小很多，同时创建一个线程的开销也比进程要小很多。</li>
<li>线程之间的通信更方便，同一进程下的线程共享全局变量、静态变量等数据，而进程之间的通信需要以通信的方式（IPC)进行。不过如何处理好同步与互斥是编写多线程程序的难点。</li>
<li>但是多进程程序更健壮，多线程程序只要有一个线程死掉，整个进程也死掉了，而一个进程死掉并不会对另外一个进程造成影响，因为进程有自己独立的地址空间。</li>
</ul>
<p>转载：<a href="https://www.cnblogs.com/zhehan54/p/6130030.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhehan54/p/6130030.html</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>shell中的单引号、双引号</title>
    <url>/2018/06/29/shell%E4%B8%AD%E7%9A%84%E5%8D%95%E5%BC%95%E5%8F%B7%E3%80%81%E5%8F%8C%E5%BC%95%E5%8F%B7/</url>
    <content><![CDATA[<h1 id="单引号"><a href="#单引号" class="headerlink" title="单引号"></a>单引号</h1><p>在单引号中不能使用任何变量和命令。<br>由单引号括起来的字符<strong><em>都</em></strong>作为普通字符出现。比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ind&#x3D;1</span><br><span class="line">newday&#x3D;&#96;date -d &quot;$&#123;ind&#125; day ago&quot; +&quot;%Y-%m-%d&quot;&#96;</span><br></pre></td></tr></table></figure>
<p>报错：<code>date: 无效的日期&quot;${ind} day ago&quot;</code><br>原因：<code>${}</code>由单引号括起来了，只能作为普通字符。</p>
<h1 id="双引号"><a href="#双引号" class="headerlink" title="双引号"></a>双引号</h1><p>除了<code>$</code>（美元符号）、<code>\</code>（反斜杠）、<code>’</code>（单引号）、和<code>”</code>（双引号）四个符号，其他字符都作为普通字符含义。</p>
<p>对于“$”来说，就是用其后指定的变量的值来代替这个变量和$；<br>对于“\”而言，是转义字符，它告诉shell不要对其后面的那个字符进行特殊处理，只当作普通字符即可。可以想见，在双引号中需要在前面加上“\”的只有四个字符<code>$</code>，<code>\</code>，<code>’</code>和<code>”</code>本身。<br>而对”号，若其前面没有加<code>\</code>，则Shell会将它同前一个”号匹配。</p>
<h1 id="反引号"><a href="#反引号" class="headerlink" title="反引号"></a>反引号</h1><p>这个字符所对应的键一般位于键盘的左上角。<br>反引号括起来的字符串被shell解释为命令行，在执行时，shell首先执行该命令行，并以它的标准输出结果取代整个反引号（包括两个反引号）部分。<br>可用新用法<code>$()</code>代替。</p>
<p>参考自：<a href="https://blog.csdn.net/iamlaosong/article/details/54728393" target="_blank" rel="noopener">https://blog.csdn.net/iamlaosong/article/details/54728393</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>shell单/双引号</tag>
      </tags>
  </entry>
  <entry>
    <title>maven打包java工程</title>
    <url>/2018/06/21/maven%E6%89%93%E5%8C%85java%E5%B7%A5%E7%A8%8B/</url>
    <content><![CDATA[<p>(1)maven package直接打包<br>(2)maven-assembly-plugin<br>(3)maven-shade-plugin<br>这是了解到的三种打包。<br>第一种是直接打包，没有用插件，问题是不会把项目的依赖包打包，作为执行包可能会出错。<br>后面两个是插件。网上说(2)有bug，多个依赖包可能会依赖不同版本的同一个包，这时会把这个包的某一个版本打包，然后就会出错。<br>插件(3)会把这个包的所有版本都打包。（2）的goal有single、help；（3）的goal的goal有shade、help</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--tomcat插件，使用mvn:tomcat7 clean run--&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.tomcat.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>tomcat7-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">port</span>&gt;</span>8080<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">path</span>&gt;</span>/<span class="tag">&lt;/<span class="name">path</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">uriEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">uriEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">      </span><br><span class="line">      <span class="comment">&lt;!--编译插件 maven 2 默认使用jdk1.3来编译，maven 3默认使用jdk1.5来编译--&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- since 2.0 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.7.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- use the Java 8 language features --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- want the compiled classes to be compatible with JVM 1.8 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- The -encoding argument for the Java compiler. --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">      </span><br><span class="line">      <span class="comment">&lt;!--maven-shade-plugin插件--&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">plugin</span>&gt;</span>  </span><br><span class="line">      <span class="comment">&lt;!--都是org.apache.maven.plugins， 可以省略--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span>  </span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">               <span class="comment">&lt;!--将shade插件的shade:shade加入到Maven的package阶段--&gt;</span></span><br><span class="line">               <span class="comment">&lt;!--可以通过命令mvn clean package 也执行shade:shade阶段--&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">               <span class="comment">&lt;!--直接在代码里指定主函数，使用run jar时，不再需要指定主函数--&gt;</span></span><br><span class="line">               <span class="comment">&lt;!--例如，hadoop的run jar执行时只需要 hadoop jar jar包 参数--&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                                        com.mr.wordcount.WordCountJob</span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span>  </span><br><span class="line">      <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">      </span><br><span class="line">      <span class="comment">&lt;!--maven-assembly-plugin插件--&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">index</span>&gt;</span>true<span class="tag">&lt;/<span class="name">index</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.mr.wordcount.WordCountJob<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">      </span><br><span class="line">      <span class="tag">&lt;<span class="name">plugin</span>&gt;</span> </span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span> </span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-resources-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span> </span><br><span class="line">         <span class="tag">&lt;<span class="name">configuration</span>&gt;</span> </span><br><span class="line">            <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span> </span><br><span class="line">            <span class="comment">&lt;!-- 过滤后缀为pem、pfx、p12的证书文件 --&gt;</span> </span><br><span class="line">            <span class="tag">&lt;<span class="name">nonFilteredFileExtensions</span>&gt;</span> </span><br><span class="line">               <span class="tag">&lt;<span class="name">nonFilteredFileExtension</span>&gt;</span>pem<span class="tag">&lt;/<span class="name">nonFilteredFileExtension</span>&gt;</span> </span><br><span class="line">               <span class="tag">&lt;<span class="name">nonFilteredFileExtension</span>&gt;</span>pfx<span class="tag">&lt;/<span class="name">nonFilteredFileExtension</span>&gt;</span> </span><br><span class="line">               <span class="tag">&lt;<span class="name">nonFilteredFileExtension</span>&gt;</span>p12<span class="tag">&lt;/<span class="name">nonFilteredFileExtension</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">nonFilteredFileExtensions</span>&gt;</span> </span><br><span class="line">         <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span> </span><br><span class="line">      <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>maven-compiler-plugin用来编译Java代码，maven-resources-plugin则用来处理资源文件。<br>默认的主资源文件目录是src/main/resources，很多用户会需要添加额外的资源文件目录，这个时候就可以通过配置maven-resources-plugin来实现。此外，资源文件过滤也是Maven的一大特性，你可以在资源文件中使用${propertyName}形式的Maven属性，然后配置maven-resources-plugin开启对资源文件的过滤，之后就可以针对不同环境通过命令行或者Profile传入属性的值，以实现更为灵活的构建。</p>
<p>总结自：<a href="http://chenzhou123520.iteye.com/blog/1706242" target="_blank" rel="noopener">http://chenzhou123520.iteye.com/blog/1706242</a><br>链接：<a href="https://www.jianshu.com/p/ce01bb1615a8" target="_blank" rel="noopener">https://www.jianshu.com/p/ce01bb1615a8</a></p>
<p>在项目下pom.xml的project节点下创建了开发环境和线上环境的profile</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">profiles</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>dev<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">env</span>&gt;</span>dev<span class="tag">&lt;/<span class="name">env</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">activation</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">activeByDefault</span>&gt;</span>true<span class="tag">&lt;/<span class="name">activeByDefault</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">activation</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>prd<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">env</span>&gt;</span>prd<span class="tag">&lt;/<span class="name">env</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profiles</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>其中id代表这个环境的唯一标识.<br>properties下我们我们自己自定义了标签env，内容分别是dev和prd。<br>activeByDefault=true代表如果不指定某个固定id的profile，那么就使用这个环境.<br>参考：<a href="https://www.cnblogs.com/nfcm/p/7550772.html" target="_blank" rel="noopener">https://www.cnblogs.com/nfcm/p/7550772.html</a></p>
]]></content>
      <categories>
        <category>spring</category>
      </categories>
  </entry>
  <entry>
    <title>Mybatis拦截器介绍及分页插件</title>
    <url>/2018/05/31/Mybatis%E6%8B%A6%E6%88%AA%E5%99%A8%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%88%86%E9%A1%B5%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<p>Mybatis提供了一个Interceptor接口，通过实现该接口就可以定义我们自己的拦截器。该接口中一共定义有三个方法，intercept、plugin和setProperties。plugin方法是拦截器用于封装目标对象的，通过该方法我们可以返回目标对象本身，也可以返回一个它的代理。当返回的是代理的时候我们可以对其中的方法进行拦截来调用intercept方法，当然也可以调用其他方法，这点将在后文讲解。setProperties方法是用于在Mybatis配置文件中指定一些属性的。</p>
<p>对于实现自己的Interceptor而言有两个很重要的注解，一个是@Intercepts，其值是一个@Signature数组。<br>@Intercepts用于表明当前的对象是一个Interceptor，而@Signature则表明要拦截的接口、方法以及对应的参数类型。</p>
<p>Plugin的wrap方法，它根据当前的Interceptor上面的注解定义哪些接口需要拦截，然后判断当前目标对象是否有实现对应需要拦截的接口，如果没有则返回目标对象本身，如果有则返回一个代理对象。</p>
<p>插件PageHelper通过实现Interceptor接口实现了物理分页。<br>使用方法：<br>1、mvn</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.github.pagehelper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>pagehelper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.7.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>2、在mybatis的配置文件中写上</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;plugin interceptor="com.data.intercepts.MyIntercept"&gt;&lt;/plugin&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span> <span class="attr">interceptor</span>=<span class="string">"com.github.pagehelper.PageHelper"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"dialect"</span> <span class="attr">value</span>=<span class="string">"mysql"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>参数<code>dialect</code>的值代表连接数据库类型。<br>3、直接使用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Page page = PageHelper.startPage(<span class="number">1</span>, <span class="number">1</span>, <span class="keyword">true</span>);</span><br><span class="line">select ...</span><br></pre></td></tr></table></figure>
<p>PageHelper必须在执行sql前调用，只对第一个执行的sql有效。<br><code>PageHelper.startPage()</code>参数：<br><code>pageNum</code> 页码，与<code>RowBound</code>的<code>offset</code>不同，本参数使用<strong><em>1</em></strong>做为起始值。<br><code>pageSize</code> 每页显示数量<br><code>count</code> 是否进行count查询，为true时同时得到数据库中此条sql的总数。<br><code>reasonable</code> 分页合理化,null时用默认配置<br><code>pageSizeZero</code> 当设置为true的时候，如果pagesize设置为0（或RowBounds的limit=0），就不执行分页，返回全部结果<br>通过看源码可以看到，物理分页还是通过limit实现，<br>count操作是通过<code>select count(1) from origin_sql</code>实现，使用时想要得到总条目数/总页数等使用<code>Page</code>的方法。<br>所以从效率上来说和直接limit应该是一样的，只是代码更简洁，扩展性更好了。<br>还可以拦截RowBound()，但是使用起来非常不方便，只能有limit效果，其他都要搭配Page才能使用，还不如直接用Page。</p>
<p>出错：<br><code>java.lang.NoSuchMethodError: org.apache.ibatis.reflection.MetaObject.forObject(Ljava/lang/Object;Lorg/apache/ibatis/reflection/factory/ObjectFactory;</code><br>原因：mybatis和PageHelper插件的版本不匹配或者PageHelper3.6.3版本的问题，修改PageHelper版本即可。</p>
]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive VS HBase</title>
    <url>/2018/05/28/Hive%20VS%20HBase/</url>
    <content><![CDATA[<p>HBase在Hadoop的生态圈是扮演这一个重要的角色那就是  实时、分布式、高维数据 的数据存储；<br>Hbase和Hive在大数据架构中处在不同位置，Hbase主要解决实时数据查询问题，Hive主要解决数据处理和计算问题，一般是配合使用。<br>HBase是建立在Hadoop文件系统之上的分布式面向列的数据库，是一个数据模型,提供快速随机访问海量结构化数据.<br>HBase当前noSql（非关系型的数据库管理系统）数据库的一种,最常见的应用场景就是采集的网页数据的存储，是key-value型数据库.<br>HBase使用HDFS来持久化存储数据；为了提供行级数据更新查询，还是用了缓存等技术。</p>
<h2 id="VS"><a href="#VS" class="headerlink" title="VS"></a>VS</h2><p>在大数据架构中，Hive和HBase是协作关系，数据流一般如下图：<br>1、通过ETL工具将数据源抽取到HDFS存储；<br>2、通过Hive清洗、处理和计算原始数据；<br>3、HIve清洗处理后的结果，如果是面向海量数据随机查询场景的可存入Hbase<br>4、数据应用从HBase查询数据；<br><img src="1.jpg" alt=""></p>
<p>(1)<br>Hbase： Hadoop database 的简称，也就是<strong><em>基于Hadoop数据库</em></strong>，是一种NoSQL数据库，主要适用于海量明细数据（十亿、百亿）的<strong>随机实时查询</strong>，如日志明细、交易清单、轨迹行为等。<br>Hive：Hive是Hadoop数据仓库，严格来说，<strong><em>不是数据库</em></strong>，主要是让开发人员能够通过SQL来计算和处理HDFS上的结构化数据，适用于<strong>离线的批量数据计算</strong>。<br>(2)<br>Hive中的表是纯逻辑表，只是表的定义等，即表的元数据。Hive本身不存储数据，它完全依赖HDFS和MapReduce。这样就可以将结构化的数据文件映射为为一张数据库表，并提供完整的SQL查询功能，并将SQL语句最终转换为MapReduce任务进行运行。<br>而HBase表是物理表，适合存放非结构化的数据。<br>(3)<br>Hive是基于MapReduce来处理数据,而MapReduce处理数据是基于行的模式；<br>HBase处理数据是基于列的而不是基于行的模式，适合海量数据的随机访问。<br>(4)<br>Hive使用Hadoop来分析处理数据，而Hadoop系统是批处理系统，因此不能保证处理的低迟延问题；<br>而HBase是近实时系统，支持实时查询。</p>
<p>参考：<a href="https://www.zhihu.com/question/21677041" target="_blank" rel="noopener">https://www.zhihu.com/question/21677041</a> , taiwo的回答更适合做web的人了解。</p>
<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><p>类似于Google的BigTable，其分布式计算采用MapReduce，通过MapReduce完成大块的数据加载和全表扫描操作等。<br>文件存储系统采用HDFS，通过Zookeeper来完成状态管理协同服务。不过BigTable只支持一级索引，Hbase不仅支持一级索引，还支持二级索引。<br>在HBase中，表被分割成区域，并由区域服务器提供服务。区域被列族垂直分为“Stores”。Stores被保存在HDFS文件。</p>
<h3 id="HBase组成"><a href="#HBase组成" class="headerlink" title="HBase组成"></a>HBase组成</h3><p><img src="2.jpg" alt=""><br>HBase有三个主要组成部分：客户端，主服务器(HMaster)和区域服务器(HRegion Server)。区域服务器可以按要求添加或删除。<br>主服务器(HMaster):类似与namenode/jobTrackers，用来管理区域服务器。包括，区域被分割后新区域的放置、区域服务器出问题后区域的移动（FailOver，故障转移）等。<br>客户端：只与zk、区域服务器打交道，不会联系主服务器。因此如果主服务器挂了，其他客户端仍可以提供服务。<br>两个Catelog表：Root和Meta表。Root表中存储了Meta表的位置。即Meta表的Region key。Meta表存储了所有Region的位置及每个Region所包含的RowKey的范围。Root表的存储位置记录在zookeeper中，Meta表的存储位置记录在Root表中。<br>Hbase已经无缝集成了HDFS，其中所有的数据最终都会通过DFS客户端API持久化到HDFS中。</p>
<p>当客户端发起一个查询数据的请求后，首先，客户端会先连接上zookeeper集群，获取-Root-表的存放在哪一个HRegionServer上。接着找到对应的HRegionServer后，就能够获取到-Root-表中对应的.Meta.表的位置。最后客户端根据.Meta.表存储的HRegion的位置到相应的HRegionServer中取对应的Hregion中的数据信息。经过一次查询后，访问Catalog表的过程就会被缓存起来，下次客户端就可以直接到相应的HRegion上获取数据。<br><img src="3.jpg" alt=""><br>一个集群中有多个区域服务器(HRegion Server)；一个区域服务器中有一个WAL（write Ahead Log，日志文件，用作数据恢复）和多个区域（HRegion）；一个区域中有Store；一个Store中有一个memStore和0到多个HFile（数据存储的地方）。</p>
<p>参考：<a href="https://blog.csdn.net/jiangtao_st/article/details/19499923" target="_blank" rel="noopener">https://blog.csdn.net/jiangtao_st/article/details/19499923</a></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>== is 区别</title>
    <url>/2018/05/24/==%20is%20%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>== 比较的是数值，自定义对象由eq方法决定； is比较的是 地址。</p>
<h3 id="基础数据"><a href="#基础数据" class="headerlink" title="基础数据"></a>基础数据</h3><p>例1：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">400</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = <span class="number">400</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a == b</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a <span class="keyword">is</span> b</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = <span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = <span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c == d</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c <span class="keyword">is</span> d</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>例1的结果推测， == 比较的是数值； is比较的是 地址。<br>可是例2的结果仿佛推翻这个结论。但其实并没有。<br>造成例2结果的原因是python的垃圾回收。python对数值在【-5， 256】的数建立了一个对象池，所有在这个范围里的相同数指向的都是同一个对象。因此当数值为3时， is的结果也是true。</p>
<h3 id="对自定义对象"><a href="#对自定义对象" class="headerlink" title="对自定义对象"></a>对自定义对象</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line"> </span><br><span class="line">p1 = Person(<span class="string">"lili"</span>, <span class="number">23</span>)</span><br><span class="line">p2 = Person(<span class="string">"lili"</span>, <span class="number">23</span>)</span><br><span class="line"><span class="keyword">print</span> id(p1)</span><br><span class="line"><span class="keyword">print</span> id(p2)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'== '</span>, p1 == p2</span><br><span class="line"><span class="keyword">print</span> <span class="string">'is'</span>, p1 <span class="keyword">is</span> p2</span><br><span class="line">输出：</span><br><span class="line"><span class="number">140352009155664</span></span><br><span class="line"><span class="number">140352008229072</span></span><br><span class="line">==  <span class="literal">False</span></span><br><span class="line"><span class="keyword">is</span> <span class="literal">False</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.name == other.name <span class="keyword">and</span> self.age == other.age</span><br><span class="line"> </span><br><span class="line">p1 = Person(<span class="string">"lili"</span>, <span class="number">23</span>)</span><br><span class="line">p2 = Person(<span class="string">"lili"</span>, <span class="number">23</span>)</span><br><span class="line"><span class="keyword">print</span> id(p1)</span><br><span class="line"><span class="keyword">print</span> id(p2)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'== '</span>, p1 == p2</span><br><span class="line"><span class="keyword">print</span> <span class="string">'is'</span>, p1 <span class="keyword">is</span> p2</span><br><span class="line">输出：</span><br><span class="line"><span class="number">139796538532944</span></span><br><span class="line"><span class="number">139796537606416</span></span><br><span class="line">==  <span class="literal">True</span></span><br><span class="line"><span class="keyword">is</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>结论： == 的结果由对象的<strong>eq</strong>方法的定义决定；is判断内存地址。</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Base64编码原理</title>
    <url>/2018/05/24/Base64%E7%BC%96%E7%A0%81%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>Base64的编码原理就是在有时我们的一些特殊字符无法进行传输，经常可以正确传输都是0<del>9,a</del>z,A<del>Z，+,/，它们正好可以用power（2,6）表示，即6位就可以了。<br><strong><em>所以就是3 × 8 = 4 × 6的运算</em></strong>，我们原本的文本都是一个字节（8位）实现的，Base64就是把3个字节表示的内容转成用6位的0</del>9,a<del>z,A</del>Z，+,/表示。</p>
<p>总结自：<a href="http://blog.csdn.net/xiekuntarena/article/details/53521656" target="_blank" rel="noopener">http://blog.csdn.net/xiekuntarena/article/details/53521656</a></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>base64</tag>
      </tags>
  </entry>
  <entry>
    <title>java、guava常用用法</title>
    <url>/2018/05/23/java%E3%80%81guava%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95/</url>
    <content><![CDATA[<p>StringUtils：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-lang3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>CollectionUtils：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>commons-collections<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-collections<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>1、<code>private static String LINE_SEPETOR = System.getProperty(&quot;line.separator&quot;);</code>   得到当前系统的换行符，防止因为系统不同造成不同结果。</p>
<h2 id="guava"><a href="#guava" class="headerlink" title="guava"></a>guava</h2><p>2、Preconditions 判断参数是否正确/是否为null<br>3、<code>Strings.isNullOrEmpty</code><br>3、<code>Splitter.on(LINE_SEPETOR).trimResults().omitEmptyStrings().withKeyValueSeparator(“=”).split(document);</code><br>     字符串按照“LINE_SEPETOR”分割后，去除每个分割结果前后的空格，去除空的分割结果，再把每个分割结果用“=”分割成map。<br>    （想象场景，想把字符串url的参数解析出来成map）<br>4、<code>Joiner.on(“&amp;”).withKeyValueSeparator(“=”).join(map)</code>  把map的key和value之间用=连接，之后用&amp;把所有的key=value对儿连接起来。（与上面方法相反，把一个map转成一个url的形式。）<br>5、<code>Maps.difference</code> 比较两个map，返回结果通过entriesOnlyOnLeft可以得到第一个map有第二个map没有的结果；entriesOnlyOnRight得到第二个map有，第一个map没有的结果；entriesDiffering得到key一样value不一样的结果。<br>6、ImmutableMap、ImmutableSet不可变。</p>
<h2 id="集合过滤"><a href="#集合过滤" class="headerlink" title="集合过滤"></a>集合过滤</h2><p>1、过滤</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PostPredication</span> <span class="keyword">implements</span> <span class="title">Predicate</span>&lt;<span class="title">RequestLog</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">apply</span><span class="params">(RequestLog input)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(...)&#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">		&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对集合里的每条数据执行apply方法，符和条件的返回true。</p>
<p>2、使用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;RequestLog&gt; logs = FileUtils.loadText(ACCESS_LOG, <span class="keyword">new</span> OneLineProcess());</span><br><span class="line">Collections2.filter(logs, <span class="keyword">new</span> PostPredication());</span><br></pre></td></tr></table></figure>
<p>把集合logs符和条件的返回。<br>还可以不实现Predicate方法，在filter的参数里直接放一个匿名类。</p>
<h2 id="对map过滤"><a href="#对map过滤" class="headerlink" title="对map过滤"></a>对map过滤</h2><p>map可以通过key/value/entry等过滤，下面的例子是对value过滤。使用时用的是匿名类的方法，还可以使用上面的方法新建一个类实现Predicate类。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> Map&lt;City, Province&gt; separateResult = <span class="keyword">new</span> HashMap&lt;City, Province&gt;();</span><br><span class="line">Map resultEntry = Maps.filterValues(separateResult, <span class="keyword">new</span> Predicate&lt;Province&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">apply</span><span class="params">(Province province)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> province.getProvinceId() == provinceId ? <span class="keyword">true</span> : <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<h2 id="读-处理文件，更简洁"><a href="#读-处理文件，更简洁" class="headerlink" title="读/处理文件，更简洁"></a>读/处理文件，更简洁</h2><p>1、对文件每行内容处理、实现LineProcessor。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class OneLineProcess implements LineProcessor&lt;List&lt;RequestLog&gt;&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private List&lt;RequestLog&gt; requestLogs &#x3D; new ArrayList&lt;RequestLog&gt;();</span><br><span class="line"></span><br><span class="line">    public OneLineProcess()&#123;</span><br><span class="line">        。。。</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public boolean processLine(String oneLine)&#123;</span><br><span class="line">        Preconditions.checkNotNull(oneLine);</span><br><span class="line">		。。。。对文件每行内容处理</span><br><span class="line">        return true;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public List&lt;RequestLog&gt; getResult()&#123;</span><br><span class="line">        return requestLogs;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>2.使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">File textFile &#x3D; new File(filePath);</span><br><span class="line">Files.readLines(textFile, Charsets.UTF_8, oneLineProcess)；</span><br></pre></td></tr></table></figure>
<p>第二行代码的返回就是调用第一步中getResult方法的返回结果。</p>
<h2 id="spring拦截器"><a href="#spring拦截器" class="headerlink" title="spring拦截器"></a>spring拦截器</h2><p>1、在mvc配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;mvc:interceptors&gt;</span><br><span class="line">    &lt;mvc:interceptor&gt;</span><br><span class="line">        &lt;mvc:mapping path&#x3D;&quot;&#x2F;**&quot;&#x2F;&gt;</span><br><span class="line">        &lt;ref bean&#x3D;&quot;loginInterceptor&quot;&#x2F;&gt;</span><br><span class="line">    &lt;&#x2F;mvc:interceptor&gt;</span><br><span class="line">&lt;&#x2F;mvc:interceptors&gt;</span><br></pre></td></tr></table></figure>
<p>标签<code>mvc:mapping</code>的参数<code>path</code>的值是要拦截的请求</p>
<p>2、代码实现loginInterceptor</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@Component</span><br><span class="line">public class LoginInterceptor extends HandlerInterceptorAdapter&#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123;</span><br><span class="line">        Cookie[] cookies &#x3D; request.getCookies();</span><br><span class="line">        for(Cookie cookie: cookies)&#123;</span><br><span class="line">            if(。。。)&#123;</span><br><span class="line">                return true;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        response.sendRedirect(&quot;&#x2F;tologin&quot;);</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="spring切片"><a href="#spring切片" class="headerlink" title="spring切片"></a>spring切片</h2><p>1、spring配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;aop:aspectj-autoproxy proxy-target-class&#x3D;&quot;true&quot;&#x2F;&gt;</span><br></pre></td></tr></table></figure>
<p>2、切片类</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@Aspect</span><br><span class="line">@Component</span><br><span class="line">public class DaoAop &#123;</span><br><span class="line"></span><br><span class="line">    @Pointcut(&quot;execution(* com.fresh.dao.*.*(..))&quot;)</span><br><span class="line">    public void daoPoint()&#123;&#125;</span><br><span class="line"></span><br><span class="line">    @Around(&quot;daoPoint()&quot;)</span><br><span class="line">    public Object timingDao(ProceedingJoinPoint proceedingJoinPoint)&#123;</span><br><span class="line">        Object result &#x3D; null;</span><br><span class="line">        Stopwatch stopwatch &#x3D; Stopwatch.createStarted();</span><br><span class="line">        try &#123;</span><br><span class="line">            result &#x3D; proceedingJoinPoint.proceed();</span><br><span class="line">            long elapsed &#x3D; stopwatch.elapsed(TimeUnit.MILLISECONDS);</span><br><span class="line">            log.info(&quot;dao层&#123;&#125;执行耗时: &#123;&#125;&quot;, proceedingJoinPoint.getSignature(), elapsed);</span><br><span class="line">        &#125; catch (Throwable throwable) &#123;</span><br><span class="line">            log.error(&quot;aop记录，dao层执行时出错: &#123;&#125;&quot;, throwable);</span><br><span class="line">        &#125;</span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>com.fresh.dao.*.*(..)  是包.类.方法(参数),上面的例子是统计dao层方法的执行时间。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>guava</tag>
        <tag>util</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu更新Git至最新版本</title>
    <url>/2018/05/11/Ubuntu%20%E6%9B%B4%E6%96%B0%20Git%20%E8%87%B3%E6%9C%80%E6%96%B0%E7%89%88%E6%9C%AC/</url>
    <content><![CDATA[<p>linux给git升级<br>直接<code>sudo apt-get install git</code><br>上面的命令没有反应，这是因为 Ubuntu 自带的源中，Git 版本就是这么低，能怎么办。<br>所以需要加入一个源，带有最新 Git 版本的源，步骤如下：<br>1，添加源：<br><code>sudo add-apt-repository ppa:git-core/ppa</code><br>2，更新安装列表：<br><code>sudo apt-get update</code><br>3，升级 Git：<br><code>sudo apt-get install git</code><br>一气呵成，完成 Git 升级。</p>
<p>最后，可以执行<code>git --version</code>验证是否升级到最新版本。</p>
<p>转载自：<br>作者：诸葛_瓜皮<br>来源：CSDN<br>原文：<a href="https://blog.csdn.net/Ezreal_King/article/details/79999131" target="_blank" rel="noopener">https://blog.csdn.net/Ezreal_King/article/details/79999131</a> </p>
]]></content>
      <categories>
        <category>开发常用工具</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>git常用</title>
    <url>/2018/05/11/git/</url>
    <content><![CDATA[<p>场景1：关联远程并提交<br><code>git init</code> 初始化本地<br><code>git remote add origin url</code> 本地仓库关联远程仓库<br><code>git checkout -b master</code>  创建分支master并切换到master分支<br><code>git add</code>  添加<br><code>git commit</code> 添加<br><code>git push -u origin master</code>  推送本地到远程<br>场景2：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout – file。<br>场景3：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，可以分两步，第一步用命令git reset HEAD file，就回到了场景1，第二步按场景1操作。<br>场景4：本地在cl分支开发，现在要合并master的内容使cl分支有最新的master代码。<br>      <code>git checkout master</code>切换分支<br>      <code>git pull</code>拉取并合并master分支最新代码<br>      <code>git checkout cl</code>切换分支<br>      <code>git merge master</code>更新最新master代码到cl分支</p>
<p>出错1：<code>fatal: 拒绝合并无关的历史</code><br>解决：首先将远程仓库和本地仓库关联起来：<br><code>git branch --set-upstream-to=origin/master master</code><br>然后使用git pull整合远程仓库和本地仓库，<br><code>git pull --allow-unrelated-histories    (忽略版本不同造成的影响)</code></p>
<p>出错2：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">To XXX</span><br><span class="line">! [rejected] master -&gt; master (non-fast-forward)</span><br><span class="line">error: 无法推送一些引用到 &#39;git@XXX.git&#39;</span><br><span class="line">提示：更新被拒绝，因为您当前分支的最新提交落后于其对应的远程分支。</span><br></pre></td></tr></table></figure>
<p>解决：本地当前的分支和远程的对应分支不对应。远程分支文件多于本地文件，无法合并。需要手动拉取并合并，之后就可以正常push了。<br><code>git fetch origin</code><br><code>git merge origin/master（换成你要推送的分支）</code> </p>
<p>出错3：<br><code>The RSA host key for github.com has changed, and the key for the corresponding IP address ××.××.×××.××</code><br>解决：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Step 1: Go to Folder or use command+shift+g</span><br><span class="line">Step 2: type &quot;~&#x2F;.ssh&#x2F;&quot;</span><br><span class="line">Step 3: Open &quot;known_hosts&quot; file and Remove all the content</span><br><span class="line">Step 4: Now Open terminal and pull from another branch, It will ask for password give your system password.</span><br></pre></td></tr></table></figure>

<p>1、修改后放在了修改区；add之后是暂存区；commit是本地仓库。<br>暂存区是存放下次将要提交的图表信息，索引。本地仓库存放原数据，对象数据库。<br>2、<br><code>git branch</code> <strong>查看所有分支</strong><br><code>git branch</code> 分支名 <strong>创建分支</strong><br><code>git checkout</code> 分支名 <strong>切换分</strong>支<br><code>git checkout -b newbranch</code> <strong>创建并切换分支</strong><br><code>git branch -d</code> 删除分支<br><code>git branch -v</code> 查看分支详细信息（分支名及最后一个提交等）<br><code>git checkout -- 文件名</code> 将本地文件返回到本地仓库的状态（一般在你push时，又不想提交本地某些文件修改的时候使用，移出来后与远程merge后本地被移出来的文件修改部分会被远程覆盖！    ）<br><code>git checkout 版本号 文件名</code> 返回到文件的某版本（git log 查看所有版本）<br><code>git checkout HEAD～num</code> 返回到前num次提交<br><code>git remote -v</code> 查看远程仓库地址命令</p>
<p><code>git merge --abort</code><strong>取消本次合并</strong><br><code>git fetch</code> <strong>拉取远程分支</strong><br><code>git merge origin master</code> <strong>合并分支</strong><br><code>git pull=git fetch+git merge</code> <strong>拉取并合并</strong><br>&lt;&lt;&lt;&lt; 自己的 ==== 别人的 &gt;&gt;&gt;&gt;</p>
<p>git status<br>git add  <strong>暂存区</strong><br>git commit -m ‘描述’ <strong>本地仓库</strong><br>git push -u origin branch名字 <strong>推送到远程</strong>（第一次推送使用 -u，意思是推送远程并关联远程分支）</p>
<p>git reset HEAD 文件 将文件从暂存区撤回<br>git reset –mixed HEAD~ 将本地仓库的内容复制到暂存区<br>git reset –hard HEAD~ 将本地仓库的内容复制到暂存区和本地工作区（覆盖不可逆，要小心）<br>git reset –soft HEAD~ 将本地仓库的head指针指向前一次提交</p>
<p>git revert 版本号 撤销某版本的提交（与reset类似，但是reset会将某版本之后的提交全部抹去）</p>
<p>git cherry-pick 版本号 将某次提交内容合并到当前分支（只想合并某次提交的内容，其他次提交的内容不提交）<br>git cherry-pick –continue 继续提交</p>
<p>git rebase 把一个分支的修改合并到当前分支</p>
<p>git diff 工作区、暂存区之间的差别<br>git diff –cached/staged 暂存区、本地仓库差别<br>git diff 两个分支/两次提交码 比较他们的区别</p>
<p>ssh-keygen 生成key<br>cat ~/.ssh/id_rsa.publs</p>
<p>vim .gitignore修改不想添加到git中的文件status不在提示</p>
<p>git init创建新的/重新初始化<br>git clone<br>git remote add origin url在本地关联远程仓库<br>git remote -v 等<br>./git就是本地仓库<br>git log 提交日志，本地远程都有<br>git log –graph –oneline<br>git reflog 所有日志，只在本地</p>
<p>git config 添加用户等<br>git config –add user.name/user.email<br>git config -e 看用户</p>
]]></content>
      <categories>
        <category>开发常用工具</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>数据术语</title>
    <url>/2018/05/11/%E6%95%B0%E6%8D%AE%E6%9C%AF%E8%AF%AD/</url>
    <content><![CDATA[<h3 id="DAU和UV的区别"><a href="#DAU和UV的区别" class="headerlink" title="DAU和UV的区别"></a>DAU和UV的区别</h3><p>DAU(Daily Active User)：是日活跃用户数，通过用户ID排重统计数据；<br>UV(Unique Visitor)：是独立访客。通过用户IP排重统计数据；<br>一个独立访问的用户，如果不登录，就没有办法转化为一个日活跃用户；抛出特定业务来说， 这两个就是一样的东西。 只是DAU用在移动端APP中， UV用在Web当中。</p>
<h3 id="全量-增量-快照表"><a href="#全量-增量-快照表" class="headerlink" title="全量/增量/快照表"></a>全量/增量/快照表</h3><table>
<thead>
<tr>
<th align="left"></th>
<th align="left">全量表</th>
<th align="left">增量表</th>
<th align="left">快照表</th>
</tr>
</thead>
<tbody><tr>
<td align="left">数据</td>
<td align="left">包含到前一天的全量数据</td>
<td align="left">前一天的增量数据</td>
<td align="left">包含到前一天的全量数据</td>
</tr>
<tr>
<td align="left">分区</td>
<td align="left">不分区（ymd为当前日期）</td>
<td align="left">按照每一天分区</td>
<td align="left">按照每一天分区</td>
</tr>
</tbody></table>
<p>全量表：没有分区。表中数据是包含前一天到之前的所有数据。<br>增量表：按天分区。对应分区存对应天的数据。<br>快照表：按天分区。每个分区是包含前一天到之前的所有数据。</p>
<h3 id="T-1"><a href="#T-1" class="headerlink" title="T+1"></a>T+1</h3><p><code>T+1</code> T日就是工作日（除了周六、日，或者是法定节假日）。 如果是周五、周六、周日，那么T+1日就是周一。 T+1就是次工作日了<br><code>D+1</code> 就是不分交易日还是节假日的第二天；<br><code>S+1</code> 就是第二秒。</p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>数据术语</tag>
      </tags>
  </entry>
  <entry>
    <title>python annotate练习画决策树</title>
    <url>/2018/05/04/python%20annotate%E7%BB%83%E4%B9%A0%E7%94%BB%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<h1 id="annotate-画箭头例子"><a href="#annotate-画箭头例子" class="headerlink" title="annotate 画箭头例子"></a>annotate 画箭头例子</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># __author__='chenliclchen'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_node</span><span class="params">(node_text, parent_pt, center_parent, nodetype)</span>:</span></span><br><span class="line">    <span class="comment"># s 标签； xy 注释的位置（就是箭头的起始位置）；xytext 标签的位置；</span></span><br><span class="line">    <span class="comment"># xycoords 注释位置（xy）坐标的类型；textcoords 标签位置(xytext)坐标的类型；arrowprops 字典，箭头的属性</span></span><br><span class="line">    <span class="comment"># https: // matplotlib.org / users / annotations.html</span></span><br><span class="line">    ax.annotate(node_text, xy=parent_pt, xytext=center_parent,</span><br><span class="line">                xycoords=<span class="string">'axes fraction'</span>, textcoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">                va=<span class="string">'center'</span>, ha=<span class="string">'center'</span>, bbox=nodetype, arrowprops=arrow_args)</span><br><span class="line"></span><br><span class="line">tree = &#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;&#125;&#125;</span><br><span class="line">decisionNode = dict(boxstyle=<span class="string">"sawtooth"</span>, fc=<span class="string">"0.8"</span>)</span><br><span class="line">leafNode = dict(boxstyle=<span class="string">"round4"</span>, fc=<span class="string">"0.8"</span>)</span><br><span class="line">arrow_args = dict(arrowstyle=<span class="string">"&lt;-"</span>)</span><br><span class="line"><span class="comment"># 1(num)是作为一个id，如果这个id已经存在就激活返回引用否则就创建返回引用</span></span><br><span class="line"><span class="comment"># figsize 元组，图片大小，宽高; dpi 图片分辨率； facecolor 是背景色; edgecolor 边框色；</span></span><br><span class="line">fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'w'</span>)</span><br><span class="line">fig.clf()</span><br><span class="line"><span class="comment"># frameon=False 是否显示坐标轴架构</span></span><br><span class="line">ax = plt.subplot(<span class="number">111</span>, frameon=<span class="literal">False</span>)</span><br><span class="line">plot_node(<span class="string">'de'</span>, (<span class="number">0.1</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.1</span>), decisionNode)</span><br><span class="line">plot_node(<span class="string">'leaf'</span>, (<span class="number">0.3</span>, <span class="number">0.8</span>), (<span class="number">0.8</span>, <span class="number">0.1</span>), leafNode)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>效果：<img src="1.png" alt=""></p>
<h1 id="annotate-画决策树"><a href="#annotate-画决策树" class="headerlink" title="annotate 画决策树"></a>annotate 画决策树</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># __author__='chenliclchen'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画每个节点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_node</span><span class="params">(node_text, parent_pt, center_parent, nodetype)</span>:</span></span><br><span class="line">    <span class="comment"># s 标签； xy 注释的位置（就是箭头的起始位置）；xytext 标签的位置；</span></span><br><span class="line">    <span class="comment"># xycoords 注释位置（xy）坐标的类型； textcoords 标签位置(xytext)坐标的类型； arrowprops 字典，箭头的属性</span></span><br><span class="line">    ax.annotate(node_text, xy=parent_pt, xytext=center_parent,</span><br><span class="line">                xycoords=<span class="string">'axes fraction'</span>, textcoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">                va=<span class="string">'center'</span>, ha=<span class="string">'center'</span>, bbox=nodetype, arrowprops=arrow_args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画箭头上的内容，也就是每个节点的选择值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_text</span><span class="params">(parent_pt, center_parent, text)</span>:</span></span><br><span class="line">    x = (parent_pt[<span class="number">0</span>] - center_parent[<span class="number">0</span>]) / <span class="number">2</span> + center_parent[<span class="number">0</span>]</span><br><span class="line">    y = (parent_pt[<span class="number">1</span>] - center_parent[<span class="number">1</span>]) / <span class="number">2</span> + center_parent[<span class="number">1</span>]</span><br><span class="line">    ax.text(x, y, text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算树的深度，为了画树的y坐标做准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tree_deep</span><span class="params">(tree)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(tree, str):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> isinstance(tree, dict):</span><br><span class="line">        key = tree.keys()[<span class="number">0</span>]</span><br><span class="line">        value = tree[key]</span><br><span class="line">        deep = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> value.keys():</span><br><span class="line">            this_deep = <span class="number">1</span> + get_tree_deep(value[item])</span><br><span class="line">            deep = this_deep <span class="keyword">if</span> this_deep &gt; deep <span class="keyword">else</span> deep</span><br><span class="line">        <span class="keyword">return</span> deep</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算叶子节点，为画树时的x坐标准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_leafnode_num</span><span class="params">(tree, leafnode_num)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(tree, str):</span><br><span class="line">        leafnode_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> leafnode_num</span><br><span class="line">    <span class="keyword">elif</span> isinstance(tree, dict):</span><br><span class="line">        key = tree.keys()[<span class="number">0</span>]</span><br><span class="line">        value = tree[key]</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> value.keys():</span><br><span class="line">            leafnode_num = get_leafnode_num(value[item], leafnode_num)</span><br><span class="line">        <span class="keyword">return</span> leafnode_num</span><br><span class="line"></span><br><span class="line"><span class="comment"># node_text是节点里的内容； parent_pt是箭头出发的位置； center_parent箭头结束的位置 ；</span></span><br><span class="line"><span class="comment"># key是箭头上的标示，也是决策树的选择值； had_draw_leafnode 已经画过的叶子节点，为了计算下一个叶子节点的x坐标</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_tree</span><span class="params">(node_text, parent_pt, center_parent, key, had_draw_leafnode)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(node_text, str):</span><br><span class="line">        plot_text(parent_pt, center_parent, key)</span><br><span class="line">        plot_node(node_text, parent_pt, center_parent, decisionNode)</span><br><span class="line">        had_draw_leafnode += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> had_draw_leafnode</span><br><span class="line">    <span class="keyword">elif</span> isinstance(node_text, dict):</span><br><span class="line">        next_key = node_text.keys()[<span class="number">0</span>]</span><br><span class="line">        value = node_text[next_key]</span><br><span class="line">        plot_text(parent_pt, center_parent, key)</span><br><span class="line">        plot_node(next_key, parent_pt, center_parent, leafNode)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> value.keys():</span><br><span class="line">            this_leafnum_node = get_leafnode_num(value[item], <span class="number">0</span>)</span><br><span class="line">            next_xind = had_draw_leafnode * x_off + (this_leafnum_node - <span class="number">1</span>) * x_off / <span class="number">2</span></span><br><span class="line">            had_draw_leafnode = draw_tree(value[item], center_parent, (next_xind, center_parent[<span class="number">1</span>]-y_off), item, had_draw_leafnode)</span><br><span class="line">        <span class="keyword">return</span> had_draw_leafnode</span><br><span class="line"></span><br><span class="line"><span class="comment"># tree = &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;</span></span><br><span class="line"><span class="comment"># tree = &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: &#123;'head': &#123;0: 'no', 1: 'yes'&#125;&#125;, 1: 'no'&#125;&#125;&#125;&#125;</span></span><br><span class="line"><span class="comment"># tree = &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;, 3: 'maybe'&#125;&#125;</span></span><br><span class="line">tree = &#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: &#123;<span class="string">'head'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;, <span class="number">1</span>: <span class="string">'no'</span>&#125;&#125;, <span class="number">3</span>: <span class="string">'maybe'</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line">decisionNode = dict(boxstyle=<span class="string">"sawtooth"</span>, fc=<span class="string">"0.8"</span>)</span><br><span class="line">leafNode = dict(boxstyle=<span class="string">"round4"</span>, fc=<span class="string">"0.8"</span>)</span><br><span class="line">arrow_args = dict(arrowstyle=<span class="string">"&lt;-"</span>)</span><br><span class="line"><span class="comment"># 1(num)是作为一个id，如果这个id已经存在就激活返回引用否则就创建返回引用</span></span><br><span class="line"><span class="comment"># figsize 元组，图片大小，宽高; dpi 图片分辨率； facecolor 是背景色; edgecolor 边框色；</span></span><br><span class="line">fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'w'</span>)</span><br><span class="line">fig.clf()</span><br><span class="line"><span class="comment"># 不显示坐标数据</span></span><br><span class="line">axprops = dict(xticks=[], yticks=[])</span><br><span class="line"><span class="comment"># frameon=False 是否显示坐标轴架构</span></span><br><span class="line">ax = plt.subplot(<span class="number">111</span>, frameon=<span class="literal">False</span>, **axprops)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了计算画每个节点时xy坐标需要移动的大小</span></span><br><span class="line">tree_deep = get_tree_deep(tree)</span><br><span class="line">leafnum_node = get_leafnode_num(tree, <span class="number">0</span>)</span><br><span class="line">x_off = <span class="number">1.0</span> / (leafnum_node - <span class="number">1</span>)</span><br><span class="line">y_off = <span class="number">1.0</span> / (tree_deep - <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 画树并展示</span></span><br><span class="line">draw_tree(tree, (<span class="number">0.5</span>, <span class="number">1.0</span>), (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">''</span>, <span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>效果：<img src="2.png" alt=""><br>ax.annotate方法的xy（箭头起始位置）和xytext（箭头结束位置）一样时，箭头会消失，只画方格里的内容，用此原理画根节点。</p>
<p>以上代码最麻烦的就是如何画每个节点的x坐标。<br>我最开始的计划是每个节点的子节点以当前节点左右分开，因此使用当前节点的x坐标通过子节点的下标加减得到子节点的x坐标。但是效果不太好，而且仔细思考有很多的问题，比如说当前层的其他节点的子节点可能会和当前节点的子节点重合到一起等等一堆的问题。<br>我后来苦思冥想啊，上厕所时终于想到一个极好的办法。简单描述就是：用叶子节点推断当前节点的x坐标的方法。由下往上推导的方法。<br>具体：<br>（1）所有叶子节点在x轴均匀分布，因此算出两个叶子节点的x轴距离x_off。<br>（2）假设当前节点有四个叶子节点，然后当前节点的x坐标肯定在这四个叶子节点的中间啊，因此可以算出当前节点在四个叶子节点的x坐标是<code>(this_leafnum - 1) * x_off / 2</code><br>（3）这时我们只需知道当前节点的四个叶子节点的最左边叶子节点的x轴刻度，然后加上步骤（2）的结果就可以得到当前节点的x轴的刻度。这时我们只需知道我们已经画了多少个叶子节点（因为我们是从左往右画的），已经画的叶子节点数乘以x_off就得到四个叶子节点最左边叶子节点的x轴刻度。<br>因此代码<code>had_draw_leafnode * x_off + (this_leafnum - 1) * x_off / 2</code>得到x轴坐标。</p>
<h1 id="代码下载地址"><a href="#代码下载地址" class="headerlink" title="代码下载地址"></a>代码下载地址</h1><p><a href="https://github.com/meihuakaile/decision_tree" target="_blank" rel="noopener">代码地址</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>join on和 where 条件放置区别</title>
    <url>/2018/05/03/join%20on%E5%92%8C%20where%20%E6%9D%A1%E4%BB%B6%E6%94%BE%E7%BD%AE%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>执行顺序：先on条件再 where条件筛选。<br>on筛选之后会生成一个临时表；where在临时表上再进行筛选。<br>（inner）join on时和where的效果一样。<br>left/right on时与where效果不同。两表都不为空时，显而易见left/right on时总会有结果出来，where可能会导致结果为空。<br>本应放在on的条件放在了where时会失去left/right操作，效果会等同于（inner）join on</p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>有两个表：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from user;</span><br><span class="line">+----+--------+--------+</span><br><span class="line">| id | name   | pwd    |</span><br><span class="line">+----+--------+--------+</span><br><span class="line">|  1 | lilili | 123456 |</span><br><span class="line">+----+--------+--------+</span><br><span class="line">1 row in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from hotel;</span><br><span class="line">+----+-------+-------+</span><br><span class="line">| id | level | name  |</span><br><span class="line">+----+-------+-------+</span><br><span class="line">|  1 |     2 | test1 |</span><br><span class="line">|  2 |     2 | test2 |</span><br><span class="line">|  3 |     2 | test3 |</span><br><span class="line">|  4 |     1 | test3 |</span><br><span class="line">|  5 |     2 | test2 |</span><br><span class="line">+----+-------+-------+</span><br><span class="line">5 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>
<p><code>select * from user u left join hotel h on u.id=h.id where h.level=1;</code>和<br><code>select * from user u left join hotel h on u.id=h.id and h.level=1;</code>  区别<br>下面例子有三条命令。明显看到第二条命令在第一个上加了一个where条件导致输出为空。<br>但是第三条命令把第二条多加的where加到了on中判断，使得右表没有对应的数据全部以null填充。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from user u left join hotel h on u.id&#x3D;h.id;</span><br><span class="line">+----+--------+--------+----+-------+-------+</span><br><span class="line">| id | name   | pwd    | id | level | name  |</span><br><span class="line">+----+--------+--------+----+-------+-------+</span><br><span class="line">|  1 | lilili | 123456 |  1 |     2 | test1 |</span><br><span class="line">+----+--------+--------+----+-------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from user u left join hotel h on u.id&#x3D;h.id where h.level&#x3D;1;</span><br><span class="line">Empty set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from user u left join hotel h on u.id&#x3D;h.id and h.level&#x3D;1;</span><br><span class="line">+----+--------+--------+------+-------+------+</span><br><span class="line">| id | name   | pwd    | id   | level | name |</span><br><span class="line">+----+--------+--------+------+-------+------+</span><br><span class="line">|  1 | lilili | 123456 | NULL |  NULL | NULL |</span><br><span class="line">+----+--------+--------+------+-------+------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>如果需要不满足连接条件的行也在我们的查询范围内的话，我们就必需把连接条件放在ON后面，而不能放在WHERE后面，如果我们把连接条件放在了WHERE后面，那么所有的LEFT,RIGHT,等这些操作将不起任何作用，对于这种情况，它的效果就完全等同于INNER连接。<br>所有的连接条件都必需要放在ON后面，不然前面的所有LEFT,和RIGHT关联将作为摆设，而不起任何作用.</p>
<p>参考：<a href="https://blog.csdn.net/muxiaoshan/article/details/7617533" target="_blank" rel="noopener">https://blog.csdn.net/muxiaoshan/article/details/7617533</a></p>
]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2018/04/27/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<p>优点:计算复杂度不高,输出结果易于理解,对中间值的缺失不敏感,可以处理不相关特征数据。<br>缺点:可能会产生过度匹配问题。<br>适用数据类型:数值型和标称型。</p>
<h1 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h1><p>本文讲解的算法基于ID3。它是一个好的算法但并不完美。ID3算法无法直接处理数值型数据,尽管我们可以通过量化的方法将数值型数据转化为标称型数值,但是如果存在太多的特征划分,ID3算法仍然会面临其他问题。</p>
<h1 id="过程概述"><a href="#过程概述" class="headerlink" title="过程概述"></a>过程概述</h1><p>分类算法除了需要测量信息熵,还需要划分数据集,度量划分数据集的熵,以便判断当前是否正确地划分了数据集。<br>对每个特征划分数据集的结果计算一次信息熵,然后判断按照哪个特征划分数据集是最好的划分方式。</p>
<p>评估每个特征。完成评估之后,<strong><em>原始数据集就被划分为几个数据子集</em></strong>。评估特征的方法使用信息增益，信息增益越高越好，信息增益的讲解看下面。<br>这些数据子集会分布在第一个决策点的所有分支上。<br>如果某个分支下的数据属于同一类型,则当前分支已经正确地划分数据分类,无需进一步对数据集进行分割。<br>如果某个分支下的数据子集内的数据不属于同一类型,则需要重复划分数据子集的过程。<br>如何划分数据子集的算法和划分原始数据集的方法相同,直到所有具有相同类型的数据均在一个数据子集内。</p>
<h1 id="度量方法—熵-条件熵-信息增益"><a href="#度量方法—熵-条件熵-信息增益" class="headerlink" title="度量方法—熵/条件熵/信息增益"></a>度量方法—熵/条件熵/信息增益</h1><p>划分数据集的大原则是:将无序的数据变得更加有序。<br>信息熵:集合信息的度量方式称为香农熵，或者简称为熵。<br>      信息的期望值。信息熵越高,则混合的数据也越多,纯度越低。<br>信息增益：在划分数据集之前之后信息发生的变化称为信息增益。每个特征在划分数据集时获得的信息收益越高越好。<br>下面公式第一个是信息； 第二个是信息熵；第三个是条件熵<br><img src="1.png" alt=""><br><img src="2.png" alt=""><br><img src="3.jpg" alt=""></p>
<center>信息增益 = 信息熵 - 条件熵</center>

<p><strong><em>以下来自<a href="https://www.zhihu.com/question/22104055" target="_blank" rel="noopener">知乎</a>的回答：</em></strong><br><strong>熵</strong>：表示随机变量的不确定性。<br><strong>条件熵</strong>：在一个条件下，随机变量的不确定性。<br><strong>信息增益</strong>：熵 - 条件熵在一个条件下，信息不确定性减少的程度！<br>通俗地讲，X(明天下雨)是一个随机变量，X的熵可以算出来， Y(明天阴天)也是随机变量，在阴天情况下下雨的信息熵我们如果也知道的话（此处需要知道其联合概率分布或是通过数据估计）即是条件熵。两者相减就是信息增益！原来明天下雨例如信息熵是2，条件熵是0.01（因为如果是阴天就下雨的概率很大，信息就少了），这样相减后为1.99，在获得阴天这个信息后，下雨信息不确定性减少了1.99！是很多的！所以信息增益大！也就是说，阴天这个信息对下雨来说是很重要的！</p>
<h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>来自<a href="https://www.zhihu.com/question/22104055" target="_blank" rel="noopener">知乎</a>的例子：<br>数据：<img src="4.jpg" alt=""><br>可以求得随机变量X（嫁与不嫁）的信息熵为：嫁的个数为6个，占1/2，<br>那么信息熵为-1/2log1/2-1/2log1/2 = -log1/2=0.301<br>现在假如我知道了一个男生的身高信息。<br>身高有三个可能的取值{矮，中，高}<br>矮包括{1,2,3,5,6,11,12}，嫁的个数为1个，不嫁的个数为6个<br>中包括{8,9} ，嫁的个数为2个，不嫁的个数为0个<br>高包括{4,7,10}，嫁的个数为3个，不嫁的个数为0个<br>先回忆一下条件熵的公式如下：<br><img src="3.jpg" alt=""><br>我们先求出公式对应的:<br>H(Y|X = 矮) = -1/7log1/7-6/7log6/7=0.178<br>H(Y|X=中) = -1log1-0 = 0<br>H(Y|X=高） = -1log1-0=0<br>p(X = 矮) = 7/12,p(X =中) = 2/12,p(X=高) = 3/12<br>则可以得出条件熵为：7/12<em>0.178+2/12</em>0+3/12*0 = 0.103<br>那么我们知道信息熵与条件熵相减就是我们的信息增益，为0.301-0.103=0.198所以我们可以得出我们在知道了身高这个信息之后，信息增益是0.198<br>结论:<br>我们可以知道，本来如果我对一个男生什么都不知道的话，作为他的女朋友决定是否嫁给他的不确定性有0.301这么大。当我们知道男朋友的身高信息后，不确定度减少了0.198.<br>也就是说，身高这个特征对于我们广大女生同学来说，决定嫁不嫁给自己的男朋友是很重要的。</p>
<h1 id="另一个度量方法—基尼不纯度"><a href="#另一个度量方法—基尼不纯度" class="headerlink" title="另一个度量方法—基尼不纯度"></a>另一个度量方法—基尼不纯度</h1><p>另一个度量集合无序程度的方法是基尼不纯度(Gini impurity),简单地说就是从一个数据集中随机选取子项,度量其被错误分类到其他分组里的概率。</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="">整体代码下载地址</a><br>决策树的构建使用递归，递归的停止有两种条件：<br>（1）数据集的类别只有一种。<br>（2）特征已经用完，但是数据集还没有完。此时以剩下数据集出现次数最多的类别作为剩下数据集的类别。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">计算增益最好的特征i作为当前节点</span><br><span class="line">把数据集包含特征i的列去除，得到新的数据集</span><br><span class="line">对特征i的所有不同values：</span><br><span class="line">    把原来数据集中特征i的值等于当前value的行抽出来组成新的数据集</span><br><span class="line">    对新的数据集重复所有操作（在此处以当前value作为‘特征i的j值’，返回值作为它的value）</span><br></pre></td></tr></table></figure>
<p>决策树最后构建的效果<code>{特征i：{特征i的j1值：{...}, 特征值i的j2值：{}}}</code><br>例如<code>{&#39;no surfacing&#39;: {0: &#39;no&#39;, 1: {&#39;flippers&#39;: {0: &#39;no&#39;, 1: &#39;yes&#39;}}}}</code></p>
<p>[画决策树](/2018/05/04/python annotate练习画决策树)</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>KNN</title>
    <url>/2018/04/26/KNN/</url>
    <content><![CDATA[<p>优点:精度高、对异常值不敏感、无数据输入假定。<br>缺点:计算复杂度高、空间复杂度高。<br>适用数据范围:数值型和标称型。</p>
<h1 id="k近邻算法"><a href="#k近邻算法" class="headerlink" title="k近邻算法"></a>k近邻算法</h1><p>存在一个样本数据集合,也称作训练样本集,并且样本集中每个数据<br>都存在标签,即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后,<br>将新数据的每个特征与样本集中数据对应的特征进行比较,然后算法提取样本集中特征最相似数<br>据(最近邻)的分类标签。一般来说,我们只选择样本数据集中前k个最相似的数据,这就是k-近<br>邻算法中k的出处,通常k是不大于20的整数。最后,选择k个最相似数据中出现次数最多的分<br>类,作为新数据的分类。</p>
<h1 id="优化：归一化"><a href="#优化：归一化" class="headerlink" title="优化：归一化"></a>优化：归一化</h1><p><img src="1.png" alt="">是knn的计算距离的公式，从公式可以清楚的看到公式中数据差越大对计算结果的影响越大。<br>但其实我们期望所有特征是同等重要的，属性的权重都是一样的。<del>（之前一直以为归一化只是为了简化计算而存在的）</del><br>因此对数据进行归一化处理。在使用机器学习实战的约会网站例子时，归一化后错误率降低0.13+<br>在处理这种不同取值范围的特征值时,我们通常采用的方法是将数值归一化,如将取值范围处理为0到1或者-1到1之间。<br>公式<img src="2.png" alt="">将数据归一到0-1之间。其中 min 和 max 分别是数据集中的最小特征值和最大特征值。</p>
<h1 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h1><p>如果训练数据集的很大,必须使用大量的存储空间。<br>由于必须对数据集中的每个数据计算距离值,实际使用时可能非常耗时。<br>？？？<br>K近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息,因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/meihuakaile/machine/blob/master/knn.py" target="_blank" rel="noopener">代码下载地址</a><br>包括mnist、约会、病马等数据集、分类代码、展示数据分布代码<br><img src="3.png" alt=""></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>svm理论及代码</title>
    <url>/2018/04/25/svm%E7%90%86%E8%AE%BA%E5%8F%8A%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<p>本文将大量参考书《机器学习实战》，包括代码部分，因此更像是这本书svm章节的读书笔记。</p>
<p>优点:泛化错误率低,计算开销不大,结果易解释。<br>缺点:对参数调节和核函数的选择敏感,原始分类器不加修改仅适用于处理二类问题。<br>适用数据类型:数值型和标称型数据。</p>
<p>可以线性分类，也可以非线性分类。<br>找到一个超平面可以把数据很好的分类，使超平面两边数据到超平面的距离和最大。因此超平面(hyperplane)就是分类的决策边界。<br>比较简单的是可以用一条线分割，就是线性分割。<br>但是更多时候有（1）特殊情况。有个别比较特殊的样本的数据位置不对劲，比如下图，有两个特殊的圆圈。这种情况，要增加惩罚因子。</p>
<p><img src="1.png" alt=""></p>
<p>（2）还有更特殊的，完全不能使用线性分割，需要使用核函数向高维空间映射之后再分类。<br>（来自网络的步骤：<br>1：利用一个非线性的映射把原数据集中的向量点转化到一个更高维的空间中<br>2：在这个高纬度的空间中找一个线性超平面来根据线性可分的情况处理<br> ）<br>如下图：<br><img src="2.png" alt=""></p>
<p>SVM有很多实现,但是本章只关注其中最流行的一种实现,即序列最小优化(Sequential Minimal Optimization,SMO)算法。</p>
<p>希望找到<strong><em>离分隔超平面最近的点,确保它们离分隔面的距离尽可能远</em></strong>。点到超平面的距离称为‘间隔’。<br>我们希望<em>*<em>间隔大的原因</em></em>____*是，在我们犯错或者数据集有限的情况下，svm的分类器能够更加强壮，就是更‘容错’。</p>
<p>数据集中所有点到分隔面的最小间隔的2倍,称为分类器或数据集的间隔。<br>svm就是在找最大的数据集间隔。</p>
<p>常数 C 用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0”这两个目标的权重。在<br>优化算法的实现代码中,常数 C 是一个参数,因此我们就可以通过调节该参数得到不同的结果。<br>一旦求出了所有的 alpha ,那么分隔超平面就可以通过这些 alpha 来表达。这一结论十分直接,SVM中的主要工作就是求解这些 alpha 。</p>
<p><img src="3.png" alt=""><br>可以看到两个支撑着中间的 gap 的超平面，它们到中间的纯红线separating hyper plane 的距离相等，即我们所能得到的最大的 geometrical margin γ˜ 。而“支撑”这两个超平面的必定会有一些点，而这些“支撑”的点便叫做<strong><em>支持向量Support Vector</em></strong>。<br>其实就是离超平面最近点。smo就是为了计算支持向量。<br>由于这些 supporting vector 刚好在边界上，所以它们是满足 y(wTx+b)=1。<br>而对于所有不是支持向量的点，也就是在“阵地后方”的点，则显然有 y(wTx+b)&gt;1 。</p>
<p>SMO算法：<br>二次规划求解工具：一种用于在线性约束下优化具有多个变量的二次目标函数的软件。<br>一旦得到 alpha 的最优值,我们就得到了分隔超平面(2维平面中就是直线)并能够将之用于数据分类。<br>Platt的SMO算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解,并且对<br>它们进行顺序求解的结果与将它们作为整体来求解的结果是完全一致的。在结果完全相同的同时,SMO算法的求解时间短很多。</p>
<p>smo算法公式推导：<a href="https://www.jianshu.com/p/f6382ecc417f" target="_blank" rel="noopener">https://www.jianshu.com/p/f6382ecc417f</a></p>
<p>三个要点：拉格朗日乘子法、KKT条件、核函数。     其他QP、SMO优化算法。</p>
<p>（1）拉格朗日乘子法、KKT条件：求取有约束条件的优化问题。对于等式约束的优化问题，可以应用拉格朗日乘子法去求取最优值；如果含有不等式约束，可以应用KKT条件去求取。<br>只有当是凸函数的情况下，才能保证是充分必要条件，否则两个方法求得的结果只是必要条件。KKT条件是拉格朗日乘子法的泛化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">clf = svm.SVC(C=<span class="number">0.3</span>, kernel=<span class="string">'linear'</span>, decision_function_shape=<span class="string">'ovo'</span>)</span><br><span class="line"><span class="comment"># 分类器</span></span><br><span class="line"><span class="comment"># kernel = 'linear'时，为线性核，C越大分类效果越好，但有可能会过拟合（defaulC = 1）。  C 就是错误项的惩罚值。 </span></span><br><span class="line"><span class="comment"># kernel = 'rbf'时（default），为高斯核，gamma值越小，分类界面越连续；gamma值越大，分类界面越“散”，分类效果越好，但有可能会过拟合。  gamma默认是rbf，是公式中的1/.</span></span><br><span class="line"><span class="comment"># decision_function_shape='ovr'时，为one v rest，即一个类别与其他类别进行划分，</span></span><br><span class="line"><span class="comment"># decision_function_shape='ovo'时，为one v one，即将类别两两之间进行划分，用二分类的方法模拟多分类的结果。</span></span><br></pre></td></tr></table></figure>


<p>参考：<a href="https://www.cnblogs.com/berkeleysong/articles/3251245.html" target="_blank" rel="noopener">https://www.cnblogs.com/berkeleysong/articles/3251245.html</a><br><a href="http://blog.csdn.net/alvine008/article/details/9097105" target="_blank" rel="noopener">http://blog.csdn.net/alvine008/article/details/9097105</a><br><a href="https://www.cnblogs.com/harvey888/p/5852687.html" target="_blank" rel="noopener">https://www.cnblogs.com/harvey888/p/5852687.html</a><br><a href="http://blog.csdn.net/szlcw1/article/details/52259668" target="_blank" rel="noopener">http://blog.csdn.net/szlcw1/article/details/52259668</a><br>机器学习实战</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>python折线图</title>
    <url>/2018/04/24/python%E6%8A%98%E7%BA%BF%E5%9B%BE/</url>
    <content><![CDATA[<h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># __author__='chenliclchen'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">iteration = [<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>, <span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>]</span><br><span class="line">loss = [<span class="number">0.123</span>, <span class="number">0.234</span>, <span class="number">0.345</span>, <span class="number">0.134</span>, <span class="number">0.034</span>, <span class="number">0.089</span>, <span class="number">0.432</span>, <span class="number">0.234</span>, <span class="number">0.342</span>, <span class="number">0.300</span>]</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line"><span class="comment"># 坐标数据</span></span><br><span class="line">ax.plot(iteration, loss, <span class="string">'r'</span>, c=(<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>), ls=<span class="string">'--'</span>, lw=<span class="number">5</span>, marker=<span class="string">'o'</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># 横坐标描述</span></span><br><span class="line">plt.xlabel(<span class="string">'iteration'</span>)</span><br><span class="line"><span class="comment"># 纵坐标描述</span></span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line"><span class="comment"># 限定横坐标限度</span></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">90</span>)</span><br><span class="line"><span class="comment"># 限定纵坐标限度</span></span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">0.45</span>)</span><br><span class="line"><span class="comment"># 设置数字标签</span></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> zip(iteration, loss):</span><br><span class="line">    plt.text(x, y, y, fontsize=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果：<br><img src="1.png" alt=""></p>
<h1 id="坐标轴数据"><a href="#坐标轴数据" class="headerlink" title="坐标轴数据"></a>坐标轴数据</h1><p><code>ax.plot(x, y)</code> 加入x、y坐标轴数据</p>
<h1 id="线条名称-标签"><a href="#线条名称-标签" class="headerlink" title="线条名称(标签)"></a>线条名称(标签)</h1><p>label : 给所绘制的曲线一个名字，此名字在图示/图例(legend)中显示<br><code>plt.xlabel()</code> <code>plt.ylabel()</code> 分别定义x/y坐标标签</p>
<h1 id="线条颜色"><a href="#线条颜色" class="headerlink" title="线条颜色"></a>线条颜色</h1><p>线条颜色命名方式有三种：</p>
<ul>
<li>英文全名， 如blue</li>
<li>缩写， 如b</li>
<li>16进制 ，如FF00FF</li>
<li>(r, g, b) 或 (r, g, b, a)，如（1,0,1,1） ，其中 r g b a 取均为[0, 1]之间，[0, 1]之间的浮点数的字符串形式，表示灰度值。0表示黑色，1表示白色</li>
</ul>
<p>使用：<br>plot方法的关键字参数color(或c)用来设置线的颜色。<br>如 <code>ax.plot(iteration, loss, color=&#39;blue&#39;)</code> 或 <code>ax.plot(iteration, loss, c=&#39;blue&#39;)</code><br>还可以直接<code>ax.plot(iteration, loss, &#39;r&#39;)</code>指定颜色， 如例子<br>详细颜色参数：<a href="http://www.cnblogs.com/darkknightzh/p/6117528.html" target="_blank" rel="noopener">http://www.cnblogs.com/darkknightzh/p/6117528.html</a></p>
<h1 id="线条形状"><a href="#线条形状" class="headerlink" title="线条形状"></a>线条形状</h1><p>plot方法的关键字参数linestyle(或ls)用来设置线的样式。<br>如<code>ax.plot(iteration, loss, linestyle=&#39;:&#39;)</code> 或 <code>ax.plot(iteration, loss, ls=&#39;:&#39;)</code><br>可取值为：<br><code>&#39;-&#39;</code>      实线(solid)<br><code>&#39;--&#39;</code>     短线(dashed)<br><code>&#39;-.&#39;</code>     短点相间线(dashdot)<br><code>&#39;：&#39;</code>    虚点线(dotted)<br><code>&#39;&#39;</code>, <code>&#39; &#39;</code>, None</p>
<h1 id="线条大小"><a href="#线条大小" class="headerlink" title="线条大小"></a>线条大小</h1><p>设置plot方法的关键字参数linewidth(或lw)可以改变线的粗细，其值为浮点数。<br>如例：plt.plot(x, y1, c=’r’, ls=’–’, lw=3)</p>
<h1 id="折点样式"><a href="#折点样式" class="headerlink" title="折点样式"></a>折点样式</h1><p>(1)marker – 折点形状<br>(2)markeredgecolor 或 mec – 折点外边颜色<br>(3)markeredgewidth 或 mew – 折点线宽<br>(4)markerfacecolor 或 mfc –折点实心颜色<br>(5)markerfacecoloralt 或 mfcalt<br>(6)markersize 或 ms –折点大小<br>折点形状选择:<br>================    ===============================<br>character           description<br>================    ===============================<br><code>&#39;-&#39;</code>             solid line style<br><code>&#39;--&#39;</code>            dashed line style<br><code>&#39;-.&#39;</code>            dash-dot line style<br><code>&#39;:&#39;</code>             dotted line style<br><code>&#39;.&#39;</code>             point marker<br><code>&#39;,&#39;</code>             pixel marker<br><code>&#39;o&#39;</code>             circle marker<br><code>&#39;v&#39;</code>             triangle_down marker<br><code>&#39;^&#39;</code>             triangle_up marker<br><code>&#39;&lt;&#39;</code>             triangle_left marker<br><code>&#39;&gt;&#39;</code>             triangle_right marker<br><code>&#39;1&#39;</code>             tri_down marker<br><code>&#39;2&#39;</code>             tri_up marker<br><code>&#39;3&#39;</code>             tri_left marker<br><code>&#39;4&#39;</code>             tri_right marker<br><code>&#39;s&#39;</code>             square marker<br><code>&#39;p&#39;</code>             pentagon marker<br><code>&#39;*&#39;</code>             star marker<br><code>&#39;h&#39;</code>             hexagon1 marker<br><code>&#39;H&#39;</code>             hexagon2 marker<br><code>&#39;+&#39;</code>             plus marker<br><code>&#39;x&#39;</code>             x marker<br><code>&#39;D&#39;</code>             diamond marker<br><code>&#39;d&#39;</code>             thin_diamond marker<br><code>&#39;|&#39;</code>             vline marker<br><code>&#39;_&#39;</code>             hline marker<br>================    ===============================</p>
<p>语法如以下例:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(x, y1, marker=<span class="string">'o'</span>, mec=<span class="string">'r'</span>, mfc=<span class="string">'w'</span>)</span><br><span class="line">plt.plot(x, y2, marker=<span class="string">'*'</span>, ms=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h1 id="alpha线条透明度"><a href="#alpha线条透明度" class="headerlink" title="alpha线条透明度"></a>alpha线条透明度</h1><p>alpha的值在[0,1]之间</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(x,y1,ls=<span class="string">'--'</span>,lw=<span class="number">4</span>,c=<span class="string">'b'</span>,alpha=<span class="number">0.5</span>,label=<span class="string">'total'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="显示纵坐标标签"><a href="#显示纵坐标标签" class="headerlink" title="显示纵坐标标签"></a>显示纵坐标标签</h1><p>使用<code>plt.text(x, y, y)</code> 如例子中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> zip(iteration, loss):</span><br><span class="line">    plt.text(x, y, y, fontsize=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<h1 id="例2"><a href="#例2" class="headerlink" title="例2"></a>例2</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># __author__='chenliclchen'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pylab <span class="keyword">import</span> datestr2num</span><br><span class="line"></span><br><span class="line"><span class="comment"># iteration = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]</span></span><br><span class="line">iteration = [<span class="string">'2018-01-24'</span>, <span class="string">'2018-02-24'</span>, <span class="string">'2018-03-24'</span>, <span class="string">'2018-04-24'</span>, <span class="string">'2018-05-24'</span>, <span class="string">'2018-06-24'</span>, <span class="string">'2018-07-24'</span>, <span class="string">'2018-08-24'</span>, <span class="string">'2018-09-24'</span>, <span class="string">'2018-10-24'</span>]</span><br><span class="line">loss = [<span class="number">0.123</span>, <span class="number">0.234</span>, <span class="number">0.345</span>, <span class="number">0.134</span>, <span class="number">0.034</span>, <span class="number">0.089</span>, <span class="number">0.432</span>, <span class="number">0.234</span>, <span class="number">0.342</span>, <span class="number">0.300</span>]</span><br><span class="line">loss1 = [<span class="number">0.345</span>, <span class="number">0.134</span>, <span class="number">0.034</span>, <span class="number">0.123</span>, <span class="number">0.234</span>, <span class="number">0.342</span>, <span class="number">0.300</span>, <span class="number">0.234</span>, <span class="number">0.089</span>, <span class="number">0.432</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置图形的风格</span></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"><span class="comment"># 设置图形的大小</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"><span class="comment"># 坐标数据</span></span><br><span class="line">xnum = [datestr2num(date) <span class="keyword">for</span> date <span class="keyword">in</span> iteration]</span><br><span class="line">plot1 = plt.plot_date(xnum, loss, <span class="string">'r'</span>, ls=<span class="string">'--'</span>, lw=<span class="number">5</span>, marker=<span class="string">'o'</span>, alpha=<span class="number">0.5</span>, label=<span class="string">'loss1'</span>)</span><br><span class="line">plot2 = plt.plot_date(xnum, loss1, <span class="string">'g'</span>, label=<span class="string">'loss2'</span>)</span><br><span class="line">plt.title(<span class="string">'loss VS loss1'</span>)</span><br><span class="line"><span class="comment"># 横坐标描述</span></span><br><span class="line">plt.xlabel(<span class="string">'iteration'</span>)</span><br><span class="line"><span class="comment"># 横坐标数据旋转45度</span></span><br><span class="line">plt.xticks(rotation=<span class="number">45</span>)</span><br><span class="line"><span class="comment"># 纵坐标描述</span></span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line"><span class="comment"># 限定横坐标限度</span></span><br><span class="line"><span class="comment"># plt.xlim(0, 90)</span></span><br><span class="line"><span class="comment"># 限定纵坐标限度</span></span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">0.45</span>)</span><br><span class="line"><span class="comment"># 设置数字标签</span></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> zip(xnum, loss):</span><br><span class="line">    plt.text(x, y, y, fontsize=<span class="number">10</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"><span class="comment"># 显示背景网格</span></span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果：<br><img src="2.png" alt=""><br><strong><em>注意</em></strong>要把<code>plt.xlim(0, 90)</code>注释掉，不然会报错<code>ValueError ordinal must be &gt;= 1</code><br>情景：使用python的matplotlib画图时报错。<br>原因：matplotlibx轴数据找不到x轴对应地方 The error is due to matplotlib’s inability to find the location of x-axis value along the x-axis<br>解决：不同细节不同解决。我的是设置了xlim导致x轴找不到地方放数据</p>
<h1 id="多个折线"><a href="#多个折线" class="headerlink" title="多个折线"></a>多个折线</h1><p>通过多个<code>plt.plot</code>可以有多条折线，如上例2。</p>
<h1 id="图例"><a href="#图例" class="headerlink" title="图例"></a>图例</h1><p>在 方法<code>plt.plot()</code>的参数里加上label参数后，再后面执行<code>plt.legend()</code>可以直接显示折线的示例</p>
<h1 id="图形的风格"><a href="#图形的风格" class="headerlink" title="图形的风格"></a>图形的风格</h1><p>使用<code>plt.style.use(&#39;ggplot&#39;)</code> 设置图形的风格，共有五种不同风格：bmh、ggplot、dark_background、fivethirtyeight和grayscale。</p>
<h1 id="背景网格"><a href="#背景网格" class="headerlink" title="背景网格"></a>背景网格</h1><p>普通<code>plt.grid()</code>设置，有设置图形风格时需要<code>plt.grid(True)</code>才能正常显示网格</p>
<h1 id="横坐标刻度旋转"><a href="#横坐标刻度旋转" class="headerlink" title="横坐标刻度旋转"></a>横坐标刻度旋转</h1><p><code>plt.xticks(rotation=45)</code>设置横坐标刻度旋转45度</p>
<h1 id="横坐标是时间"><a href="#横坐标是时间" class="headerlink" title="横坐标是时间"></a>横坐标是时间</h1><p>先引入<code>from matplotlib.pylab import datestr2num</code>，<br>再使用<code>datestr2num()</code>方法把时间字符串转成数字<br>最后使用方法<code>plt.plot_date()</code>代替<code>plt.plot()</code>方法显示时间</p>
<h1 id="中文"><a href="#中文" class="headerlink" title="中文"></a>中文</h1><p>python 2.7.12<br>使用<code>label=&#39;中&#39;.decode(&#39;utf-8&#39;)</code> 保证不会中文不会报错，但是会是乱码；再使用<code>plt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;]</code>保证中文正确显示。</p>
<p>另外，中文报错还可以使用另一种方法解决：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">reload(sys)  <span class="comment">#Python2.5 初始化后会删除 sys.setdefaultencoding 这个方法，我们需要重新载入</span></span><br><span class="line">sys.setdefaultencoding(<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure>

<p>参考：<a href="https://blog.csdn.net/helunqu2017/article/details/78629136" target="_blank" rel="noopener">https://blog.csdn.net/helunqu2017/article/details/78629136</a><br><a href="https://zhuanlan.zhihu.com/p/24675460" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24675460</a></p>
]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title>logistic回归LR及代码实现</title>
    <url>/2018/04/23/logistic%E5%9B%9E%E5%BD%92LR%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>参看：<a href="http://blog.csdn.net/zouxy09/article/details/20319673" target="_blank" rel="noopener">http://blog.csdn.net/zouxy09/article/details/20319673</a><br>公式推导比较合理，参看：<a href="http://blog.csdn.net/lizhe_dashuju/article/details/49864569" target="_blank" rel="noopener">http://blog.csdn.net/lizhe_dashuju/article/details/49864569</a><br>注意，公式的推导使用矩阵抵消了加号/乘号</p>
<p>（1）找最合适的模型（2）找损失函数（3）求模型里参数的最优解。</p>
<h1 id="logistic回归（LR）"><a href="#logistic回归（LR）" class="headerlink" title="logistic回归（LR）"></a>logistic回归（LR）</h1><p>优点:计算代价不高,易于理解和实现。<br>缺点:容易欠拟合,分类精度可能不高。<br>适用数据类型:数值型和标称型数据。</p>
<p>logistic回归是线性分类模型。《机器学习》书中给logistic函数叫做对数几率函数，简称对率函数。但是网上都称其为逻辑回归。<br>它与线性回归的差别是 ，它把线性回归的输出的很大范围的值缩小到0-1之间，看着更像是百分比/可能性的含义。<br>w*x+b得到的是一条直线，但是真正的数据可能并不是分布在直线两侧，有些个例可能分布并不如意，如果使用曲线就可以很好的绕开一些数据。<br>想要实现这样的功能，只需要在最后加一个logistic函数。对于二分类来说，可以简单的认为大于0.5和小于0.5的两个类别。<br>所以，logistic回归就是最后被logistic函数归一化后的线性回归。<br><del>逻辑回归函数就是sigmoid函数，实质是 正样本可能性/负样本可能性 的变换得到。</del>（理解有误，看下面！！！）<br>sigmoid：1/（1+exp（Y））        Y=xi × wi。</p>
<p><strong><em>！！！！</em></strong></p>
<p>看《机器学习》后可知选用sigmoid，并不是推导出来的（看了网上的 正样本可能性/负样本可能性 的变换得到 说法），而是w*x+b得到的是一个数值，<br>但是考虑到是二分类，因此希望是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">        0     z&lt;0</span><br><span class="line"></span><br><span class="line">y  &#x3D;   0.5    z&#x3D;0</span><br><span class="line"></span><br><span class="line">        1     z&gt;0</span><br></pre></td></tr></table></figure>
<p>但是这是跳跃形式的，不是连续的函数，因此才考虑到去使用sigmoid(看图更清楚)。Sigmoid函数是一种阶跃函数(step function)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">        &lt;0.5     z&lt;0</span><br><span class="line"></span><br><span class="line">y       &#x3D;0.5     z&#x3D;0</span><br><span class="line"></span><br><span class="line">        &gt;0.5     z&gt;0</span><br></pre></td></tr></table></figure>
<p><img src="1.png" alt=""></p>
<p>之后把 y=wi * xi  带入  y = 1 / ( 1 + exp( -z ) ) 推导可得 z = wi * xi = ln( y/ (1 - y))   就得到了上面的  正样本可能性/负样本可能性 的说法。【《机器学习》中，y视为正样本可能性，1-y是为负样本可能性，y / (1-y) 为 x为正样本的相对可能性，称推导出来的z是对数几率logit，它和sigmoid正好相反】</p>
<h1 id="损失函数-代价函数"><a href="#损失函数-代价函数" class="headerlink" title="损失函数 == 代价函数"></a>损失函数 == 代价函数</h1><p><strong><em>一般选用平方差/最大似然等作为损失误差函数，它求导方便，更容易做梯度下降优化</em></strong>：<br><img src="2.png" alt=""><br>当它趋近于0时，损失最小，更趋近于准确值。</p>
<h1 id="求最优解—梯度下降"><a href="#求最优解—梯度下降" class="headerlink" title="求最优解—梯度下降"></a>求最优解—梯度下降</h1><p>求最优解的两种方法：<br>（1）求导。一般对优化函数求解是使用求导的方式，求导以后，导函数为零的地方一般最优解的地方。<br>    损失函数一般无法直接求得最小值。我们都知道，函数求导为0时函数值最小/大。因此对损失函数求导得最小值。<br>（2）迭代。但是很多情况下，很难求出最优解。可能是参数间太耦合无法求导，也可能是参数太多求导以后无法求解，这时候要使用迭代。理解爬山，能求解的一般只有一个山峰，需要迭代的有很多山峰，要一步一步去找那个最高/低的山峰（如果参数不好，可能结果找到得只是局部最优解）。<br>    如果损失函数是凸函数，那一定有全局最优解。<br>使用梯度下降法求得w的值，推导可以参考上面第二个链接里的，它在推导时没有漏掉任何一个加减号，但是有个小问题，看评论就可以看到。<br>梯度下降法就是给w随机赋一个初始值，然后按照一介偏导的反方向移动：<br><img src="3.png" alt=""><br>以爬坡为例：我们需要每一步都找下坡最快的地方，也就是每一步我走某个方向，都比走其他方法，要离最小值更近。而这个下坡最快的方向，就是梯度的负方向了。</p>
<h1 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h1><p>梯度下升算法在每次更新回归系数时都需要遍历整个数据集,该方法在处理100个左右的数据集时尚可,但如果有数十亿样本和成千上万的特征,那么该方法的计算复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数,该方法称为随机梯度下升算法。</p>
<p>数据集并非线性可分,在每次迭代时会引发系数的剧烈改变。</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/meihuakaile/machine/blob/master/logistic.py" target="_blank" rel="noopener">代码</a><br>使用逻辑回归识别数字0/1，附有数据</p>
<p>参考：《机器学习》、《机器学习实战》</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>(())、（）、[]区别</title>
    <url>/2018/04/20/(())%E3%80%81%EF%BC%88%EF%BC%89%E3%80%81%5B%5D%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h3 id="（）"><a href="#（）" class="headerlink" title="（）"></a>（）</h3><h4 id="定义数组"><a href="#定义数组" class="headerlink" title="定义数组"></a>定义数组</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array&#x3D;(0 1 2 3)</span><br></pre></td></tr></table></figure>
<p>以空格为分隔符 分割 字符串为数组。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">a=<span class="string">"1 2 3 4"</span></span><br><span class="line">arr=(<span class="variable">$&#123;a&#125;</span>)</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;arr[0]&#125;</span></span><br></pre></td></tr></table></figure>
<p>可以利用这个特性把一些函数的输出作为参数，转换成数组。</p>
<h4 id="命令替换"><a href="#命令替换" class="headerlink" title="命令替换"></a>命令替换</h4><p><strong><em>与``一样，用以执行shell命令。需要搭配 $使用。</em></strong><br>并不是所有的shell解释器都可以使用。至少bash可以使用。用$() 代替``在shell脚本里执行shell命令。</p>
<p>$()优点、``缺点：<br>``只用命令嵌套时不容易使用。` ` 须要额外的( ` )处理，而 $( ) 则比较直观。如：command1 `command2 `command3` `，<br>原本的意图是要在 command2 `command3` 先将 command3 提换出来给 command 2 处理，<br>然后再将结果传给 command1 `command2 …` 来处理。<br>然而，真正的结果在命令行中却是分成了 `command2 ` 与 “ 两段。<br>正确的输入应该如下：<br>command1 `command2 \`command3\` `</p>
<p>或者，换成 $( ) 就没问题了（嵌套多少层都没有问题）：<br>command1 $(command2 $(command3))</p>
<p>` ` 很容易与 ‘ ‘ ( 单引号)搞混乱。有时在一些奇怪的字形显示中，两种符号是一模一样的。</p>
<p>$()缺点： ``几乎所有版本的解释器都可以使用，但是$()较少，移植性不高。</p>
<h4 id="命令组"><a href="#命令组" class="headerlink" title="命令组"></a>命令组</h4><p>括号中的命令将会新开一个子shell顺序执行，所以括号中的变量不能够被脚本余下的部分使用。括号中多个命令之间用分号隔开，最后一个命令可以没有分号，各命令和括号之间不必有空格。</p>
<h3 id=""><a href="#" class="headerlink" title="(())"></a>(())</h3><p>双括号中的变量可以不使用$符号前缀。</p>
<h4 id="重定义变量值"><a href="#重定义变量值" class="headerlink" title="重定义变量值"></a>重定义变量值</h4><p>单纯用 (( )) 也可重定义变量值，比如 a=5; ((a++)) 可将 $a 重定义为6。</p>
<h4 id="算术运算比较"><a href="#算术运算比较" class="headerlink" title="算术运算比较"></a>算术运算比较</h4><p>常用于算术运算比较。<br>比如可以直接使用for((i=0;i&lt;5;i++))  不用(())时，可以使用 for i in `seq 0 4`或者for i in {0..4}。<br>再比如可以直接使用if ((i&lt;5)) （if和括号之间需要空格）, 如果不使用双括号, 则可用if test $a -gt 5或者if [ $i -lt 5 ]  （[]的前后都需要空格）。</p>
<h4 id="整数扩展"><a href="#整数扩展" class="headerlink" title="整数扩展"></a>整数扩展</h4><p>这种扩展计算是整数型的计算，不支持浮点型。((exp))结构扩展并计算一个算术表达式的值，如果表达式的结果为0，那么返回的退出状态码为1，或者 是”假”，而一个非零值的表达式所返回的退出状态码将为0，或者是”true”。若是逻辑判断，表达式exp为真则为1,假则为0。</p>
<h4 id="C语言运算规则可用"><a href="#C语言运算规则可用" class="headerlink" title="C语言运算规则可用"></a>C语言运算规则可用</h4><p>只要括号中的运算符、表达式符合C语言运算规则，都可用在$((exp))中，甚至是三目运算符。作不同进位(如二进制、八进制、十六进制)运算时，输出结果全都自动转化成了十进制。如：echo $((16#5f)) 结果为95 (16进位转十进制)</p>
<h3 id="-1"><a href="#-1" class="headerlink" title="[]"></a>[]</h3><p>（1）bash 的内部命令，[和test是等同的。if/test结构中的左中括号是调用test的命令标识，右中括号是关闭条件判断的。<br>（2）Test和[]中可用的<strong><em>比较运算符只有=和!=，两者都是用于字符串比较的，不可用于整数比较，整数比较只能使用-eq，-gt这种形式。无论是字符串比较还是整数比较都不支持大于号小于号.</em></strong><br>（3）在一个array 结构的上下文中，中括号用来引用数组中每个元素的编号。<br>（4）字符范围。用作正则表达式的一部分，描述一个匹配的字符范围。</p>
<h3 id="-2"><a href="#-2" class="headerlink" title="[[]]"></a>[[]]</h3><p>(1)可以直接使用大于小于号。&amp;&amp;、||、&lt;和&gt; 操作符能够正常存在于[[ ]]条件判断结构中。</p>
<p>比如可以直接使用if [[ $a != 1 &amp;&amp; $a != 2 ]], 如果不适用双括号, 则为if [ $a -ne 1] &amp;&amp; [ $a != 2 ]或者if [ $a -ne 1 -a $a != 2 ]</p>
<p>参看：<a href="http://blog.csdn.net/taiyang1987912/article/details/39551385" target="_blank" rel="noopener">http://blog.csdn.net/taiyang1987912/article/details/39551385</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>awk</title>
    <url>/2018/04/20/awk/</url>
    <content><![CDATA[<p>把文件逐行的读入，以空格（还可以指定别的字符）为默认分隔符将每行切片，切开的部分再进行各种分析处理。</p>
<h1 id="使用注意点"><a href="#使用注意点" class="headerlink" title="使用注意点"></a>使用注意点</h1><p>（1）指定分隔符是引号<code>“</code>时，需要转义<code>\&quot;</code> 。<code>awk -F&quot;\&quot;,\&quot;&quot; &#39;{}&#39;</code>用<code>&quot;，&quot;</code>分割<br>（2）<code>awk &#39;pattern{action}&#39;</code>的 <code>pattern{action}</code> 的外面<strong><em>必须是单引号，不能是双引号</em></strong><br>（3）内值函数<code>NF</code>是一行被分割的列数。<code>awk -F&quot;_&quot; &#39;{for(i=1; i&lt;= NF; i++){print i,$i}}&#39;</code>  <code>$0</code>输出一行<br>（4）<code>BEGIN</code> <code>END</code>必须是大写。<br>（5）由上面的（2），外面是单引号，那里面字符必须使用双引号。<br>（6）每个print之后都会输出一个ORS。</p>
<h1 id="print-单-双引号"><a href="#print-单-双引号" class="headerlink" title="print 单/双引号"></a>print 单/双引号</h1><p>双引号：使用<code>\</code>转义<code>awk &#39;{print &quot;version=\&quot;1.0\&quot;&quot;}&#39;</code> 输出结果<code>version=&quot;1.0&quot;</code><br>单引号：使用<code>\</code>转义，并用单引号引起来<code>awk &#39;{print &quot;version=&#39;\&#39;&#39;1.0&#39;\&#39;&#39;&quot;}&#39;</code> 输出结果<code>version=&#39;1.0&#39;</code></p>
<h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><p>（1）awk ‘pattern{action}’ input-file(s)<br>pattern匹配行，并对匹配做出action操作。{action}可省。<br>pattern 表示 AWK 在数据中查找的内容，而 action 是在找到匹配内容时所执行的一系列命令。<br>pattern常用的有：</p>
<ul>
<li>/正则表达式/：使用通配符的扩展集。</li>
<li>关系表达式：可以用下面运算符表中的关系运算符进行操作，可以是字符串或数字的比较，如$2&gt;%1选择第二个字段比第一个字段长的行。</li>
<li>模式匹配表达式：用运算符<del>(匹配)和</del>!(不匹配)。</li>
<li>模式，模式：指定一个行的范围。该语法不能包括BEGIN和END模式。</li>
<li>BEGIN：让用户指定在第一条输入记录被处理之前所发生的动作，通常可在这里设置全局变量。</li>
<li>END：让用户在最后一条输入记录被读取之后发生的动作。</li>
</ul>
<p>action用{}括起来。<br>例1：搜索/etc/passwd中有root的关键词的行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk -F: &#39;&#x2F;root&#x2F;&#39; &#x2F;etc&#x2F;passwd</span><br><span class="line">输出：</span><br><span class="line">root:x:0:0:root:&#x2F;root:&#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>
<p>例2：搜索/etc/passwd中以‘r’开头的行，并输出匹配行的用户名和对应的shell（注意：正则表达式中，^出现在[]里时是非的意思，其他是以…开头的意思）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk -F: &#39;&#x2F;^r&#x2F;&#123;print $1, $7&#125;&#39; &#x2F;etc&#x2F;passwd</span><br><span class="line">输出：</span><br><span class="line">root &#x2F;bin&#x2F;bash</span><br><span class="line">rtkit &#x2F;bin&#x2F;false</span><br></pre></td></tr></table></figure>
<p>在例2中pattern 是 ^r  ，用以匹配以 r 开头的行； action是 {print $1, $7}’   ，输出第1、7个域。-F: 是每行按照冒号分割。<br>合起来的意思就是：搜索/etc/passwd中以‘r’开头的行，行按   冒号： 分割，输出这些行第1、7个分割值.</p>
<h1 id="awk-F-”field-separator“-‘commands’-input-file-s"><a href="#awk-F-”field-separator“-‘commands’-input-file-s" class="headerlink" title="awk [-F ”field-separator“] ‘commands’ input-file(s)"></a>awk [-F ”field-separator“] ‘commands’ input-file(s)</h1><p>其中commands=BEGIN{} {} END{}<br>commands 对文件的真正处理操作，[-F 域分隔符]是可选的，有默认值。 input-file(s) 是待处理的文件。<br>在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格。commands的执行流程：</p>
<ol>
<li>先执行BEGING，</li>
<li>然后读取文件，读入有/n换行符分割的一条记录，然后将记录（行）按指定的域分隔符划分域，填充域，</li>
<li>$0则表示所有域,$1表示第一个域,$n表示第n个域,随后开始执行模式所对应的动作action。</li>
<li>接着开始读入第二条记录······直到所有的记录都读完，最后执行END操作。<br>例如：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk -F: &#39;BEGIN &#123;count&#x3D;0;&#125; &#123;name[count] &#x3D; $1; count++;&#125; END&#123;for (i&#x3D;0; i&lt;NR; i++) &#123;print i, name[i]&#125;&#125;&#39; &#x2F;etc&#x2F;passwd</span><br><span class="line">部分输出：</span><br><span class="line">0 root</span><br><span class="line">1 daemon</span><br><span class="line">2 bin</span><br><span class="line">3 sys</span><br><span class="line">4 sync</span><br><span class="line">5 games</span><br><span class="line">6 man</span><br><span class="line">7 lp</span><br><span class="line">8 mail</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<h1 id="BEGIN-END"><a href="#BEGIN-END" class="headerlink" title="BEGIN/END"></a>BEGIN/END</h1>BEGIN/END后面的第一个{}的内容是跟随begin和end的含义，BEGIN后的第二个{}是每行都会执行。如：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ls -l |awk &#39;BEGIN &#123;size&#x3D;0;print &quot;[begin]size is &quot;, size;&#125; &#123;size&#x3D;size+$5;&#125; END&#123;print &quot;[end]size is &quot;, size&#125;&#39;</span><br><span class="line">输出：</span><br><span class="line">[begin]size is 0</span><br><span class="line">[end]size is 0</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ls -l |awk &#39;BEGIN &#123;size&#x3D;0;&#125; &#123;print &quot;[begin]size is &quot;, size;size&#x3D;size+$5;&#125; END&#123;print &quot;[end]size is &quot;, size&#125;</span><br><span class="line">输出：</span><br><span class="line">[begin]size is 0</span><br><span class="line">[begin]size is 0</span><br><span class="line">[begin]size is 4096</span><br><span class="line">[begin]size is 8192</span><br><span class="line">[begin]size is 12288</span><br><span class="line">......</span><br><span class="line">[end]size is 232404</span><br></pre></td></tr></table></figure>
<h2 id="begin"><a href="#begin" class="headerlink" title="begin"></a>begin</h2>BEGIN模块后紧跟着动作块，这个动作块在awk处理任何输入文件之前执行。所以它可以在没有任何输入的情况下进行测试。它通常用来改变内建变量的值，如OFS,RS和FS等，以及打印标题。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk &#39;BEGIN&#123;FS&#x3D;&quot;:&quot;; OFS&#x3D;&quot;\t&quot;; ORS&#x3D;&quot;\n\n&quot;&#125;&#123;print $1,$2,$3&#125;&#39; test </span><br><span class="line">#在处理输入文件test以前，域分隔符(FS)被设为冒号，输出文件分隔符(OFS)被设置为制表符，输出记录分隔符(ORS)被设置为两个换行符</span><br><span class="line">awk &#39;BEGIN&#123;print &quot;TITLE TEST&quot;&#125;&#39;     #只打印标题</span><br></pre></td></tr></table></figure>
<h1 id="awk的语法"><a href="#awk的语法" class="headerlink" title="awk的语法"></a>awk的语法</h1>if判断语法、循环如while、for、break等都是借鉴与c语言。</li>
</ol>
<h1 id="awk的内置变量"><a href="#awk的内置变量" class="headerlink" title="awk的内置变量"></a>awk的内置变量</h1><p><strong><em>可以通过-v指定， 也可在BEGIN中指定。</em></strong><br><code>ARGC</code> 命令行参数个数<br><code>ARGV</code> 命令行参数排列<br><code>ENVIRON</code> 支持队列中系统环境变量的使用<br><code>FILENAME</code> awk浏览的文件名<br><code>FNR</code> 浏览文件的记录数<br><code>NF</code> 浏览记录的域的个数<br><code>NR</code> 已读的记录数（当前记录编号）<br><code>FS</code> 设置输入域分隔符，等价于命令行 -F选项<br><code>OFS</code> 输出域分隔符<br><code>ORS</code> 输出记录分隔符<br><code>RS</code> 控制记录分隔符</p>
<p><code>FS</code> : 输入字段分隔符，默认空格。<br><code>RS</code> : 输入行分隔符，默认\n。<br><code>OFS</code> : 输出字段分隔符，默认空格。<br><code>ORS</code> : 输出行分隔符，默认\n</p>
<p>eg：每个人的信息条数不确定，区分是一个空行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">路人甲</span><br><span class="line">电话：13777707771</span><br><span class="line">手机：010-12345678</span><br><span class="line"></span><br><span class="line">路人乙</span><br><span class="line">电话：13912344321</span><br><span class="line">手机：010-56784321</span><br><span class="line">QQ： 87654221</span><br></pre></td></tr></table></figure>
<p>把每个人的信息有行变成列：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">路人甲　　电话：13777707771　　手机：010-12345678</span><br><span class="line">路人乙　　电话：13912344321　　手机：010-56784321　　QQ： 87654221</span><br></pre></td></tr></table></figure>

<p>命令：<code>cat t5|awk &#39;BEGIN{FS=&quot;\n&quot;;RS=&quot;&quot;;ORS=&quot;&quot;}{for(i=1; i&lt;=NF; i++){print $i,&quot;\t&quot;;} print &quot;\n&quot;;}&#39;</code></p>
<p>解析踩坑：for循环的每个print之后会输出一个ORS，看<code>cat t5|awk &#39;BEGIN{FS=&quot;\n&quot;;RS=&quot;&quot;;ORS=&quot;\n&quot;}{for(i=1; i&lt;=NF; i++){print $i,&quot;\t&quot;;}}&#39;</code>这个命令的结果是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">路人甲 	</span><br><span class="line">电话：13777707771 	</span><br><span class="line">手机：010-12345678 	</span><br><span class="line">路人乙 	</span><br><span class="line">电话：13912344321 	</span><br><span class="line">手机：010-56784321 	</span><br><span class="line">QQ： 87654221</span><br></pre></td></tr></table></figure>
<p>这个命令是不可行的。需要指定可见<code>ORS</code>，手动输出<code>\n</code>.<br>OFS在输出$0时不起作用，OFS只用在输出多个域时会插入到每个域之间。</p>
<p>总结自：<a href="https://www.cnblogs.com/leezhxing/p/4694323.html" target="_blank" rel="noopener">https://www.cnblogs.com/leezhxing/p/4694323.html</a></p>
<h1 id="多个分隔符"><a href="#多个分隔符" class="headerlink" title="多个分隔符"></a>多个分隔符</h1><p>（1）width:720 height: 360</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;width::720 height: 360&quot; | awk -F &#39;[ :]+&#39; &#39;&#123;print $2, $4&#125; &#39;</span><br><span class="line">输出：720 360</span><br></pre></td></tr></table></figure>
<p> 命令用两个分隔符空格和分号，一次把720和360提出来。<br>‘+’的作用是遇到两个相邻的分隔符时视为一个，如height后面的冒号后面紧跟了一个空格，或者是width后面的冒号后面紧跟了另一个冒号。<br>弊端、约束：多个字符只能是单字符，无法处理多个字符串的情况。</p>
<p>（2）i have two apples and one banana</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;i have two apples and one banana&quot; | awk -F &#39;one|two&#39; &#39;&#123;for(i &#x3D; 1; i &lt;&#x3D; NF; i++) &#123;print &quot;i&#x3D; &quot;, $i&#125;&#125;&#39; </span><br><span class="line">输出：</span><br><span class="line">i&#x3D;  i have </span><br><span class="line">i&#x3D;   apples and </span><br><span class="line">i&#x3D;   banana</span><br></pre></td></tr></table></figure>
<p>命令用one、two分割。<br>使用的是正则表达式， -F后面的分割处可以使用正则。</p>
<h1 id="常用字符串内置函数"><a href="#常用字符串内置函数" class="headerlink" title="常用字符串内置函数"></a>常用字符串内置函数</h1><p>（1）split（a，b，c）以字符c分割字符串a并保存在数组b中<br><code>n =split(string, array, separator)</code><br>注：n为split返回的分割的数据元素的个数（访问最后一个元素array[n]）；如果不指定separator，则使用FS；separator可以是正则表达式。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat test.txt</span><br><span class="line">输出：</span><br><span class="line">Tom　　  2012-12-11      car     53000</span><br><span class="line">John　　 2013-01-13      train    41000</span><br><span class="line">vivi    2013-01-18      car     42800</span><br><span class="line">Tom　　  2013-01-20      car     32500</span><br><span class="line">John　　 2013-01-28      train    63500</span><br></pre></td></tr></table></figure>
<p>计算cat.txt中每个人一月份的工资总和 （注意，语法for ind in b的ind是数组下标，而不是java中理解的数组元素） ：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat test.txt|awk &#39;&#123;split($2,a,&quot;-&quot;);if(a[2]&#x3D;&#x3D;01)&#123;b[$1]+&#x3D;$4&#125;&#125;END&#123;for(i in b)print i,b[i]&#125;&#39;</span><br><span class="line">输出：</span><br><span class="line">vivi 42800</span><br><span class="line">Tom　　 32500</span><br><span class="line">John　　 104500</span><br></pre></td></tr></table></figure>
<p>（2）substr(a, b[, c])从b下标开始截取a，取长度c;c可选，没有默认到末尾。<br>（3）length每个记录的长度。如awk ‘{print length}’ ./test.txt 是test中每行长度。<br>（4）sub (a，b[，c])目标字符串c中的a子串用b串替换，只替换每个匹配到的第一个，c默认是整个记录。a可以是正则表达式。改变的原串，不是返回值。<br>（5）gsub（a，b[，c]）目标字符串c中的所有a子串用b串替换，替换所有匹配到的字符串。<br>（6）index（string， substring）substring在string中第一次被匹配到的下标。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk &#39;&#123; print index(&quot;mytest&quot;, &quot;test&quot;) &#125;&#39; testfile</span><br><span class="line">输出：</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<h1 id="匹配操作符～"><a href="#匹配操作符～" class="headerlink" title="匹配操作符～"></a>匹配操作符～</h1><p>test.txt的内容如上</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat test.txt | awk &#39;$3 ~&#x2F;^ca&#x2F;&#39;    #匹配第三列以ca开头的行</span><br><span class="line">输出：</span><br><span class="line">Tom　　  2012-12-11      car     53000</span><br><span class="line">vivi    2013-01-18      car     42800</span><br><span class="line">Tom　　  2013-01-20      car     32500</span><br></pre></td></tr></table></figure>
<h1 id="比较表达式"><a href="#比较表达式" class="headerlink" title="比较表达式"></a>比较表达式</h1><p>conditional expression1 ? expression2: expression3，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk &#39;&#123;max &#x3D; &#123;$1 &gt; $3&#125; ? $1: $3: print max&#125;&#39; test。如果第一个域大于第三个域，$1就赋值给max，否则$3就赋值给max。</span><br><span class="line">awk &#39;$1 + $2 &lt; 100&#39; test。如果第一和第二个域相加大于100，则打印这些行。</span><br><span class="line">awk &#39;$1 &gt; 5 &amp;&amp; $2 &lt; 10&#39; test,如果第一个域大于5，并且第二个域小于10，则打印这些行。</span><br></pre></td></tr></table></figure>
<h1 id="next"><a href="#next" class="headerlink" title="next"></a>next</h1><p>next 语句从输入文件中读取一行，然后执行awk脚本。  通过next在某条件时跳过该行，对下一行执行操作(类似于其他语言的continue)。如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if ($4&gt;3000) </span><br><span class="line">	next; </span><br><span class="line">else c4+&#x3D;$4;</span><br></pre></td></tr></table></figure>
<h1 id="数组注意点"><a href="#数组注意点" class="headerlink" title="数组注意点"></a>数组注意点</h1><p>（1）语法for ind in b的ind是数组下标，而不是java中理解的数组元素。<br>（2）下标可以是字符串。可以上面计算工资的例子。<br>（3）delete(b[ind])。删除元素</p>
<h1 id="多文件例子"><a href="#多文件例子" class="headerlink" title="多文件例子"></a>多文件例子</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cat account</span><br><span class="line">张三|000001</span><br><span class="line">李四|000002</span><br><span class="line"># cat cdr </span><br><span class="line">000001|10</span><br><span class="line">000001|20</span><br><span class="line">000002|30</span><br><span class="line">000002|15</span><br><span class="line"># awk -F | &#39;NR&#x3D;&#x3D;FNR&#123;a[$2]&#x3D;$0;next&#125;&#123;print a[$1]&quot;|&quot;$2&#125;&#39; account cdr</span><br><span class="line">张三|000001|10</span><br><span class="line">张三|000001|20</span><br><span class="line">李四|000002|30</span><br><span class="line">李四|000002|15</span><br></pre></td></tr></table></figure>
<p>多个文件时，NR=FNR为真时,判断当前读入的是第一个文件。</p>
<h1 id="外部变量"><a href="#外部变量" class="headerlink" title="外部变量"></a>外部变量</h1><p>（1）-v 参数 （不要使用内置函数的名字做变量名，会报错）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk -v a&#x3D;&quot;test&quot; -v b&#x3D;&quot;mytest&quot; &#39;&#123;print a,b&#125;&#39; .&#x2F;test.txt</span><br></pre></td></tr></table></figure>
<p>（2）awk运行在shell环境中。所以，写在awk中的命令，要先经过shell解析后，再交由awk来解释和执行.</p>
<p>  awk为了防止shell解析自己的命令 ，在命令外面用了单引号（最前面的commands）。</p>
<p>‘{print “‘$b’”,$2, $4}’跟java调用mysql的非注入方式类似，在拼接sql语句。准确理解是三部分:<code>&#39;{print &quot;&#39;    $b     &#39;&quot;,$2, $4} &#39;</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">b&#x3D;&quot;begin&quot;;echo &quot;width::720 height: 360&quot; | awk -F &#39;[ :]+&#39; &#39;&#123;print &quot;&#39;$b&#39;&quot;,$2, $4&#125; &#39;</span><br><span class="line">输出：</span><br><span class="line">begin 720 360</span><br></pre></td></tr></table></figure>

<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.cnblogs.com/emanlee/p/3327576.html" target="_blank" rel="noopener">https://www.cnblogs.com/emanlee/p/3327576.html</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>crontab定时任务</title>
    <url>/2018/04/20/crontab%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h1 id="crontab参数"><a href="#crontab参数" class="headerlink" title="crontab参数"></a>crontab参数</h1><p>-e 编辑<br>-l 查看定时任务<br>-r 删除定时任务<br>-ir 删除前提醒</p>
<h1 id="crontab使用"><a href="#crontab使用" class="headerlink" title="crontab使用"></a>crontab使用</h1><p>定时文件格式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">minute hour day-of-month month-of-year day-of-week commands</span><br></pre></td></tr></table></figure>
<p>用空格隔开，和spring的定时任务类似，有*等特殊字符<br>如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">*&#x2F;1 * * * * echo &quot;hello&quot; &gt;&gt; ~&#x2F;hello.log      每一分钟发送一个“hello”到hello.log文件</span><br></pre></td></tr></table></figure>
<p>如果想定时的执行某个shell文件，注意一定要是绝对路径，shell文件后还可以跟参数，如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">*&#x2F;1 * * * * &#x2F;home&#x2F;chenliclchen&#x2F;study&#x2F;crontab&#x2F;test.sh  &#x2F;home&#x2F;chenliclchen&#x2F;study&#x2F;crontab&#x2F;result.log &gt;&gt; &#x2F;home&#x2F;chenliclchen&#x2F;study&#x2F;crontab&#x2F;hello.log  </span><br><span class="line"># 每分钟执行test.sh文件 result.log是test.sh的参数，然后把输出结果输出到hello.log文件里。</span><br></pre></td></tr></table></figure>
<p>如何让上面的定时命令执行起来，有两种方法：<br>1、把上面的定时任务写在cron文件里，如crontest.cron文件<br>执行<code>crontab ./crontest.cron</code>就加入了上面的定时任务 ，此时已经不关crontest.cron文件的事了，如果修改了这个文件又想使定时任务真的有效需要再次执行这条命令。<br>之后就可以通过<code>crontab -l</code> 查看定时任务了；<br>也可以通过 -e的编辑定时任务，注意此时编辑并不会更新到上面的crontest.cron文件里。</p>
<p>2、通过 -e的直接编辑定时任务，把上面的定时任务命令写入。</p>
<h1 id="cron服务命令"><a href="#cron服务命令" class="headerlink" title="cron服务命令"></a>cron服务命令</h1><p>（1）查看cron状态</p>
<pre><code>sudo service cron status　</code></pre><p>（2）开启cron</p>
<pre><code>sudo /etc/init.d/cron start</code></pre><p>（3）关闭cron</p>
<pre><code>sudo /etc/init.d/cron stop</code></pre><p>（4）重启cron</p>
<pre><code>sudo /etc/init.d/cron restart</code></pre><h1 id="定时提醒订饭的例子"><a href="#定时提醒订饭的例子" class="headerlink" title="定时提醒订饭的例子"></a>定时提醒订饭的例子</h1><p>每天中午11点每隔5分钟提醒订饭。<br>（1）写一个文件eat.cron，内容（notify-send命令在cron下不会启动消息弹窗。需要在notify-send命令执行之前添加<code>export DISPLAY=:0.0.</code>）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">*&#x2F;5 11 * * * export DISPLAY&#x3D;:0.0 &amp;&amp; notify-send [&#39;订饭啦&#39;] &quot;提醒大家订饭&quot;</span><br></pre></td></tr></table></figure>
<p>（2）执行crontab ./eat.cron 添加定时任务<br>（3）执行 sudo /etc/init.d/cron  start  启动定时任务<br>（4）查看是否添加成功  crontab -l  显示（1）中的代码<br>（5）执行 sudo /etc/init.d/cron  stop  停止定时任务</p>
<p>参考：<a href="https://www.cnblogs.com/kaituorensheng/p/4494321.html" target="_blank" rel="noopener">https://www.cnblogs.com/kaituorensheng/p/4494321.html</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>linux后台任务</title>
    <url>/2018/04/20/linux%E5%90%8E%E5%8F%B0%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<p>1、通过 jobs  命令可以看到所有后台的所有的任务。</p>
<p>2、执行命令后面加 &amp; ，命令会在后台执行。但是输入输出仍然会出现在终端上，所以这个任务还和这个终端关联这的。</p>
<p>3、通过 ctrl + z 终止的命令，可以用 pg 命令使其继续执行。</p>
<p>4、nobup命令可以把命令把终端彻底分开。如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup .&#x2F;install&#x2F;qtalk&#x2F;run.sh &amp;</span><br><span class="line">nohup .&#x2F;install&#x2F;qtalk&#x2F;run.sh &gt;qtalk_log.txt  2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<p>&gt;filename 规定输出目录文件，默认是 当前目录下的nobup.out<br>0、1和2分别表示标准输入、标准输出和标准错误信息输出，2&gt;&amp;1：即将错误信息重定向到标准输出</p>
<p>5、如果是已经用了&amp;在后台的任务，现在想与终端脱离，就使用disown。</p>
<p>在终端中启动的任务是属于这个终端的子进程，终端（父进程）关闭子进程就会关闭。所以一般使用的办法是修改任务的父进程。</p>
<p>参考：<a href="https://yq.aliyun.com/ziliao/54263" target="_blank" rel="noopener">https://yq.aliyun.com/ziliao/54263</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>linux常用命令</title>
    <url>/2018/04/20/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h3 id="防火墙"><a href="#防火墙" class="headerlink" title="防火墙"></a>防火墙</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ufw status   #查看防火墙状态</span><br><span class="line">sudo ufw enable|disable   #开启&#x2F;关闭防火墙 (默认设置是’disable’)</span><br></pre></td></tr></table></figure>
<h3 id="SecureCRT上传下载"><a href="#SecureCRT上传下载" class="headerlink" title="SecureCRT上传下载"></a>SecureCRT上传下载</h3><p>rz上传  sz下载<br>在SecureCRT中使用 rz -be 可以打开上传界面，把本地文件上传到服务器<br>sz 文件1 文件2    可下载多个文件 文件在本地的保存地址看：options — session options — X/Y/Zmodem。<br>rz 上传文件<br>总结自：<a href="http://blog.csdn.net/lioncode/article/details/7921525" target="_blank" rel="noopener">http://blog.csdn.net/lioncode/article/details/7921525</a></p>
<h3 id="vim搜索"><a href="#vim搜索" class="headerlink" title="vim搜索"></a>vim搜索</h3><p>/字符串  第一个出现的字符串<br>?字符串 最后出现的字符串<br>n跳到下一个匹配  N跳到上一个匹配</p>
<h3 id="zcat"><a href="#zcat" class="headerlink" title="zcat"></a>zcat</h3><p>查看压缩文件。</p>
<h3 id="alias"><a href="#alias" class="headerlink" title="alias"></a>alias</h3><p>主要是有些经常用又很长比较难记的命令。<br>输入alias可以看到已有的别名。<br>添加别名的两种办法：<br>（1）打开 .bashrc文件 vim .bashrc<br># some more ls aliases<br>alias ll=’ls -alF’<br>添加别名。如alias d2=’ssh data2.cn’就是只输入d2就相当与输入了ssh登陆“data2.cn”的命令。</p>
<p>（2）新建文件vim ~/.bash_aliases，然后添加：alias d2=’ssh data2.cn’。最后退出输入source .bashrc就可以用了。</p>
<p>详细可参看：<a href="http://blog.csdn.net/a746742897/article/details/52228422" target="_blank" rel="noopener">http://blog.csdn.net/a746742897/article/details/52228422</a></p>
<h3 id="读取数据库中信息。"><a href="#读取数据库中信息。" class="headerlink" title="读取数据库中信息。"></a>读取数据库中信息。</h3><p>mysql -h 127.0.0.1 -u root -p XXXX -P 3306 -e “select * from table”  &gt; /tmp/test/txt<br>-h 后面是主机； -u后面是用户名； -p后面是数据库名字 ； -P后面是端口号 ； -e就是mysql的select语句 ； 最后是重定向到一个文件。</p>
<p>参看：<a href="https://www.cnblogs.com/emanlee/p/4233602.html" target="_blank" rel="noopener">https://www.cnblogs.com/emanlee/p/4233602.html</a></p>
<h3 id="查看端口号占用"><a href="#查看端口号占用" class="headerlink" title="查看端口号占用"></a>查看端口号占用</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">netstat -apn | grep 8080</span><br><span class="line">ps -ef | grep 8080</span><br></pre></td></tr></table></figure>
<p>pid是进程id（一般kill的时候用） ppid是父进程id  c是cpu占用率 其他看<a href="http://blog.csdn.net/lg632/article/details/52556139" target="_blank" rel="noopener">http://blog.csdn.net/lg632/article/details/52556139</a></p>
<h3 id="lsof查看文件被什么进程占用"><a href="#lsof查看文件被什么进程占用" class="headerlink" title="lsof查看文件被什么进程占用"></a>lsof查看文件被什么进程占用</h3><p>lsof=list open files<br><code>lsof 文件名</code>  查看某个文件被哪些进程在读写</p>
<h3 id="set-e-set-o-pipefail"><a href="#set-e-set-o-pipefail" class="headerlink" title="set -e/set -o pipefail"></a>set -e/set -o pipefail</h3><p><code>set -e</code> 表示一旦脚本中有命令的返回值为非0，则脚本立即退出，后续命令不再执行;<br><code>set -o pipefail</code> 表示在管道连接的命令序列中，只要有任何一个命令返回非0值，则整个管道返回非0值，即使最后一个命令返回0. </p>
<h3 id="解压纯-gz文件"><a href="#解压纯-gz文件" class="headerlink" title="解压纯 .gz文件"></a>解压纯 .gz文件</h3><p>方法（1）：gunzip 文件       方法（2）：zcat输出 之后用 &gt; 保存</p>
<h3 id="date"><a href="#date" class="headerlink" title="date"></a>date</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">date -d today +&quot;%Y-%m-%d&quot;   输出2017-12-08  按照格式“%Y-%m-%d”输出今天的日期。</span><br><span class="line">date +%Y-%m-%d --date&#x3D;&quot;-1 day&quot;  输出2017-12-07 按照格式“%Y-%m-%d”输出昨天的日期。 “-1”可以修改 “day”可以是month&#x2F;year。</span><br><span class="line">date -d &quot;last sunday&quot; +%Y-%m-%d   输出上周日的日期。</span><br><span class="line">date -d &quot;2018-03-12 1 days ago&quot; +%Y-%m-%d  输出2018-03-11.此办法可以得到字符串2018-03-12的前一天</span><br><span class="line">date --date&#x3D;&quot;2018-03-12 1 days ago&quot; +%Y-%m-%d  和上面那条效果一样。</span><br></pre></td></tr></table></figure>
<p>注意：<br>（1）加号“+”跟要输出的格式   -s 可以设置时间；<br>（2）最后两条的 <code>ago</code>可以省略，是加的效果；<br>（3）最后两条的<code>days</code>可以换成hours等；<br>（4）最后两条的<code>2018-03-12</code>可以省略，把时间默认成当前时间；<br>参考：<a href="http://www.linuxidc.com/Linux/2016-11/137559.htm" target="_blank" rel="noopener">http://www.linuxidc.com/Linux/2016-11/137559.htm</a></p>
<h3 id="if-ne-0-then"><a href="#if-ne-0-then" class="headerlink" title="if [ $? -ne 0 ];then"></a>if [ $? -ne 0 ];then</h3><p>$? -ne 0表示前面的命令的执行结果是不是0（0代表成功）   $?就是前面命令的执行结果。<br>另外切记，shell里的`` 是执行里面内容的命令。 -ne是不等时true； -eq是相等时true。</p>
<h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><p>sort 命令对 File 参数指定的文件中的行排序，并将结果写到标准输出。如果 File 参数指定多个文件，那么 sort 命令将这些文件连接起来，并当作一个文件进行排序。<br><code>-r</code> 从大到小<br><code>-n</code> 按照数字排序，否则可能会有<code>11 112 12</code>这样的结果<br><code>-k 2</code> 以第2列排序<br><code>-t 字符</code> 以字符分割，和<code>-k</code>一起用，可以自定义分割并排序.<br><code>-o</code> 指定输出文件，类似于重定向的效果<br><code>-f</code> 会将小写字母都转换为大写字母来进行比较，亦即忽略大小写<br><code>-c</code> 会检查文件是否已排好序，如果乱序，则输出第一个乱序的行的相关信息，最后返回1<br><code>-C</code> 会检查文件是否已排好序，如果乱序，不输出内容，仅返回1<br><code>-M</code> 会以月份来排序，比如JAN小于FEB等等<br><code>-b</code> 会忽略每一行前面的所有空白部分，从第一个可见字符开始比较。</p>
<p>eg：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">test.txt:</span><br><span class="line"></span><br><span class="line">banana:30:5.5</span><br><span class="line">apple:20:2.5</span><br><span class="line">pear:90:2.3</span><br><span class="line">orange:20:3.4</span><br></pre></td></tr></table></figure>
<p>第一列表示水果类型，第二列表示水果数量，第三列表示水果价格.<br>以水果数量排序 ：<code>sort -n -r -t : -k 2 test.txt</code></p>
<p>作用较大且复杂的是<code>-k</code>参数，参数有<code>[ FStart [ .CStart ] ] [ Modifier ] [ , [ FEnd [ .CEnd ] ][ Modifier ] ]</code><br>这个语法格式可以被其中的逗号（“，”）分为两大部分，Start部分和End部分。如果不设定End部分，那么就认为End被设定为行尾.<br>Start部分也由三部分组成，其中的Modifier部分就是我们之前说过的类似n和r的选项部分。我们重点说说Start部分的FStart和C.Start。<br>C.Start也是可以省略的，省略的话就表示从本域的开头部分开始。之前例子中的-k 2和-k 3就是省略了C.Start的例子喽。<br>FStart.CStart，其中FStart就是表示使用的域，而CStart则表示在FStart域中从第几个字符开始算“排序首字符”。</p>
<p>总结自：<a href="https://www.cnblogs.com/51linux/archive/2012/05/23/2515299.html" target="_blank" rel="noopener">https://www.cnblogs.com/51linux/archive/2012/05/23/2515299.html</a></p>
<h3 id="uniq"><a href="#uniq" class="headerlink" title="uniq"></a>uniq</h3><p>uniq命令可以去除排序过的文件中的重复行，因此uniq经常和sort合用。也就是说，为了使uniq起作用，所有的重复行必须是相邻的。</p>
<p>选项与参数：<br>-i   ：忽略大小写字符的不同；<br>-c  ：进行计数<br>-u  ：只显示唯一的行<br>13、例子sort+uniq</p>
<h3 id="经典！计算ip出现次数例子"><a href="#经典！计算ip出现次数例子" class="headerlink" title="经典！计算ip出现次数例子"></a>经典！计算ip出现次数例子</h3><p>文件：<br>178.60.128.31 <a href="http://www.google.com.hk" target="_blank" rel="noopener">www.google.com.hk</a><br>193.192.250.158 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>210.242.125.35 adwords.google.com<br>210.242.125.35 accounts.google.com.hk<br>210.242.125.35 accounts.google.com<br>210.242.125.35 accounts.l.google.com<br>64.233.181.49 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>212.188.10.167 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>23.239.5.106 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>64.233.168.41 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>62.1.38.89 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>62.1.38.89 chrome.google.com<br>193.192.250.172 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>212.188.10.241 <a href="http://www.google.com" target="_blank" rel="noopener">www.google.com</a><br>命令：<br>cat test.txt|awk ‘{print $1}’|sort|uniq -c<br>参考：<a href="http://james-lover.iteye.com/blog/2105795" target="_blank" rel="noopener">http://james-lover.iteye.com/blog/2105795</a></p>
<h3 id="dir-cd-dirname-0-pwd-P"><a href="#dir-cd-dirname-0-pwd-P" class="headerlink" title="dir=$( cd $(dirname $0) ; pwd -P )"></a>dir=$( cd $(dirname $0) ; pwd -P )</h3><p>得到当前运行脚本实际物理地址，而非链接地址。<br>$0就是当前运行脚本；<br><code>dirname</code>得到指定脚本所在的目录，在执行时相对的路径。<br><code>pwd 的-P</code> 得到的实际物理地址，不是连接的地址；-L是链接地址，而非物理地址。<br><code>basename</code> 是去除目录后剩下的名字.</p>
<h3 id="1-gt-dev-null-2-gt-amp-1"><a href="#1-gt-dev-null-2-gt-amp-1" class="headerlink" title="1&gt;/dev/null 2&gt;&amp;1"></a>1&gt;/dev/null 2&gt;&amp;1</h3><p>（1）1&gt;/dev/null 首先表示标准输出重定向到空设备文件，也就是不输出任何信息到终端，说白了就是不显示任何信息。<br>（2）2&gt;&amp;1 接着，标准错误输出重定向等同于 标准输出，因为之前标准输出已经重定向到了空设备文件，所以标准错误输出也重定向到空设备文件。</p>
<h3 id="cat-test-gt-test-txt"><a href="#cat-test-gt-test-txt" class="headerlink" title="cat /test/* &gt; test.txt"></a>cat /test/* &gt; test.txt</h3><p>把test目录下的文件以行的方式追加到test.txt文件中。</p>
<h3 id="ssh转接端口"><a href="#ssh转接端口" class="headerlink" title="ssh转接端口"></a>ssh转接端口</h3><p>通过转接端口访问其他机器数据库<br><code>ssh -L host1:port1:host2:port2 host3</code>命令必须在host1上执行，host3必须有sshd<br>-f 后台运行<br>-N 不开shell<br>-T 不分配tty<br><a href="http://mingxinglai.com/cn/2015/09/connect-mysql-via-ssh-tunnel/" target="_blank" rel="noopener">http://mingxinglai.com/cn/2015/09/connect-mysql-via-ssh-tunnel/</a></p>
<h3 id="多行注释"><a href="#多行注释" class="headerlink" title="多行注释"></a>多行注释</h3><p>使用:&lt;&lt;BLOCK 和 BLOCK。BLOCK是任意字符串。<br>更多参考：<a href="http://www.jb51.net/article/52377.htm" target="_blank" rel="noopener">http://www.jb51.net/article/52377.htm</a><br>反引号就是1旁边的键。</p>
<h3 id="报错：变量与空格-too-many-arguments"><a href="#报错：变量与空格-too-many-arguments" class="headerlink" title="报错：变量与空格[: too many arguments"></a>报错：变量与空格[: too many arguments</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ret="Peter Anne"</span><br><span class="line">if [ $ret == "Peter Anne" ]; then</span><br><span class="line">  echo "pass"</span><br><span class="line">else</span><br><span class="line">  echo "failed"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
<p>这段脚本会报错<code>变量与空格[: too many arguments</code> 。原因：<br><code>if [ $ret == &quot;Peter Anne&quot; ];</code>它的参数分别为<code>[，$ret， ==，&quot;Peter Anne&quot;，]</code>，一共5个参数。（<code>[</code>也是被当作参数，这就是为什么<code>[</code>一定要有空格的缘故。<br>如果正常5个参数，是没有问题的，但是问题出在了$ret变量里。<br>在Linux系统中的真实解析是，<code>if [ Peter Anne == &quot;Peter Anne&quot; ]</code>，参数则分为：<code>[，Peter，Anne， ==，&quot;Peter Anne&quot;，]</code>，一共6个参数，这时就会报上面的错。<br>解决办法是<code>if [ &quot;${ret}&quot; == &quot;Peter Anne&quot; ]</code>。</p>
<p>参考自：<a href="https://blog.csdn.net/qq_22520587/article/details/62455740" target="_blank" rel="noopener">https://blog.csdn.net/qq_22520587/article/details/62455740</a></p>
<h3 id="eval-command-line"><a href="#eval-command-line" class="headerlink" title="eval command-line"></a>eval command-line</h3><p>其中<code>command－line</code>是在终端上键入的一条普通命令行。然而当在它前面放上eval时，其结果是shell在执行命令行之前扫描它两次。如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pipe&#x3D;&quot;|&quot;</span><br><span class="line">eval ls $pipe wc -l</span><br></pre></td></tr></table></figure>
<p>shell第1次扫描命令行时，它替换出pipe的值｜，接着eval使它再次扫描命令行，这时shell把｜作为管道符号了。<br>如果变量中包含任何需要shell直接在命令行中看到的字符（不是替换的结果），就可以使用eval。命令行结束符<code>（</code> <code>；</code> <code>｜</code> <code>&amp;</code> <code>）</code> ，I／o重定向符（&lt; &gt;）和引号就属于对shell具有特殊意义的符号，必须直接出现在命令行中。<br>eval其他用法参考：<a href="http://blog.51cto.com/363918/1341977" target="_blank" rel="noopener">http://blog.51cto.com/363918/1341977</a></p>
<h3 id="grep"><a href="#grep" class="headerlink" title="grep"></a>grep</h3><p><code>grep -v</code>过滤掉某些的数据<br><code>grep -C 5 foo file</code> 显示file文件中匹配foo字串那行以及上下5行<br><code>grep -B 5 foo file</code> 显示foo及前5行<br><code>grep -A 5 foo file</code> 显示foo及后5行</p>
<p>grep 参数：<a href="http://man.linuxde.net/grep" target="_blank" rel="noopener">http://man.linuxde.net/grep</a></p>
<h3 id=""><a href="#" class="headerlink" title="$!"></a>$!</h3><p><code>$!</code> Shell最后运行的后台Process的PID<br>wait PID || exit 1</p>
<h3 id="递归查文件个数"><a href="#递归查文件个数" class="headerlink" title="递归查文件个数"></a>递归查文件个数</h3><p><code>ls -lR ./order/ ./user/|grep &quot;^-&quot;|wc -l</code><br>递归查普通文件个数<br><code>-h</code> 以M格式看文件大小</p>
<h3 id="drwxrwxrwx"><a href="#drwxrwxrwx" class="headerlink" title="drwxrwxrwx"></a>drwxrwxrwx</h3><p>文件夹的所有者 所属组 其他人对这个文件夹的权限</p>
<h3 id="逐行读文件"><a href="#逐行读文件" class="headerlink" title="逐行读文件"></a>逐行读文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat testdata | <span class="keyword">while</span> <span class="built_in">read</span> line</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$line</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<p>其他方法：<a href="https://www.cnblogs.com/DengGao/p/5935688.html" target="_blank" rel="noopener">https://www.cnblogs.com/DengGao/p/5935688.html</a></p>
<h3 id="函数返回字符串"><a href="#函数返回字符串" class="headerlink" title="函数返回字符串"></a>函数返回字符串</h3><p>shell脚本的return只能返回数值类型，可是我们很多时候想返回字符串。在函数中直接用echo 字符串，在调用时就可以得到返回字符串。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">function getStr ()</span><br><span class="line">&#123;</span><br><span class="line">    String="very good"</span><br><span class="line">    echo $String</span><br><span class="line">&#125;</span><br><span class="line">str=$(getStr)</span><br><span class="line">echo $str</span><br></pre></td></tr></table></figure>
<p>结果：<code>very good</code></p>
<h3 id="-1"><a href="#-1" class="headerlink" title=""></a></h3><p>常用参看：<a href="https://www.cnblogs.com/yu2000/p/4089011.html" target="_blank" rel="noopener">https://www.cnblogs.com/yu2000/p/4089011.html</a><br>shell数值比较：<a href="https://www.cnblogs.com/happyhotty/articles/2065412.html" target="_blank" rel="noopener">https://www.cnblogs.com/happyhotty/articles/2065412.html</a><br>read:<a href="https://blog.csdn.net/u012359618/article/details/52329346" target="_blank" rel="noopener">https://blog.csdn.net/u012359618/article/details/52329346</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>maven常用</title>
    <url>/2018/04/20/mvn%E5%B8%B8%E7%94%A8/</url>
    <content><![CDATA[<p>mvn –version == mvn -v<br>mvn archetype:generate 自动生成mvn项目</p>
<p><strong><em>使用archetype插件生成一个webapp项目：</em></strong><br>mvn archetype:generate -DgroupId=com.fresh -DartifactId=mytestapp -DarchetypeArtifactId=maven-archetype-webapp -DinteractiveMode=false</p>
<p><code>mvn clean</code> <strong><em>清除项目</em></strong><br><code>mvn compile</code> <strong><em>编译代码</em></strong><br><code>mvn test-compile</code> <strong><em>只编译测试代码</em></strong><br><code>mvn test</code> <strong><em>运行单元测试代码</em></strong>（编译源代码、测试代码；运行测试代码）<br><code>mvn package</code> 打包成jar/war(根据pom文件packaging定)文件，在target文件下<br><code>mvn install</code> <strong><em>部署到本地仓库</em></strong><br><code>mvn deploy</code> 在远程仓库中安装jar（一般不用）</p>
<p><code>mvn clean install -Dmaven.test.skip=true</code> <strong><em>单元测试逻辑不编译，不执行，直接跳过</em></strong><br><code>mvn clean install -DskipTests</code> <strong><em>单元测试编译，只跳过测试过程</em></strong></p>
<p><code>mvn help:effective-pom</code> 查看依赖的父pom<br><code>mvn dependency:tree</code> <strong><em>查看pom依赖树，解决pom依赖问题</em></strong></p>
<p><code>mvn dependency:analyze</code>  <strong><em>分析pom依赖</em></strong><br><code>mvn clean package -Pdev -Dmaven.test.skip=true</code> <strong><em>过滤掉test指定环境打包</em></strong></p>
<p><code>mvn clean tomcat7:run -Pdev</code> <strong><em>使用tomcat指定环境启动项目</em></strong></p>
<p><strong>scrop 规定jar包的作用域</strong><br>1.<code>compile</code>（默认） 在编译、测试、运行时均需要使用此依赖，表示编译打包之后这个jar包会放在编译后的文件里；<br>2.<code>provided</code> 编译时有用，但是打包后不存在。对于编译和测试有效，但在运行时无效。常见的是servlet，servlet的实现在容器里（tomcat/jetty），这个在运行时会由你的 tomcat 、jboss 、jetty等容器来提供，但在编译和测试阶段你仍是需要这个 jar 包的；<br>3.<code>runtime</code> 运行时使用，写代码时不能使用。常见为日志、mysql driver<br>4.<code>test</code> 正常不能用，只用在用test时才能用，如junit。<br>5.<code>import</code> 内部组件，公共pom，pom中定义了很多jar版本号</p>
<p><strong>解决依赖冲突</strong><br>mvn自带解决依赖冲突：1、依赖最近者优先。2、路径相同，先声明的优先。<br>但是最好能自己指定：<br>1、<code>dependencyManegement</code> 解决包依赖冲突（首选办法），指定优选包版本<br>2、<code>exclusion</code> 解决包依赖冲突（加在不用的那个dependency里）。有一些公司规定不让使用某些包，如日志输出上不能用commons-logging</p>
<p>生命周期<br>clean Pre-clean（准备） Clean（移除） Post-clean 会以此执行上面的命令<br>default 构建的所有步骤 23个阶段 process-resources 资源文件的准备<br>site 站点（）Pre-site site Post-site site-deploy 项目的描述和使用指南</p>
<p>groupid 项目的唯一标示<br>ati。。。 项目组唯一id<br>version snapshot还在开发，快照；Release 稳定版本<br><strong><em>finalname build中，，默认ROOT（最终的打包名字，，，）</em></strong><br><strong><em>profile 选择环境</em></strong></p>
<p><strong>本地仓库</strong> 默认就在.m2/repository/ 设置在setting.xml ！！！！<br>中央仓库 mvn/lib/maven-model-builder-3.jar,org/apache/macen/model/pom-4.0.0.xml</p>
<p>type 依赖类型（默认jar包）<br>scope 依赖范围<br>optional 依赖是否可选<br>exclusions 依赖冲突解决</p>
<modules>
<packaging>pom</ 使用一个命令进行多个模块的聚合

<p>groupid 部门名称 com.fresh.项目组名称<br>arti… 项目名<br>mvn package -Plocal profile用local<br>mvn clean package -Pdev<br>mvn clean package 工程打包<br>mvn enforcer:enforcer 检查重复类<br>mvn dependency:tree -Dverbose 具体看重复类的位置</p>
<p><a href="https://stackoverflow.com/questions/24827194/maven-enforcer-plugin-missing-or-invalid-rules" target="_blank" rel="noopener">https://stackoverflow.com/questions/24827194/maven-enforcer-plugin-missing-or-invalid-rules</a></p>
]]></content>
      <categories>
        <category>开发常用工具</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title>maven踩坑总结</title>
    <url>/2018/04/20/maven%E8%B8%A9%E5%9D%91%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>1.关于mvn的使用总结。<br>（1）把每个子模块中要使用的依赖都放在父pom的dependencyManagement中，所有version放在properties中。<br>（2）把几乎每个子模块都需要的依赖包放在父pom的dependencies中，不需要version。如，junit，slf4j等<br>（3）每个子模块单独需要的放在该子模块的pom中，不要version。<br>（4）spring中的一些包是互相包含的，但是在项目中要把所有的spring包都写上，写在dependencyManagement中。</p>
<p>（4）的原因：如果父pom中有spring，我们的项目中也加了spring的包，这个时候如果我们只引用了部分的spring包，然后这个部分的spring包包括了其他的spring包，这个其他的spring包如果在dependencyManagement没有指定版本就可能会导致spring包版本不一致。<br>如context自动导入aop，而且dependencyManagement没有指定aop版本，项目会去上一层dependencyManagement（即父pom）中找aop版本，这个时候就可能导致aop包的版本冲突（父pom的aop版本和引入的context包含的aop版本版本不一样）。</p>
<p>（5）某些情景中（比如公司强制）强制不让引入commons-logging包,但是spring-core中默认会导入commons-logging包。<br>SpringJUnit4ClassRunner中使用了commons-logging中的类，如果直接用exclusions，会抛出异常。<br>解决办法，加入依赖包：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jcl-over-slf4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;jcl.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>这个依赖包是一个桥接工具，可以把原本的log重定向到slf4j，其他类似的包还有log4j-over-slf4j等。<br>如果同时使用了jcl-over-slf4j和slf4j-jcl的话,会导致jcl代理给slf4j,slf4j又绑定到jcl,就形成了一个死循环,抛出StackOverFlow异常。</p>
]]></content>
      <categories>
        <category>开发常用工具</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title>sed</title>
    <url>/2018/04/20/sed/</url>
    <content><![CDATA[<p>sed文本处理，可以配合正则使用。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出（除了-i命令）。<br>简化对文件的反复操作。</p>
<h1 id="定界符"><a href="#定界符" class="headerlink" title="定界符"></a>定界符</h1><p>可以使用任意定界符，如/，：，|等。<br>定界符出现在内部时，用\转义。如把/bin替换成/bin/local：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;s&#x2F;\&#x2F;bin&#x2F;\&#x2F;bin\&#x2F;local&#x2F;&#39;</span><br></pre></td></tr></table></figure>
<h1 id="test-txt"><a href="#test-txt" class="headerlink" title="test.txt"></a>test.txt</h1><p>以下的命令都基于文件 test.txt</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Tom　　 2012-12-11 car 53000</span><br><span class="line">John　　 2013-01-13 bike 41000</span><br><span class="line">vivi 2013-01-18 car 42800</span><br><span class="line">Tom　　 2013-01-20 car 32500</span><br><span class="line">John　　 2013-01-28 bike 63500</span><br></pre></td></tr></table></figure>
<h1 id="s-替换命令"><a href="#s-替换命令" class="headerlink" title="s 替换命令"></a>s 替换命令</h1><p>把‘bike’替换成‘train’</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;s&#x2F;bike&#x2F;train&#x2F;&#39; test.txt</span><br><span class="line">输出：</span><br><span class="line">Tom　　 2012-12-11 car 53000</span><br><span class="line">John　　 2013-01-13 train 41000</span><br><span class="line">vivi 2013-01-18 car 42800</span><br><span class="line">Tom　　 2013-01-20 car 32500</span><br><span class="line">John　　 2013-01-28 train 63500</span><br></pre></td></tr></table></figure>
<h1 id="n-和-p一起只打印发生替换的行。-p是输出的功能。"><a href="#n-和-p一起只打印发生替换的行。-p是输出的功能。" class="headerlink" title="-n 和 p一起只打印发生替换的行。 p是输出的功能。"></a>-n 和 p一起只打印发生替换的行。 p是输出的功能。</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -n &#39;s&#x2F;bike&#x2F;train&#x2F;p&#39; test.txt</span><br><span class="line">输出：</span><br><span class="line">John　　 2013-01-13 train 41000</span><br><span class="line">John　　 2013-01-28 train 63500</span><br></pre></td></tr></table></figure>
<h1 id="i会直接编辑原文件并保存"><a href="#i会直接编辑原文件并保存" class="headerlink" title="-i会直接编辑原文件并保存"></a>-i会直接编辑原文件并保存</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -i &#39;s&#x2F;bike&#x2F;train&#x2F;g&#39; test.txt </span><br><span class="line">cat test.txt 结果：</span><br><span class="line">Tom　　 2012-12-11 car 53000</span><br><span class="line">John　　 2013-01-13 train 41000</span><br><span class="line">vivi    2013-01-18 car 42800</span><br><span class="line">Tom　　 2013-01-20 car 32500</span><br><span class="line">John　　 2013-01-28 train 63500</span><br></pre></td></tr></table></figure>
<h1 id="g会替换每一行所有-Ng可以直接从匹配到的第N个开始替换。"><a href="#g会替换每一行所有-Ng可以直接从匹配到的第N个开始替换。" class="headerlink" title="g会替换每一行所有 Ng可以直接从匹配到的第N个开始替换。"></a>g会替换每一行所有 Ng可以直接从匹配到的第N个开始替换。</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;s&#x2F;0&#x2F;7&#x2F;3g&#39; test.txt</span><br><span class="line">输出：</span><br><span class="line">Tom　　 2012-12-11 car 53077</span><br><span class="line">John　　 2013-01-13 train 41777</span><br><span class="line">vivi 2013-01-18 car 42877</span><br><span class="line">Tom　　 2013-01-27 car 32577</span><br><span class="line">John　　 2013-01-28 train 63577</span><br></pre></td></tr></table></figure>
<h1 id="d删除-Nd删除第N行-N，Md删除第N到M行（-是以…开头，-是以…结尾）"><a href="#d删除-Nd删除第N行-N，Md删除第N到M行（-是以…开头，-是以…结尾）" class="headerlink" title="d删除 Nd删除第N行  N，Md删除第N到M行（^是以…开头，$是以…结尾）"></a>d删除 Nd删除第N行  N，Md删除第N到M行（^是以…开头，$是以…结尾）</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;&#x2F;^vivi&#x2F;d&#39; test.txt #搜索以vivi开头的行并删除</span><br><span class="line">输出：</span><br><span class="line">Tom　　 2012-12-11 car 53000</span><br><span class="line">John　　 2013-01-13 train 41000</span><br><span class="line">Tom　　 2013-01-20 car 32500</span><br><span class="line">John　　 2013-01-28 train 63500</span><br><span class="line">sed &#39;&#x2F;500$&#x2F;d&#39; .&#x2F;test.txt #搜索以500结尾的行并删除</span><br><span class="line">输出：</span><br><span class="line">Tom　　  2012-12-11      car     53000</span><br><span class="line">John　　 2013-01-13      train    41000</span><br><span class="line">vivi    2013-01-18      car     42800</span><br></pre></td></tr></table></figure>
<h1 id="amp-表示已匹配字符串"><a href="#amp-表示已匹配字符串" class="headerlink" title="&amp; 表示已匹配字符串"></a>&amp; 表示已匹配字符串</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;s&#x2F;car\|train&#x2F;[&amp;]&#x2F;g&#39; test.txt</span><br><span class="line">输出：</span><br><span class="line">Tom　　 2012-12-11 [car] 53000</span><br><span class="line">John　　 2013-01-13 [train] 41000</span><br><span class="line">vivi 2013-01-18 [car] 42800</span><br><span class="line">Tom　　 2013-01-20 [car] 32500</span><br><span class="line">John　　 2013-01-28 [train] 63500</span><br><span class="line">#sed &#39;s&#x2F;\w\+&#x2F;[&amp;]&#x2F;g&#39; test.txt</span><br><span class="line">sed &#39;s&#x2F;\w\+&#x2F;[&amp;]&#x2F;g&#39; test.txt</span><br><span class="line">输出：</span><br><span class="line">[Tom]　　 [2012]-[12]-[11] [car] [53000]</span><br><span class="line">[John]　　 [2013]-[01]-[13] [train] [41000]</span><br><span class="line">[vivi] [2013]-[01]-[18] [car] [42800]</span><br><span class="line">[Tom]　　 [2013]-[01]-[20] [car] [32500]</span><br><span class="line">[John]　　 [2013]-[01]-[28] [train] [63500]</span><br></pre></td></tr></table></figure>
<h1 id="1子串匹配"><a href="#1子串匹配" class="headerlink" title="\1子串匹配"></a>\1子串匹配</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;s&#x2F;2\([0-9]\)&#x2F;\1&#x2F;&#39; test.txt    #\1是正则括号匹配的第一个</span><br><span class="line">输出：</span><br><span class="line">Tom　　 012-12-11 car 53000</span><br><span class="line">John　　 013-01-13 train 41000</span><br><span class="line">vivi 013-01-18 car 42800</span><br><span class="line">Tom　　 013-01-20 car 32500</span><br><span class="line">John　　 013-01-28 train 63500</span><br><span class="line"></span><br><span class="line">sed &#39;s&#x2F;2\([0-9]\)\([0-9]\)&#x2F;\2\1&#x2F;&#39; test.txt  #把匹配到的第一个和第二个位置交换</span><br><span class="line">输出：</span><br><span class="line">Tom　　  102-12-11      car     53000</span><br><span class="line">John　　 103-01-13      train    41000</span><br><span class="line">vivi    103-01-18      car     42800</span><br><span class="line">Tom　　  103-01-20      car     32500</span><br><span class="line">John　　 103-01-28      train    63500</span><br><span class="line"></span><br><span class="line">sed &#39;s&#x2F;2\([0-9]\)1\([0-9]\)&#x2F;2\21\1&#x2F;&#39; test.txt</span><br><span class="line">输出：</span><br><span class="line">Tom　　 2210-12-11 car 53000</span><br><span class="line">John　　 2310-01-13 train 41000</span><br><span class="line">vivi 2310-01-18 car 42800</span><br><span class="line">Tom　　 2310-01-20 car 32500</span><br><span class="line">John　　 2310-01-28 train 63500</span><br></pre></td></tr></table></figure>
<h1 id="引用shell变量"><a href="#引用shell变量" class="headerlink" title="引用shell变量"></a>引用shell变量</h1><h1 id="sed表达式可以使用单引号，但是表达式中有引用字符串（即下面的-test）时就必须用双引号。"><a href="#sed表达式可以使用单引号，但是表达式中有引用字符串（即下面的-test）时就必须用双引号。" class="headerlink" title="sed表达式可以使用单引号，但是表达式中有引用字符串（即下面的$test）时就必须用双引号。"></a>sed表达式可以使用单引号，但是表达式中有引用字符串（即下面的$test）时就必须用双引号。</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line"></span><br><span class="line">test&#x3D;hello</span><br><span class="line">echo hello | sed &quot;s&#x2F;$test&#x2F;HELLO&#x2F;&quot;</span><br></pre></td></tr></table></figure>
<h1 id="逗号-分割行范围"><a href="#逗号-分割行范围" class="headerlink" title="逗号 分割行范围"></a>逗号 分割行范围</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -n &#39;2,&#x2F;^Tom&#x2F;p&#39; test.txt   #输出第2行到以Tom开头的行（没有Tom就到文件结尾）</span><br><span class="line">输出：</span><br><span class="line">John　　 2013-01-13 train 41000</span><br><span class="line">vivi    2013-01-18 car 42800</span><br><span class="line">Tom　　 2013-01-20 car 32500</span><br><span class="line"></span><br><span class="line">sed &#39;&#x2F;^John&#x2F;,&#x2F;^vivi&#x2F;s&#x2F;$&#x2F; add&#x2F;&#39; test.txt #从以John开头的行到以vivi开头的行 结尾加add（因为s&#x2F;$&#x2F; add，s是替换，$是以...为结尾）</span><br><span class="line">输出：</span><br><span class="line">Tom　　 2012-12-11 car 53000</span><br><span class="line">John　　 2013-01-13 train 41000 add</span><br><span class="line">vivi    2013-01-18 car 42800 add</span><br><span class="line">Tom　　 2013-01-20 car 32500</span><br><span class="line">John　　 2013-01-28 train 63500 add</span><br><span class="line"></span><br><span class="line">sed -e &#39;1,3d&#39; -e &#39;s&#x2F;car&#x2F;train&#x2F;&#39; test.txt  #删除第1到3行，剩下的用car替换train。</span><br><span class="line">输出：</span><br><span class="line">Tom　　 2013-01-20 train 32500</span><br><span class="line">John　　 2013-01-28 train 63500</span><br></pre></td></tr></table></figure>
<h1 id="e-同一行里执行多条命令"><a href="#e-同一行里执行多条命令" class="headerlink" title="-e 同一行里执行多条命令"></a>-e 同一行里执行多条命令</h1><p>#（更高级的还可以用分号；分割多个命令）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -e &#39;1,3d&#39; -e &#39;s&#x2F;car&#x2F;train&#x2F;&#39; test.txt</span><br><span class="line">输出：</span><br><span class="line">Tom　　 2013-01-20 train 32500</span><br><span class="line">John　　 2013-01-28 train 63500</span><br></pre></td></tr></table></figure>
<h1 id="w-写文件"><a href="#w-写文件" class="headerlink" title="w 写文件"></a>w 写文件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -n &#39;&#x2F;^vivi&#x2F;w test1.txt&#39; test.txt   #把test.txt中以vivi开头的行写到test1.txt中</span><br><span class="line">结果：</span><br><span class="line">cat test1.txt</span><br><span class="line">vivi 2013-01-18 car 42800</span><br></pre></td></tr></table></figure>
<h1 id="r-读文件"><a href="#r-读文件" class="headerlink" title="r 读文件"></a>r 读文件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;&#x2F;^vivi&#x2F;r test.txt&#39; test1.txt     #匹配test1.txt中以vivi开头的行，把test.txt的内容跟在后面</span><br><span class="line">输出：</span><br><span class="line">vivi 2013-01-18 car 42800</span><br><span class="line">Tom　　 2012-12-11 car 53000</span><br><span class="line">John　　 2013-01-13 train 41000</span><br><span class="line">vivi 2013-01-18 car 42800</span><br><span class="line">Tom　　 2013-01-20 car 32500</span><br><span class="line">John　　 2013-01-28 train 63500</span><br></pre></td></tr></table></figure>
<h1 id="a-追加文件"><a href="#a-追加文件" class="headerlink" title="a 追加文件"></a>a 追加文件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;&#x2F;^vivi&#x2F;a\this is vivi&#39; test.txt   #把‘this is vivi’追加到test.txt中以vivi开头的行后面</span><br><span class="line">Tom　　 2012-12-11 car 53000</span><br><span class="line">John　　 2013-01-13 train 41000</span><br><span class="line">vivi    2013-01-18 car 42800</span><br><span class="line">this is vivi</span><br><span class="line">Tom　　 2013-01-20 car 32500</span><br><span class="line">John　　 2013-01-28 train 63500</span><br></pre></td></tr></table></figure>
<h1 id="i-追加到行前面"><a href="#i-追加到行前面" class="headerlink" title="i 追加到行前面"></a>i 追加到行前面</h1><p>（如果要直接修改原文件，就写上-i）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;&#x2F;^vivi&#x2F;i this is vivi&#39; test.txt</span><br><span class="line">输出</span><br><span class="line">Tom　　 2012-12-11 car 53000</span><br><span class="line">John　　 2013-01-13 train 41000</span><br><span class="line">this is vivi</span><br><span class="line">vivi          2013-01-18 car 42800</span><br><span class="line">Tom　　 2013-01-20 car 32500</span><br><span class="line">John　　 2013-01-28 train 63500</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sed -i &#39;&#x2F;^vivi&#x2F;i this is vivi&#39; test.txt  #没有输出，但是直接修改了test.txt文件</span><br></pre></td></tr></table></figure>
<h1 id="n-匹配到换行"><a href="#n-匹配到换行" class="headerlink" title="n;  匹配到换行"></a>n;  匹配到换行</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;&#x2F;vivi&#x2F;&#123;n; s&#x2F;car&#x2F;bike&#x2F;&#125;&#39; test.txt   #匹配到vivi后换行用bike替换car</span><br><span class="line">输出：</span><br><span class="line">Tom　　 2012-12-11 car 53000</span><br><span class="line">John　　 2013-01-13 train 41000</span><br><span class="line">vivi 2013-01-18 car 42800</span><br><span class="line">Tom　　 2013-01-20 bike 32500</span><br><span class="line">John　　 2013-01-28 train 63500</span><br></pre></td></tr></table></figure>
<h1 id="y一对一替换"><a href="#y一对一替换" class="headerlink" title="y一对一替换"></a>y一对一替换</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;1,10y&#x2F;abcr&#x2F;ABCR&#x2F;&#39; test.txt   #所有的abcr字符都替换成对应大写</span><br><span class="line">输出：</span><br><span class="line">Tom　　 2012-12-11 CAR 53000</span><br><span class="line">John　　 2013-01-13 tRAin 41000</span><br><span class="line">vivi 2013-01-18 CAR 42800</span><br><span class="line">Tom　　 2013-01-20 CAR 32500</span><br><span class="line">John　　 2013-01-28 tRAin 63500</span><br></pre></td></tr></table></figure>
<h1 id="q-退出"><a href="#q-退出" class="headerlink" title="q 退出"></a>q 退出</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &#39;2q&#39; test.txt  #输出两行后退出</span><br><span class="line">输出：</span><br><span class="line">Tom　　 2012-12-11 car 53000</span><br><span class="line">John　　 2013-01-13 train 41000</span><br></pre></td></tr></table></figure>
<h1 id="常用例子"><a href="#常用例子" class="headerlink" title="常用例子"></a>常用例子</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep oldString -rl &#x2F;path | xargs sed -i &quot;s&#x2F;oldString&#x2F;newString&#x2F;g&quot;    #替换一个目录下的所有文件中字符串oldString成newString</span><br></pre></td></tr></table></figure>

<h1 id="保存sed处理结果的方法"><a href="#保存sed处理结果的方法" class="headerlink" title="保存sed处理结果的方法"></a>保存sed处理结果的方法</h1><p>可以用-i命令（比较危险）或者&gt;重定向到另一个文件或者用w命令</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>sendmail</title>
    <url>/2018/04/20/sendmail/</url>
    <content><![CDATA[<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install sendmail  </span><br><span class="line">sudo apt-get install sendmail-cf</span><br></pre></td></tr></table></figure>
<p>buntu下使用最常用的mail功能，需要安装mailutils，<br>安装命令：<code>sudo apt-get install mailutils</code></p>
<h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><p>sendmail 默认只会为本机用户发送邮件，只有把它扩展到整个Internet，才会成为真正的邮件服务器。</p>
<p>打开sendmail的配置宏文件：/etc/mail/sendmail.mc<br> <code>vi  /etc/mail/sendmail.mc</code><br>找到如下行修改Addr=0.0.0.0，表明可以连接到任何服务器。： </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DAEMON_OPTIONS(&#96;Family&#x3D;inet,  Name&#x3D;MTA-v4, Port&#x3D;smtp, Addr&#x3D;127.0.0.1&#39;)dnl</span><br></pre></td></tr></table></figure>
<p>生成新的配置文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;etc&#x2F;mail  </span><br><span class="line">mv sendmail.cf sendmail.cf~      &#x2F;&#x2F;做一个备份  </span><br><span class="line">mv sendmail.mc &gt; sendmail.cf   &#x2F;&#x2F;&gt;的左右有空格，提示错误没有安装sendmail-cf</span><br></pre></td></tr></table></figure>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>中间发邮件一直发不出去（也有可能是发送太慢了），试了几种办法，最后不知道哪种达到效果.感觉是第二种，因为收到的邮件会显示由“XXX”代发，XXX就是主机名。<br>（1）执行： <code>/etc/init.d/sendmail start</code><br>Starting sendmail: [ OK ] Starting sm-client: [ OK ]</p>
<p>（2）host查看主机名<br>修改host文件：vim /etc/hosts<br>原始的可能是： 127.0.0.1 localhost<br>改成： 127.0.0.1  localhost XXX(主机名)</p>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>（1）mail youtoemail   之后回车，会提示Cc填抄送人；subject填主题；填完主题，回车写消息体body，写完ctrl+d再回车就可以发送了。<br>这种操作的前提时 必须有前面安装部分的 第三条指令。<br>还可以，mail -s 邮件标题 收件方邮箱 &lt; content.txt           content.txt放发送邮件的body。<br>（2）sendmail.  写一个文件，如email.tx：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">To: XXX@qq.com</span><br><span class="line">CC: XXXX@qq.com</span><br><span class="line">From: handy&lt;XXX@qq.com&gt;</span><br><span class="line">Subject: test</span><br><span class="line"></span><br><span class="line">hello world!</span><br></pre></td></tr></table></figure>
<p>然后执行cat email.txt | sendmail -t就可以发送邮件，这种邮件会显示发送方的邮件名。<br>sendmail另一种使用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;content test&quot; | sendmail  XXX@qq.com   </span><br><span class="line"># 发送内容是 &quot;content test&quot;到XXX@qq.com  没有发件人，没有邮箱标题。</span><br></pre></td></tr></table></figure>

<p>sendmail的更多用法参考：<a href="http://blog.csdn.net/kevinew/article/details/9147969" target="_blank" rel="noopener">http://blog.csdn.net/kevinew/article/details/9147969</a><br>参考：<a href="http://blog.csdn.net/xin_yu_xin/article/details/45115723" target="_blank" rel="noopener">http://blog.csdn.net/xin_yu_xin/article/details/45115723</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>spring常用注解</title>
    <url>/2018/04/20/spring%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="接受前端数据类"><a href="#接受前端数据类" class="headerlink" title="接受前端数据类"></a>接受前端数据类</h2><h3 id="PathVariabl"><a href="#PathVariabl" class="headerlink" title="@PathVariabl"></a>@PathVariabl</h3><p>获取路径中传递参数 ，eg：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/&#123;id&#125;/&#123;str&#125;"</span>) </span><br><span class="line"><span class="function"><span class="keyword">public</span> ModelAndView <span class="title">helloWorld</span><span class="params">(@PathVariable String id,  @PathVariable String str)</span> </span>&#123; &#125;</span><br></pre></td></tr></table></figure>
<h3 id="ModelAttribute"><a href="#ModelAttribute" class="headerlink" title="@ModelAttribute"></a>@ModelAttribute</h3><p>获取POST请求的FORM表单数据 。（实际是，不用@ModelAttribute也可以接收到数据）eg：<br>JSP</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;form method=<span class="string">"post"</span> action=<span class="string">"hao.do"</span>&gt; </span><br><span class="line">a: &lt;input id=<span class="string">"a"</span> type=<span class="string">"text"</span>   name=<span class="string">"a"</span>/&gt; </span><br><span class="line">b: &lt;input id=<span class="string">"b"</span> type=<span class="string">"text"</span>   name=<span class="string">"b"</span>/&gt; </span><br><span class="line">&lt;input type=<span class="string">"submit"</span> value=<span class="string">"Submit"</span> /&gt; </span><br><span class="line">&lt;/form&gt;</span><br></pre></td></tr></table></figure>
<p>JAVA pojo</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Pojo</span></span>&#123; </span><br><span class="line">     <span class="keyword">private</span> String a; </span><br><span class="line">     <span class="keyword">private</span> <span class="keyword">int</span> b;</span><br></pre></td></tr></table></figure>
<p>JAVA controller</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RequestMapping</span>(method = RequestMethod.POST) </span><br><span class="line">     <span class="function"><span class="keyword">public</span> String <span class="title">processSubmit</span><span class="params">(@ModelAttribute(<span class="string">"pojo"</span>)</span> Pojo pojo) </span>&#123; </span><br><span class="line">         <span class="keyword">return</span> <span class="string">"helloWorld"</span>; </span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure>
<h3 id="RequestParam"><a href="#RequestParam" class="headerlink" title="@RequestParam"></a>@RequestParam</h3><p>get请求。绑定请求参数a到变量a 。<br>解惑：为何不用这个注解也可以接收到参数，那还加这个注解有什么用。<br>例1： </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/requestParam"</span>, method = RequestMethod.GET) </span><br><span class="line"> <span class="function"><span class="keyword">public</span> String <span class="title">setupForm</span><span class="params">( String a, ModelMap model)</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<p>例2：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/requestParam"</span>, method = RequestMethod.GET) </span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">setupForm</span><span class="params">(@RequestParam String a, ModelMap model)</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<p>例3：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/requestParam"</span>, method = RequestMethod.GET) </span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">setupForm</span><span class="params">(@RequestParam&#123;required=<span class="keyword">false</span>&#125; String a, ModelMap model)</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<p>例4：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/requestParam"</span>, method = RequestMethod.GET) </span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">setupForm</span><span class="params">(@RequestParam&#123;defaultValue=<span class="string">"0"</span>&#125; String a, ModelMap model)</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<p>例5：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/requestParam"</span>, method = RequestMethod.GET) </span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">setupForm</span><span class="params">(@RequestParam&#123;value=<span class="string">"id"</span>&#125; String a, ModelMap model)</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<p>例1和例2都能接受到参数，但是对与例2来说url “/requestParam”的后面一定要有参数，没有会报错；但是例1参数可有可无。<br>但是例2也可以通过设置required=false来指定不一定要参数(就是例3)，就可例1就不行了.<br>通过设置defaultValue=”0” 可以在没有参数时指定默认值。<br>通过设置value=”id” 给参数换成其他名字。</p>
<p>当请求参数a不存在时会有异常发生,可以通过设置属性required=false解决,<br>例如: @RequestParam(value=”a”, required=false)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/requestParam"</span>, method = RequestMethod.GET) </span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">setupForm</span><span class="params">(@RequestParam(value=<span class="string">"a"</span>, required=<span class="keyword">false</span>)</span> String a, ModelMap model) </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<h3 id="RequestMapping"><a href="#RequestMapping" class="headerlink" title="@RequestMapping"></a>@RequestMapping</h3><p>处理请求地址映射的注解。<br>三类属性：<br>（1）value、method。前者是url，后者是请求类型post/get/put/delete等。<br>（2）produces、consumes。前者是只接受请求的规定返回值（accept）是某值的请求，如application/json。后者是只接受请求值是某种类型的值的请求。<br>（3）params、header。只接受参数/头包含某内容的请求。<br>eg：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@ResponseBody</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(value = <span class="string">"/portrait/tags"</span>,produces=<span class="string">"application/json;charset=UTF-8"</span>)</span><br><span class="line"> <span class="function"><span class="keyword">public</span> String <span class="title">getTagList</span><span class="params">(String callback)</span></span></span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://www.cnblogs.com/qq78292959/p/3760560.html" target="_blank" rel="noopener">https://www.cnblogs.com/qq78292959/p/3760560.html</a></p>
<h2 id="spring参数类"><a href="#spring参数类" class="headerlink" title="spring参数类"></a>spring参数类</h2><h3 id="Value"><a href="#Value" class="headerlink" title="@Value"></a>@Value</h3><p>假设有一个test.properties，内容：<code>testname=li</code><br>方法1:<br>在spring的配置文件中配置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"testProperties"</span> <span class="attr">class</span>=<span class="string">"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"locations"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">list</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>classpath*:/test.properties<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">			<span class="comment">&lt;!--添加多个properties配置文件--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">list</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"ignoreUnresolvablePlaceholders"</span> <span class="attr">value</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">context:property-placeholder</span> <span class="attr">location</span>=<span class="string">"classpath*:test.properties"</span>  <span class="attr">ignore-unresolvable</span>=<span class="string">"true"</span>/&gt;</span></span><br></pre></td></tr></table></figure>
<p>使用，注意类的成员变量name的注解${}里的值就是properties里的key：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestProper</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;testname&#125;"</span>)</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testProperties</span><span class="params">()</span></span>&#123;</span><br><span class="line">        log.info(<span class="string">"name:&#123;&#125;"</span>, name);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>方法2<br>在spring中配置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"properFactory"</span> <span class="attr">class</span>=<span class="string">"org.springframework.beans.factory.config.PropertiesFactoryBean"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"locations"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">list</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>classpath*:/test.properties<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">list</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>使用，注意类的成员变量name的注解  properFactory是上面spring配置的id  testname是properties配置文件里值的key：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestProper</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"#&#123;properFactory['testname']&#125;"</span>)</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testProperties</span><span class="params">()</span></span>&#123;</span><br><span class="line">        log.info(<span class="string">"name:&#123;&#125;"</span>, name);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>spring的线程安全</title>
    <url>/2018/04/20/spring%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/</url>
    <content><![CDATA[<h3 id="参数不安全"><a href="#参数不安全" class="headerlink" title="参数不安全"></a>参数不安全</h3><p>解决办法就是：有状态的变量本地化。spring框架也有这样的实现，spring的dao是单例的，也就是线程安全的，但是按说每个方法都有一个数据库连接Connection，Connection的值肯定不是线程安全，spring为了解决这个问题就是把Connection本地化了，就是下面例子里的方法，把Connection赋值给本地的Connection变量。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestCur</span></span>&#123;</span><br><span class="line">	<span class="comment">//private Map&lt;String, String&gt; map = Maps.newHashMap();</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deal</span><span class="params">(Map map1)</span></span>&#123;</span><br><span class="line">		<span class="comment">//此处要对map1进行一些计算等操作。在多线程的情况下，又无法确保参数map1是安全的，考虑在此方法里先把map1深拷贝</span></span><br><span class="line">		<span class="comment">//（只是值拷贝，也不算是深拷贝）给方法里的本地变量，这样就不用担心其他线程在方法外面对参数map进行修改而影响本方法。</span></span><br><span class="line">		<span class="comment">//例如公司的qconfig，不能直接给类的成员变量（如，上面的map），只能先深拷贝给方法本地的map再赋值给类成员变量map。</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="spring中的线程安全"><a href="#spring中的线程安全" class="headerlink" title="spring中的线程安全"></a>spring中的线程安全</h3><p>在spring中加了spring注解的都是单例，不在类里写可变状态的类成员变量时，就不用考虑线程不安全的问题。写了可变状态变量的就比如1中的map。<br>原因是：如果控制器是使用单例形式，且controller中有一个私有的变量a,所有请求到同一个controller时，使用的a变量是共用的，即若是某个请求中修改了这个变量a，则，在别的请求中能够读到这个修改的内容。<br>解决方法有两个：<br>（1）写一个treadlocal，把a本地化（1中提到的spring解决Connection就是用的这种办法，只不过spring需要管理的太多用了treadlocalmap）<br>（2）在spring的配置文件中为这个controller写上scope=”prototype”，定义为非单例的：<br><code>&lt;bean id=&quot;userController&quot; class=&quot;com.qunar.data.controller.UserController&quot; scope=&quot;prototype&quot;&gt;&lt;/bean&gt;</code><br>但最好不要在想要线程安全的类里写类成员变量。</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>另外数据库连接方面，只要使用spring 自己实现的dao，如JdbcTemplate，就不会发生数据库连接泄露的问题。但是如果手动调用Connection而忘记关闭就很可能会导致数据库连接泄露问题。<br><code>DataSourceUtils.getConnection()</code>方法会首先查看当前是否存在事务管理上下文，如果存在就尝试从事务管理上下文拿连接，如果获取失败，直接从数据源中拿。在获取连接后，如果存在事务管理上下文则把连接绑定上去。</p>
<p>spring在servlet上扩展，都是线程安全。</p>
]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>调试shell</title>
    <url>/2018/04/20/%E8%B0%83%E8%AF%95shell/</url>
    <content><![CDATA[<h1 id="shell自带工具调试"><a href="#shell自带工具调试" class="headerlink" title="shell自带工具调试"></a>shell自带工具调试</h1><p>shell自带的调试，用参数 -x 和 -n。</p>
<p>1、 bash -x shell文件<br>会对整个的shell文件执行。<br>有+的是代码，没有的echo输出。<br>在shell脚本中某段代码前后添加set -x和set +x可以只调试此段代码。</p>
<p>2、 bash -n shell文件<br>只检查脚本是否正确，不真的执行。</p>
<h1 id="bashdb"><a href="#bashdb" class="headerlink" title="bashdb"></a>bashdb</h1><p>这是一个第三方工具，安装：<code>sudo apt-get install bashdb</code><br>bashdb可以单步的执行，有很多的参数用于调试：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l 列出当前行以下的10行</span><br><span class="line">&#x2F;pat&#x2F; 向后搜索pat</span><br><span class="line">？pat？ </span><br><span class="line">n 执行下一条语句，遇到函数，不进入函数里面执行，将函数当作黑盒</span><br><span class="line">s n 单步执行n次，遇到函数进入函数里面</span><br><span class="line">b 行号n 在行号n处设置断点</span><br><span class="line">d 行号n 撤销行号n处的断点</span><br><span class="line">c 行号n 一直执行到行号n处，如果没有写n参数，则直接执行到下一个断点处</span><br><span class="line">R 重新启动</span><br><span class="line">Finish 执行到程序最后</span><br><span class="line">cond n expr 条件断点</span><br><span class="line">print $a 表示显示变量a的值</span><br><span class="line">clear 或者d，清除所有的断点</span><br><span class="line">disable &#x2F; enable 禁用、启用断点</span><br><span class="line">skip [count] 跳过下面一些代码</span><br><span class="line">return 跳出</span><br></pre></td></tr></table></figure>
<p>bashdb –debug file 即可开始调试：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chenliclchen@chenliclchen-Latitude-E5440:~&#x2F;study&#x2F;crontab$ bashdb --debug .&#x2F;test.sh</span><br><span class="line">bashdb --debug .&#x2F;test.sh</span><br><span class="line">bash debugger, bashdb, release 4.3-0.91</span><br><span class="line"></span><br><span class="line">Copyright 2002, 2003, 2004, 2006-2012, 2014 Rocky Bernstein</span><br><span class="line">This is free software, covered by the GNU General Public License, and you are</span><br><span class="line">welcome to change it and&#x2F;or distribute copies of it under certain conditions.</span><br><span class="line"></span><br><span class="line">(&#x2F;home&#x2F;chenliclchen&#x2F;study&#x2F;crontab&#x2F;test.sh:3):</span><br><span class="line">3:	log_prefix_name&#x3D;$1</span><br><span class="line">bashdb&lt;0&gt; 1</span><br><span class="line">** Undefined command &quot;1&quot;. Try &quot;help&quot;.</span><br><span class="line">bashdb&lt;0&gt; n</span><br><span class="line">(&#x2F;home&#x2F;chenliclchen&#x2F;study&#x2F;crontab&#x2F;test.sh:4):</span><br><span class="line">4:	log_name&#x3D;$&#123;log_prefix_name&#125;&#96;date +%Y-%m-%d --date&#x3D;&quot;-1 day&quot;&#96;&#39;.log.gz&#39;</span><br><span class="line">bashdb&lt;1&gt; n</span><br><span class="line">(&#x2F;home&#x2F;chenliclchen&#x2F;study&#x2F;crontab&#x2F;test.sh:5):</span><br><span class="line">5:	threshold_count&#x3D;$2</span><br><span class="line">bashdb&lt;2&gt; n</span><br><span class="line">(&#x2F;home&#x2F;chenliclchen&#x2F;study&#x2F;crontab&#x2F;test.sh:6):</span><br><span class="line">6:	mail_to&#x3D;$3</span><br><span class="line">bashdb&lt;3&gt; n</span><br><span class="line">(&#x2F;home&#x2F;chenliclchen&#x2F;study&#x2F;crontab&#x2F;test.sh:7):</span><br><span class="line">7:	mail_content_file&#x3D;$4</span><br><span class="line">bashdb&lt;4&gt; b 10</span><br><span class="line">Breakpoint 1 set in file &#x2F;home&#x2F;chenliclchen&#x2F;study&#x2F;crontab&#x2F;test.sh, line 10.</span><br><span class="line">bashdb&lt;5&gt; c</span><br><span class="line">2017-12-10.log.gz</span><br><span class="line">file is not exist</span><br></pre></td></tr></table></figure>

<p>其他参考：<a href="http://blog.techbeta.me/2015/10/shell-debug/" target="_blank" rel="noopener">http://blog.techbeta.me/2015/10/shell-debug/</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>ERROR 1 HY000 Can&#39;t create or write to file &#39;user.txt&#39; Errcode 13 - Permission denied</title>
    <url>/2018/04/19/ERROR%201%20HY000%20Can&#39;t%20create%20or%20write%20to%20file%20&#39;user.txt&#39;%20Errcode%2013%20-%20Permission%20denied/</url>
    <content><![CDATA[<p>把mysql的自动导出数据的目录修改之后的报错“ERROR 1 (HY000): Can’t create/write to file ‘/home/chenliclchen/mysql/user.txt’ (Errcode: 13 - Permission denied)”</p>
<p>看着是权限的问题，然后各种的chmod chown的命令修改权限和 拥有者。怎么着都不行。 最后的办法是执行：sudo aa-status ，看到如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apparmor module is loaded.</span><br><span class="line">22 profiles are loaded.</span><br><span class="line">22 profiles are in enforce mode.</span><br><span class="line">   &#x2F;sbin&#x2F;dhclient</span><br><span class="line">   &#x2F;usr&#x2F;bin&#x2F;evince</span><br><span class="line">   &#x2F;usr&#x2F;bin&#x2F;evince-previewer</span><br><span class="line">   &#x2F;usr&#x2F;bin&#x2F;evince-previewer&#x2F;&#x2F;sanitized_helper</span><br><span class="line">   &#x2F;usr&#x2F;bin&#x2F;evince-thumbnailer</span><br><span class="line">   &#x2F;usr&#x2F;bin&#x2F;evince-thumbnailer&#x2F;&#x2F;sanitized_helper</span><br><span class="line">   &#x2F;usr&#x2F;bin&#x2F;evince&#x2F;&#x2F;sanitized_helper</span><br><span class="line">   &#x2F;usr&#x2F;bin&#x2F;ubuntu-core-launcher</span><br><span class="line">   &#x2F;usr&#x2F;lib&#x2F;NetworkManager&#x2F;nm-dhcp-client.action</span><br><span class="line">   &#x2F;usr&#x2F;lib&#x2F;NetworkManager&#x2F;nm-dhcp-helper</span><br><span class="line">   &#x2F;usr&#x2F;lib&#x2F;connman&#x2F;scripts&#x2F;dhclient-script</span><br><span class="line">   &#x2F;usr&#x2F;lib&#x2F;cups&#x2F;backend&#x2F;cups-pdf</span><br><span class="line">   &#x2F;usr&#x2F;lib&#x2F;lightdm&#x2F;lightdm-guest-session</span><br><span class="line">   &#x2F;usr&#x2F;lib&#x2F;lightdm&#x2F;lightdm-guest-session&#x2F;&#x2F;chromium</span><br><span class="line">   &#x2F;usr&#x2F;sbin&#x2F;cups-browsed</span><br><span class="line">   &#x2F;usr&#x2F;sbin&#x2F;cupsd</span><br><span class="line">   &#x2F;usr&#x2F;sbin&#x2F;cupsd&#x2F;&#x2F;third_party</span><br><span class="line">   &#x2F;usr&#x2F;sbin&#x2F;ippusbxd</span><br><span class="line">   &#x2F;usr&#x2F;sbin&#x2F;mysqld     ！！！！！要是有这个表示mysql被限制了执行下面绿色的命令</span><br><span class="line">   &#x2F;usr&#x2F;sbin&#x2F;tcpdump</span><br><span class="line">   webbrowser-app</span><br><span class="line">   webbrowser-app&#x2F;&#x2F;oxide_helper</span><br><span class="line">0 profiles are in complain mode.</span><br><span class="line">3 processes have profiles defined.</span><br><span class="line">3 processes are in enforce mode.</span><br><span class="line">   &#x2F;sbin&#x2F;dhclient (920) </span><br><span class="line">   &#x2F;usr&#x2F;sbin&#x2F;cups-browsed (778) </span><br><span class="line">   &#x2F;usr&#x2F;sbin&#x2F;mysqld (26226) </span><br><span class="line">0 processes are in complain mode.</span><br><span class="line">0 processes are unconfined but have a profile defined.</span><br></pre></td></tr></table></figure>
<p>If mysqld is included in enforce mode, then it is the one probably denying the write. Entries would also be written in /var/log/messages when AppArmor blocks the writes/accesses.<br>之后 修改文件： sudo vim /etc/apparmor.d/usr.sbin.mysqld， 修改如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> # Allow pid, socket, socket lock file access</span><br><span class="line">&#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.pid rw,</span><br><span class="line">&#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.sock rw,</span><br><span class="line">&#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.sock.lock rw,</span><br><span class="line">&#x2F;run&#x2F;mysqld&#x2F;mysqld.pid rw,</span><br><span class="line">&#x2F;run&#x2F;mysqld&#x2F;mysqld.sock rw,</span><br><span class="line">&#x2F;run&#x2F;mysqld&#x2F;mysqld.sock.lock rw,</span><br><span class="line">&#x2F;home&#x2F;chenliclchen&#x2F;mysql&#x2F; r,     #####添加的两行，添加想要保存导出数据的文件夹地址</span><br><span class="line">&#x2F;home&#x2F;chenliclchen&#x2F;mysql&#x2F;* rw,   #####</span><br></pre></td></tr></table></figure>
<p>最后再sudo /etc/init.d/apparmor reload 这样整个的流程完成。可以把导出的数据保存到/home/chenliclchen/mysql/目录下了。</p>
<p>参考：<a href="http://blog.csdn.net/Silver_sail/article/details/8166193" target="_blank" rel="noopener">http://blog.csdn.net/Silver_sail/article/details/8166193</a></p>
]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>ERROR 1290 HY000  The MySQL server is running with the --secure-file-priv option so it cannot execute this statement</title>
    <url>/2018/04/19/ERROR%201290%20HY000%20%20The%20MySQL%20server%20is%20running%20with%20the%20--secure-file-priv%20option%20so%20it%20cannot%20execute%20this%20statement/</url>
    <content><![CDATA[<p>在使用into outfile 进行数据导出时报的错：<br>ERROR 1290 (HY000): The MySQL server is running with the –secure-file-priv option so it cannot execute this statement。</p>
<p>原因：在MySQL 5.7.6版本之后，导入文件只能在secure_file_priv指定的文件夹下<br>在mysql中使用  show variables like ‘%secure%’; 看到结果<br><img src="1.png" alt=""><br>因此，我们可以选择把数据导出到 ‘/var/lib/mysql-files’ 的文件下。这时可能你想进入到这个文件夹下面看看，可能会出现cd不进去的情况。解决如下：<br>[sudo cd为什么不能够执行](/2018/04/19/sudo cd为什么不能够执行/)</p>
<p>或者你不想这样做，你想把数据导出你想导出的目录，可以这样：<br>修改文件：vim  /etc/mysql/mysql.conf.d/mysqld.cnf   也有的是  vim /etc/mysql/mysql.conf，我是因为后面的文件不存在才使用的前面的那个文件。（此处，我的理解是，我先看了mysql下有个文件，名字是“mysql.cnf”，它的里面是两行include，以c语言的语法是，它包含了另外两个文件，因此在另外两个文件夹下找自己需要的’[mysqld]’，当然不要一个一个的看，用grep命令： grep -nr “mysqld” ./）<br><strong><em>在[mysqld] 下边写上 secure_file_priv=/home/chenliclchen/mysql/   后面的路径替换成你想保存的路径。</em></strong></p>
<p>然后重启mysql ：  service  mysql restart<br>但是要注意你的要保存的那个路径必须是有读写权限的，这也是个问题。 修改之后文件路径还必须是绝对路径。<br>这段参考：<a href="http://blog.csdn.net/learner_lps/article/details/65448098" target="_blank" rel="noopener">http://blog.csdn.net/learner_lps/article/details/65448098</a></p>
<p>前面的全部修改之后，执行之前的导出数据的命令，报错“ERROR 1 (HY000): Can’t create/write to file ‘/home/chenliclchen/mysql/user.txt’ (Errcode: 13 - Permission denied)”<br>[点击获取解决方案](/2018/04/19/ERROR 1 HY000 Can’t create or write to file ‘user.txt’ Errcode 13 - Permission denied/)</p>
]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql 执行顺序、explain、索引</title>
    <url>/2018/04/19/mysql%20%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%E3%80%81explain%E3%80%81%E7%B4%A2%E5%BC%95/</url>
    <content><![CDATA[<h3 id="用ubuntu命令读取数据库中信息。"><a href="#用ubuntu命令读取数据库中信息。" class="headerlink" title="用ubuntu命令读取数据库中信息。"></a>用ubuntu命令读取数据库中信息。</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -h 127.0.0.1 -u root -p XXXX -P 3306 -e &quot;select * from table&quot;  &gt; &#x2F;tmp&#x2F;test&#x2F;txt</span><br></pre></td></tr></table></figure>
<p>-h 后面是主机； -u后面是用户名； -p后面是数据库名字 ； -P后面是端口号 ； -e就是mysql的select语句 ； 最后是重定向到一个文件。 </p>
<h3 id="int-num-varchar-num"><a href="#int-num-varchar-num" class="headerlink" title="int(num) varchar(num)"></a>int(num) varchar(num)</h3><p>int的num只是定长与存储无关，长度小于num时，左边补空对齐；大于num时，不管。存储长度默认是4字节。eg： int（1）代表显示宽度是1，而不是存储长度。<br>varchar的会截断数据，是数据长度。但在长度小于255时，存储的长度会是num+1；大于255时，长度会num+2；如原始字符串是‘qwertasdf‘，varchar(5)，最终的存储的字符串是qwert五位，数据库中存储是六位。<br>char基本类似与varchar，只是存储时不会多存储一位。</p>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><p>explain sql1；查看sql1的性能。第 5 条解释了它的两个重要参数 rows 和filtered。<br>show create table table1； 查看table1的建表语句。（建库语句类似）<br>show index from table1; 查看table1的索引。 第7列是索引的大小。<br>grant 权限 on 数据库对象 to 用户</p>
<p>上面的语句都可以通过添加\G来使原本的显示按行排列。<br>eg，explain select * from flight_qmq_pay_order_20171204 order by insert_time desc limit 10\G;</p>
<h3 id="sql执行顺序"><a href="#sql执行顺序" class="headerlink" title="sql执行顺序"></a>sql执行顺序</h3><p>对于一条sql语句来说，执行顺序是这样的：<br>1、from子句组装来自不同数据源的数据；<br>(join条件筛选)<br>2、where子句基于指定的条件对记录行进行筛选；<br>3、group by子句将数据划分为多个分组；<br>4、使用聚集函数进行计算；<br>5、使用having子句筛选分组；<br>6、计算所有的表达式；<br>7、使用order by对结果集进行排序；<br>8、select 集合输出。</p>
<p>由于select在group by之后执行，如果select中出现count等函数都是针对group之后的分组计算。</p>
<h3 id="explain参数"><a href="#explain参数" class="headerlink" title="explain参数"></a>explain参数</h3><p>它主要是对mysql性能评估。下面加粗的字体是几个比较重要的参数。<br>官网有它的所有参数解释：<a href="https://dev.mysql.com/doc/refman/5.7/en/explain-output.html" target="_blank" rel="noopener">https://dev.mysql.com/doc/refman/5.7/en/explain-output.html</a><br><strong><em>rows：</em></strong> 是执行sql语句需要检查的行。对于InnoDB表格，这个数字是一个估计，并不总是准确的。<br>filtered：在把命令写成explain extended sql时才会出现，意思是输出的行占寻找输出行时搜索的其他行的百分比。 最终输出的行  /  读的行。 也就是说，rows 显示了检查的行数， rows× filtered/ 100显示了与之前的表搜索的行数。<br>       <strong><em>可以查看一个索引的的扫描范围，rows可以看扫描的行数，扫描行数越少效率越高</em></strong> </p>
<ul>
<li>id：查询编号</li>
<li>select_type：<br>SIMPLE：    简单查询<br>PRIMARY：   最外层的select<br>SUBQUERY：  子查询内层查询的第一个select<br>DERIVED：   子查询派生表的select</li>
<li>table：表名</li>
<li>type：<br>index： 全索引扫描<br>const： 通过主键访问<br>all：   全表扫描,没有使用索引<br>range： 索引返回扫描，常见于使用&gt;,&lt;,is null,between ,in ,like等运算符的查询中。<br>ref：   索引扫描，结果可能有多个匹配值<br>eq_ref：索引扫描，唯一索引匹配值（唯一） </li>
</ul>
<p>访问效率：const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;all</p>
<ul>
<li>possible_keys： 可能使用到的索引</li>
<li>key： 最终使用的索引</li>
<li>key_len： 索引的长度（使用到的）</li>
<li>rows： 扫描行数</li>
<li>extra：<br>impossible where noticed after reading const tables: mysql优化器通过分析发现不可能存在的结果<br><strong>using index</strong>： 所需要的数据只需要在index即可全部获得而不需要再到表中取数据<br>using index for group-by： 当query中使用了group by或者distinct子句的时候，如果分组字段也在索引中，extra中出现该信息<br>_using filesort_： query中包含order by，且无法利用索引完成排序操作的时候，mysqlquery optimizer不得不选择相应的排序算法来实现。（并不一定带白哦磁盘顺序）<br>_using temporary_： 使用临时表时出现，主要常见于group by和order by等操作中<br>using where： 不是读取表的所有数据，或者不是仅仅通过索引就可以获取所有需要的数据，则会出现using where信息</li>
</ul>
<p><strong><em>extra</em></strong>：<strong><em>Using filesort， Using temporary说明索引不好需要改进，Using index 说明sql的执行效率很可以</em></strong><br><strong><em>type</em></strong>：连接类型。一个好的sql语句至少要达到range级别。杜绝出现all级别。<br><strong><em>key</em></strong>：使用到的索引名。如果没有选择索引，值是NULL。</p>
<p>参考：<a href="https://www.cnblogs.com/david97/p/8072164.html" target="_blank" rel="noopener">https://www.cnblogs.com/david97/p/8072164.html</a></p>
<h3 id="索引注意"><a href="#索引注意" class="headerlink" title="索引注意"></a>索引注意</h3><p><strong><em>避免like’%xxx%’</em></strong>，使用like ‘xxx%’。mysql可以使用前缀索引扫描。<br><strong><em>匹配条件字符串要加单引号</em></strong>。否则可能会导致全表扫描，数字匹配的时候不要加单引号。<br><strong><em>order by后的内容尽量放到索引中</em></strong>。Where后的内容是=而不是范围性的，就可以和order by一起组成索引。如果where后的是主键id，则可以只给order by建立索引。<br><strong><em>不要使用子查询</em></strong>，因为内部查询的结果在临时表中，没有索引可用。<br><strong><em>避免有where status=1这样的操作，status的值范围小，一般都会造成全表扫表</em></strong>。尽量增加一些时间或者主键id的字段控制。</p>
<p><strong><em>innodb一般都是行锁</em></strong>，这个一般指的是sql用到索引的时候，行锁是加在索引上的，不是加在数据记录上的，<strong><em>如果sql没有用到索引，仍然会锁定全表</em></strong>。<br>in: 包含的值不宜过多。mysql对in关键字进行了优化，把所有的值放在数组中，且自动对in里的值进行了排序。但是如果数值太多，消耗还是会很大。例如有in(1,2,3,4)这样连续的数值最好用between代替。<br>select* : 增加很多不必要的消耗（cpu、io、内存、网络带宽）；减少了覆盖索引的可能性，而且如果数据库结构发生了改变，前面的代码也需要相应的改变。</p>
<h3 id="易错"><a href="#易错" class="headerlink" title="易错"></a>易错</h3><p>update a set order_id=405 and date=’’ where id=’’    <strong><em>不应该用and而用逗号。这里会当做一个表达式返回一个值再赋值给order_id</em></strong>。<br>date：一般使用timestamp，5.6.4之后可以使用datetime类型。timestamp会采用系统调用，并发量高的情况下会造成系统自旋锁，建议使用datetime。<br>Text 比varchar存更多的东西。不能存null，或者默认为null。</p>
<h3 id="出错码"><a href="#出错码" class="headerlink" title="出错码"></a>出错码</h3><p>28000，用户名或密码错误。<br>42000，权限问题。</p>
<h3 id="物理分页-逻辑分页"><a href="#物理分页-逻辑分页" class="headerlink" title="物理分页/逻辑分页"></a>物理分页/逻辑分页</h3><p>物理分页：数据库本身提供的分页方式，比如mysql的limit。但不同数据库不同。<br>逻辑分页：使用游标（next）分页，先把数据都查出来再分页。<br>mybatis的RowBound是逻辑分页。<br>逻辑分页是把数据库的压力放在了应用中，因为是在应用中分的页。</p>
<h3 id="权限管理"><a href="#权限管理" class="headerlink" title="权限管理"></a>权限管理</h3><p>mysql里用户信息存在<code>mysql.user</code>里，通过看这个表可以看到所有用户的权限信息。<br>通过<code>SELECT DISTINCT CONCAT(&#39;User: &#39;&#39;&#39;,user,&#39;&#39;&#39;@&#39;&#39;&#39;,host,&#39;&#39;&#39;;&#39;) AS query FROM mysql.user;</code>可以查到所有的账户。<br>这里表里<code>user+host</code>必须是唯一的。<br><code>show grants</code> 查看当前用户的权限。</p>
<h4 id="create-创建用户"><a href="#create-创建用户" class="headerlink" title="create 创建用户"></a>create 创建用户</h4><p><code>create user &#39;username&#39;@&#39;host&#39; [identified by [password] &#39;password1&#39; ]</code> 创建新的用户。没有<code>[identified by [password] &#39;password1&#39; ]</code>代表该用户不需要密码；加上<code>[password]</code>表示后面的密码<code>password1</code>用password加密。<br>加密的原因：<code>create user</code>语句的操作会被记录到服务器日志文件或者操作历史文件中。有那个文件的人可以可以看到用户的密码，因此需要加密。<br>如何加密：使用password加密。先使用<code>select password(&#39;password1&#39;)</code>算出加密后的哈希值密码，再用加密后的密码代替上面的<code>password1</code>.</p>
<h4 id="grant-授权"><a href="#grant-授权" class="headerlink" title="grant 授权"></a>grant 授权</h4><p>相比与<code>create</code>需要先创建，再授权；<code>grant</code>可以直接创建用户并授权。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GRANT priv_type [(column_list)] [, priv_type [(column_list)]] ...</span><br><span class="line">    ON [object_type] &#123;tbl_name | * | *.* | db_name.*&#125;</span><br><span class="line">    TO user [IDENTIFIED BY [PASSWORD] &#39;password&#39;]</span><br><span class="line">        [, user [IDENTIFIED BY [PASSWORD] &#39;password&#39;]] ...</span><br><span class="line">    [REQUIRE</span><br><span class="line">        NONE |</span><br><span class="line">        [&#123;SSL| X509&#125;]</span><br><span class="line">        [CIPHER &#39;cipher&#39; [AND]]</span><br><span class="line">        [ISSUER &#39;issuer&#39; [AND]]</span><br><span class="line">        [SUBJECT &#39;subject&#39;]]</span><br><span class="line">    [WITH with_option [with_option] ...]</span><br></pre></td></tr></table></figure>
<p>简单的，<code>grant priv on 库.表 to &#39;username&#39;@&#39;host&#39; [identified by [password] &#39;password1&#39; ]</code> 给用户<code>&#39;username&#39;@&#39;host&#39;</code>授权。<br>当需要给多个机器权限时，就需要多个grant语句，此时如果是同一网段的，可以host采用<code>192.23.%</code>；甚至可以直接用<code>%</code>代表所有机器。</p>
<p><code>GRANT USAGE ON</code> 加空的权限，只能连库什么都没法做。</p>
<h4 id="直接修改表加权限"><a href="#直接修改表加权限" class="headerlink" title="直接修改表加权限"></a>直接修改表加权限</h4><p>不管是<code>create</code>还是<code>grant</code>，其实都是在操纵<code>user</code>表。<br><code>INSERT INTO mysql.user(host,user,password,[privilegelist]) VALUES (&#39;host&#39;,&#39;username&#39;,password(&#39;password&#39;),privilegevaluelist)</code><br>emmmm，我的mysql没有<code>password</code>字段。。。。</p>
<h4 id="删除用户"><a href="#删除用户" class="headerlink" title="删除用户"></a>删除用户</h4><p><code>drop user ‘username’@‘host’</code>/<code>delete from mysql.user where user=&#39;username&#39; and host=&#39;host&#39;</code></p>
<p>参考：<a href="https://www.cnblogs.com/lyhabc/p/3822267.html" target="_blank" rel="noopener">https://www.cnblogs.com/lyhabc/p/3822267.html</a> </p>
<h3 id="Lock-wait-timeout-exceeded"><a href="#Lock-wait-timeout-exceeded" class="headerlink" title="Lock wait timeout exceeded;"></a><code>Lock wait timeout exceeded;</code></h3><p><code>ERROR 1205 (HY000) at line 1: Lock wait timeout exceeded; try restarting transaction</code><br><code>show full processlist;</code><br><code>kill id;</code> 杀掉锁住的id进程。</p>
<p>参考：<a href="https://blog.csdn.net/wp1603710463/article/details/51721894/" target="_blank" rel="noopener">https://blog.csdn.net/wp1603710463/article/details/51721894/</a></p>
]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>mapper部分语法 CDATA foreach resultMap typeHandlers</title>
    <url>/2018/04/19/mapper%E9%83%A8%E5%88%86%E8%AF%AD%E6%B3%95%20CDATA%20foreach%20resultMap/</url>
    <content><![CDATA[<h3 id="CDATA"><a href="#CDATA" class="headerlink" title="CDATA"></a>CDATA</h3><p>总结自：<a href="http://blog.csdn.net/glory1234work2115/article/details/51695540" target="_blank" rel="noopener">http://blog.csdn.net/glory1234work2115/article/details/51695540</a><br>在 XML 元素中，”&lt;” 和 “&amp;” 是非法的。<br>“&lt;” 会产生错误，因为解析器会把该字符解释为新元素的开始。<br>“&amp;” 也会产生错误，因为解析器会把该字符解释为字符实体的开始。</p>
<p>一般要写成：</p>
<table>
<thead>
<tr>
<th align="center">Item</th>
<th align="center">Value</th>
<th align="center">Qty</th>
</tr>
</thead>
<tbody><tr>
<td align="center">&amp;lt;</td>
<td align="center">&lt;</td>
<td align="center">小于</td>
</tr>
<tr>
<td align="center">&amp;gt;</td>
<td align="center">&gt;</td>
<td align="center">大于</td>
</tr>
<tr>
<td align="center">&amp;amp;</td>
<td align="center">&amp;</td>
<td align="center">和号</td>
</tr>
<tr>
<td align="center">&amp;apos;</td>
<td align="center">‘</td>
<td align="center">省略号</td>
</tr>
<tr>
<td align="center">&amp;quot;</td>
<td align="center">“</td>
<td align="center">引号</td>
</tr>
<tr>
<td align="center">注释：严格地讲，在 XML 中仅有字符 “&lt;”和”&amp;” 是非法的。省略号、引号和大于号是合法的，但是把它们替换为实体引用是个好的习惯。</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">但是用了“&lt;![CDATA[”之后就可以直接用&#39;&lt;&#39;、‘&gt;’等了。</span><br><span class="line">CDATA 部分中的所有内容都会被解析器忽略。</span><br><span class="line">CDATA 部分由 &quot;&lt;![CDATA[&quot; 开始，由 &quot;]]&gt;&quot; 结束。</span><br></pre></td></tr></table></figure></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
<p>关于 CDATA 部分的注释：<br>CDATA 部分不能包含字符串 “]]&gt;”。也不允许嵌套的 CDATA 部分。<br>标记 CDATA 部分结尾的 “]]&gt;” 不能包含空格或折行。</p>
<h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><p>来自：<a href="http://blog.csdn.net/jason5186/article/details/40896043" target="_blank" rel="noopener">http://blog.csdn.net/jason5186/article/details/40896043</a></p>
<table>
<thead>
<tr>
<th align="center">属性</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">item</td>
<td align="left">循环体中的具体对象。支持属性的点路径访问，如item.age,item.info.details。具体说明：在list和数组中是其中的对象，在map中是value。该参数为必选。</td>
</tr>
<tr>
<td align="center">collection</td>
<td align="left">要做foreach的对象，作为入参时，List&lt;?&gt;对象默认用list代替作为键，数组对象有array代替作为键，Map对象用map代替作为键。</br>当然在作为入参时可以使用@Param(“keyName”)来设置键，设置keyName后，list,array,map将会失效。 除了入参这种情况外，还有一种作为参数对象的某个字段的时候。举个例子：</br>如果User有属性List ids。入参是User对象，那么这个collection = “ids”</br>如果User有属性Ids ids;其中Ids是个对象，Ids有个属性List id;入参是User对象，那么collection = “ids.id”</br>上面只是举例，具体collection等于什么，就看你想对那个元素做循环。该参数为必选。</td>
</tr>
<tr>
<td align="center">separator</td>
<td align="left">元素之间的分隔符，例如在in()的时候，separator=”,”会自动在元素中间用“,“隔开，避免手动输入逗号导致sql错误，如in(1,2,)这样。该参数可选。</td>
</tr>
<tr>
<td align="center">open</td>
<td align="left">foreach代码的开始符号，一般是(和close=”)”合用。常用在in(),values()时。该参数可选。</td>
</tr>
<tr>
<td align="center">close</td>
<td align="left">foreach代码的关闭符号，一般是)和open=”(“合用。常用在in(),values()时。该参数可选。</td>
</tr>
<tr>
<td align="center">index</td>
<td align="left">在list和数组中,index是元素的序号，在map中，index是元素的key，该参数可选。</td>
</tr>
</tbody></table>
<p>简单例子：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">"tmpInsertCount"</span> <span class="attr">parameterType</span>=<span class="string">"list"</span>&gt;</span></span><br><span class="line">          INSERT INTO `flight_qmq_order_count_tmp`(</span><br><span class="line">          countTime,</span><br><span class="line">          attribute,</span><br><span class="line">          value</span><br><span class="line">          )VALUES</span><br><span class="line">         <span class="tag">&lt;<span class="name">foreach</span> <span class="attr">collection</span>=<span class="string">"list"</span> <span class="attr">item</span>=<span class="string">"item"</span> <span class="attr">separator</span>=<span class="string">","</span>&gt;</span> </span><br><span class="line">			(#&#123;item.countTime&#125;, #&#123;item.attribute&#125;, #&#123;item.count&#125;) </span><br><span class="line">		<span class="tag">&lt;/<span class="name">foreach</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="resultType-or-resultMap"><a href="#resultType-or-resultMap" class="headerlink" title="resultType  or   resultMap"></a>resultType  or   resultMap</h3><p>mybatis推荐自定义类使用resultMap，而不用resultType，原因：<br>（1）resultMap适合返回值是自定义实体类的情况。需要在mapper中自定义resultMap，把数据库中的列和model对应起来。<br>（2）resultType适合使用返回值的数据类型是非自定义。如果想用自定义类型就必须命名规范，需要先设置mapUnderscoreToCamelCase为true，之后必须和数据库字段的下划线对应成驼峰。<br>（3）将来如果Model中的成员变量名字变了，resultMap只需要到Mapper中改对应resultMap；而resultType需要到数据库里改表字段的名字，如果其他项目用了这个数据库，还要改其他项目的东西。把问题的影响范围缩小的考虑，应该使用resultMap。<br>（4）而且查询时列名想用别名时就需要resultMap。</p>
<p>resultMap 需要自己定义，将数据库中的字段和model对应，简单例子：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">resultMap</span> <span class="attr">id</span>=<span class="string">"hotelInfoResultMap"</span> <span class="attr">type</span>=<span class="string">"HotelInfoModel"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span> <span class="attr">column</span>=<span class="string">"id"</span> <span class="attr">property</span>=<span class="string">"id"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">"hotel_name"</span> <span class="attr">property</span>=<span class="string">"hotelName"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">"city"</span> <span class="attr">property</span>=<span class="string">"city"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">"price"</span> <span class="attr">property</span>=<span class="string">"price"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">"level"</span> <span class="attr">property</span>=<span class="string">"level"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">resultMap</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>resultMap的属性id在写上去了时作为resultMap的值；属性type是model的值。<br>column是数据库字段名，property是model字段名。</p>
<h3 id="typeHandlers"><a href="#typeHandlers" class="headerlink" title="typeHandlers"></a>typeHandlers</h3><p>如果有自己的类型想默认转成数据库的类型，可以配置<typeHandlers>，它需要自己写一个类，数据库的读写都会默认扫描这个类，把对应的数据转换。<br>例如，一种应用场景：公司存小数用的money，存到数据库时想默认转成float等，就可以配一个typeHandlers。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span> <span class="attr">interceptor</span>=<span class="string">"XXX"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span> <span class="attr">interceptor</span>=<span class="string">"XXX"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="choose标签"><a href="#choose标签" class="headerlink" title="choose标签"></a>choose标签</h3><p>使用场景：在<where>标签里有需要<code>if...else if ...else</code>的需求</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;where&gt;</span><br><span class="line">    &lt;choose&gt;</span><br><span class="line">        &lt;when test&#x3D;&#39;...&#39;&gt;</span><br><span class="line">            and ...</span><br><span class="line">        &lt;&#x2F;when&gt;</span><br><span class="line">        &lt;when test&#x3D;&#39;...&#39;&gt;</span><br><span class="line">            and ...</span><br><span class="line">        &lt;&#x2F;when&gt;</span><br><span class="line">        &lt;otherwise&gt;</span><br><span class="line">            and ...</span><br><span class="line">        &lt;&#x2F;otherwise&gt;</span><br><span class="line">    &lt;&#x2F;choose&gt;</span><br><span class="line">&lt;&#x2F;where&gt;</span><br></pre></td></tr></table></figure>
<p>但是在 test里不能用大于小于号，目前没有找到可以替代的用法。</p>
<h3 id="和-区别"><a href="#和-区别" class="headerlink" title="#{}和${}区别"></a>#{}和${}区别</h3><p>(1)<br># 将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。<br>如：<code>order by #{user_id}</code>，如果传入的值是111,那么解析成sql时的值为<code>order by &quot;111&quot;</code>,<br>    如果传入的值是id，则解析成的sql为<code>order by &quot;id&quot;</code>.</p>
<p>$ 将传入的数据直接显示生成在sql中。<br>如：<code>order by ${user_id}</code>，如果传入的值是111,那么解析成sql时的值为<code>order by user_id</code>,<br>  如果传入的值是id，则解析成的sql为<code>order by id</code><br>(2)<br>/#方式能够很大程度防止sql注入。<br>$方式无法防止Sql注入.<br>(3)、$方式一般用于传入数据库对象，例如传入表名.<br>(4)、一般能用#的就别用$.<br>(5)、<strong><em>MyBatis排序时使用order by 动态参数时需要注意，用$而不是#</em></strong></p>
]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
        <tag>CDATA</tag>
        <tag>foreach</tag>
        <tag>resultMap</tag>
        <tag>typeHandlers</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql语句</title>
    <url>/2018/04/19/mysql%E8%AF%AD%E5%8F%A5/</url>
    <content><![CDATA[<p>select * from table1 a inner join table2 b on a.id=b.id;</p>
<p>注意mysql语句的执行顺序，正确的使用别名。</p>
<h3 id="shell参数"><a href="#shell参数" class="headerlink" title="shell参数"></a>shell参数</h3><p><code>-u</code> 用户名<br><code>-p</code> 密码<br><code>-h</code> 主机<br><code>-P</code> 端口号<br><code>-s</code> 静默模式，省略很多输出。比如select时输出的列名等。<br><code>-A</code> 省略输出列名。<br><code>数据库名</code> 在语句最后跟上数据库名会直接进入该数据库。<br><code>-A</code> 与<code>--auto-rehash</code>相反，emmmm,看不太懂，大概意思应该是<code>auto-rehash</code> 会变慢，但是可以自动补全，默认也是这种模式。<br><code>--default-character-set=utf8</code> mysql客户端默认使用latin1编码，系统使用其他编码时会出现乱码等问题。使用这个选项可以强制指定编码解决问题。</p>
<p><code>--auto-rehash</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Enable automatic rehashing. This option is on by default, which enables database, </span><br><span class="line">table, and column name completion. Use --disable-auto-rehash to disable rehashing. </span><br><span class="line">That causes mysql to start faster, but you must issue the rehash command if you want </span><br><span class="line">to use name completion.</span><br><span class="line"></span><br><span class="line">To complete a name, enter the first part and press Tab. If the name is unambiguous, </span><br><span class="line">mysql completes it. Otherwise, you can press Tab again to see the possible names </span><br><span class="line">that begin with what you have typed so far. Completion does not occur if there is </span><br><span class="line">no default database.</span><br></pre></td></tr></table></figure>

<p>参看：<a href="https://www.computerhope.com/unix/mysql.htm" target="_blank" rel="noopener">https://www.computerhope.com/unix/mysql.htm</a></p>
<h3 id="修改表结构"><a href="#修改表结构" class="headerlink" title="修改表结构"></a>修改表结构</h3><p><code>alter table table_name1 rename to table_name2</code> 把表名由表1改为表2<br><code>alter table table_name1 modify column field1 field1_type</code>  修改字段长度/字段类型<br><code>alter table table_name1 change old_field new_field field_type</code> 修改字段名字，必须跟上字段类型<br><code>alter table table_name1 add column new_field field_type</code> 新增字段</p>
<h3 id="增删改查"><a href="#增删改查" class="headerlink" title="增删改查"></a>增删改查</h3><p><code>update table_name set field1=&#39;value1&#39;, field2=&#39;value2&#39; [where ...]</code>修改多个字段值 </p>
<h3 id="多个表关联update"><a href="#多个表关联update" class="headerlink" title="多个表关联update"></a>多个表关联update</h3><p><code>UPDATE items,month SET items.price=month.price WHERE items.id=month.id;</code></p>
<h3 id="布尔类型"><a href="#布尔类型" class="headerlink" title="布尔类型"></a>布尔类型</h3><p>首先mysql是不支持布尔类型的，当把一个数据设置成布尔类型的时候,数据库会自动转换成tinyint(1)的数据类型,其实这个就是变相的布尔。<br>默认值也就是1,0两种,分别对应了布尔类型的true和false</p>
<h3 id="create-table-xxx-as-select…"><a href="#create-table-xxx-as-select…" class="headerlink" title="create table xxx as select…"></a>create table xxx as select…</h3><p>创建xxx表，并把select查询的内容直接作为信息插入到xxx表中。注意的是select的字段要记得起别名，否则新建表会自动起一些奇怪的名字。另外注意，select不要用括号括起来，否则会报错。</p>
<h3 id="case-when-then-…-else-…-end"><a href="#case-when-then-…-else-…-end" class="headerlink" title="case when then … else … end"></a>case when then … else … end</h3><p>条件语法，常用于select时,如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CASE WHEN gender&#x3D;&#39;1&#39; THEN &#39;男&#39;</span><br><span class="line">WHEN gender&#x3D;&#39;2&#39; THEN &#39;女&#39;</span><br><span class="line">ELSE &#39;人妖&#39;</span><br><span class="line">END</span><br></pre></td></tr></table></figure>
<h3 id="sql-join"><a href="#sql-join" class="headerlink" title="sql join"></a>sql join</h3><p>join=inner join;<br>left join、right join、full join属于outer join，即等于可以left/right/full [outer] join<br>CROSS JOIN 把表A和表B的数据进行一个N*M的组合，即笛卡尔积</p>
<p><img src="2.png" alt=""></p>
<h4 id="（-inner）join-on"><a href="#（-inner）join-on" class="headerlink" title="（ inner）join on"></a>（ inner）join on</h4><p>输出两张表中的列。<br>select * from table1 inner join table2 on 条件</p>
<h4 id="table1-left-join-table2-on-条件"><a href="#table1-left-join-table2-on-条件" class="headerlink" title="table1 left join table2 on 条件"></a>table1 left join table2 on 条件</h4><p>左表的数据会全部输出来，没有对应数据的补null。（如上面的语句，“全部”体现在id上，即on之后的比较，本来1中是只有在两个id相等时才会select。但是如果是本例中会把左边表的id相关数据全部输出，如左表有id=3右表没有，但是还是会把左表等于3的那列的select数据输出来，右表没有相关数据只能补null）</p>
<h4 id="…right-join-…-on…"><a href="#…right-join-…-on…" class="headerlink" title="…right join ….on…."></a>…right join ….on….</h4><p>右表的数据会全部输出来，没有的补null。<br><a href="https://www.cnblogs.com/dinglinyong/p/6656315.html" target="_blank" rel="noopener">https://www.cnblogs.com/dinglinyong/p/6656315.html</a></p>
<h4 id="UNION-UNION-ALL"><a href="#UNION-UNION-ALL" class="headerlink" title="UNION / UNION ALL"></a>UNION / UNION ALL</h4><p>用于合并两个或多个 SELECT 语句的结果集。<br>UNION 内部的 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。每条 SELECT 语句中的列的顺序必须相同。<br>UNION 操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。<br>UNION 结果集中的列名总是等于 UNION 中第一个 SELECT 语句中的列名。<br>优化建议：能使用union all，就不要使用union。因为union还要对数据进行排序后筛除重复的。比较费时。</p>
<h4 id="FULL"><a href="#FULL" class="headerlink" title="FULL"></a>FULL</h4><p>full join 返回左右表所有的行，即使只有表没有相互匹配。<br><strong><em>不过mysql对full join不支持</em></strong>，可以用join+union的方式来代替。<br>MySQL Full Join的实现 因为MySQL不支持FULL JOIN,下面是替代方法<br>left join + union(可去除重复数据)+ right join</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from A left join B on A.id &#x3D; B.id (where 条件）</span><br><span class="line">union</span><br><span class="line">select *</span><br><span class="line">from A right join B on A.id &#x3D; B.id （where条件);</span><br></pre></td></tr></table></figure>

<h3 id="in-exit"><a href="#in-exit" class="headerlink" title="in/exit"></a>in/exit</h3><p>以优化角度考虑，一般以小表驱动大表。in语句是先执行子语句，exit是后执行子语句；<br>因此，如果子语句是小表就用in，是大表就用exit；<br>in语法：select * from 表A where id in (select id from 表B)<br>exit语法：select * from 表A where exists(select * from 表B where 表B.id=表A.id)</p>
<h3 id="导入导出数据"><a href="#导入导出数据" class="headerlink" title="导入导出数据"></a>导入导出数据</h3><p>linux命令mysql 的参数-e可以后面可以直接跟mysql语句。<br>使用-e在终端执行使在导入/出命令前加上“set character_set_database=utf8;” 可以有效避免中文乱码。<br>导入数据：<br><code>load data local infile &#39;&#39; [replace] into table table_name</code> 如果有replace，会根据主键/唯一索引查询后覆盖插入。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LOAD DATA LOCAL INFILE &#39;&#x2F;home&#x2F;mysql&#x2F;tableExport.txt&#39; INTO TABLE &#96;atp_event&#96;(</span><br><span class="line">event_name, business, event_type, start_time, end_time, event_area, description);</span><br></pre></td></tr></table></figure>
<p><strong><em>如果指定local关键词，则表明从客户主机读文件。如果local没指定，文件必须位于服务器上。</em></strong>使用<code>load data local infile</code>而不是<code>load data infile</code><br>导入数据出错参考：[ERROR 1290 (HY000): The MySQL server is running with the –secure-file-priv option so it cannot execute this statement](/2018/04/19/ERROR 1 HY000 Can’t create or write to file ‘user.txt’ Errcode 13 - Permission denied)<br>导入中文乱码时参考：[导入数据乱码](/2018/04/19/ERROR 1290 HY000  The MySQL server is running with the –secure-file-priv option so it cannot execute this statement)<br>【注：hive中是<code>load data local inpath &#39;&#39; [overwrite] into table table_name</code>】<br>导出数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * into outfile &#39;&#x2F;home&#x2F;chenliclchen&#x2F;mysql&#x2F;t.txt&#39; fields terminated by &#39;,&#39; from atp_event;</span><br></pre></td></tr></table></figure>
<h3 id="导入表结构、表数据"><a href="#导入表结构、表数据" class="headerlink" title="导入表结构、表数据"></a>导入表结构、表数据</h3><p><code>mysqldump -u用戶名 -p密码 [-d] 数据库名 [表名] &gt; 脚本名</code>  有-d 时只导出结构不导数据；指定表名时只导出该表的数据和结构</p>
<h3 id="coalesce-…-…"><a href="#coalesce-…-…" class="headerlink" title="coalesce(…, ….)"></a>coalesce(…, ….)</h3><p>返回第一个不为null的值</p>
<h3 id="count-null-是0"><a href="#count-null-是0" class="headerlink" title="count(null) 是0"></a>count(null) 是0</h3><p>如图，来源于网络的例子<br><img src="1.png" alt=""></p>
<h3 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h3><p>group by 一般和聚合函数一起使用才有意义,比如 count sum avg等,使用group by的两个要素:<br>   (1) 出现在select后面的字段 要么是是聚合函数中的,要么就是group by 中的.<br>   (2) 要筛选结果 可以先使用where 再用group by 或者先用group by 再用having</p>
<h3 id="DECIMAL"><a href="#DECIMAL" class="headerlink" title="DECIMAL"></a>DECIMAL</h3><p><code>DECIMAL(P,D)</code>表示列可以存储D位小数的P位数。<br><strong><em>没有指定括号里的精度时，导入的小数会被截断</em></strong>。<br><strong><em>在hive里曾遇到这样的问题，创建的外部表没有指定精度，外部表指定的内部表有指定精度，从外部表查数据时仍然截断了小数部分。</em></strong></p>
<h3 id="limit分页"><a href="#limit分页" class="headerlink" title="limit分页"></a>limit分页</h3><p><code>LIMIT [offset,] rows</code><br>offset指定要返回的第一行的偏移量,rows第二个指定返回行的最大数目。初始行的偏移量是0(不是1)。<br><code>select * from table_name limit 10,5</code>  查询第11到第15条数据</p>
<h3 id="limit优化"><a href="#limit优化" class="headerlink" title="limit优化"></a>limit优化</h3><p>数据库数据很多的时候会发现分页查询会越来越慢，使用一个id控制就会变快很多（但其实思考到，加入表里数据并不规整有删除的情况，可能会无法使用）。<br>如，select id,name from product limit 866613, 20。——》 select id,name from product where id&gt; 866612 limit 20；</p>
<h3 id="like"><a href="#like" class="headerlink" title="like"></a>like</h3><p>(1)<code>%</code> ：0个或多个；<code>*</code>一个或多个；<code>_</code> <code>?</code> 一个字符。<br>(2)只在首或尾% 和 * 两者都可以使用；如果在头尾同时使用的话,就必须要使用%。</p>
<h3 id="索引类型"><a href="#索引类型" class="headerlink" title="索引类型"></a>索引类型</h3><p>唯一索引：很多情况下，目的不是为了提高访问速度，而只是为了避免数据出现重复。唯一索引可以有多个，但索引列的值必须唯一，索引列的值允许有空值。<br>如果能确定某个数据列将只包含彼此各不相同的值，在为这个数据列创建索引的时候就应该使用关键字UNIQUE，把它定义为一个唯一索引。<br>如果能确定某个数据列将只包含彼此各不相同的值，在为这个数据列创建索引的时候就应该用关键字UNIQUE把它定义为一个唯一索引。这么做的好处：一是简化了MySQL对这个索引的管理工作，这个索引也因此而变得更有效率；二是MySQL会在有新记录插入数据表时，自动检查新记录的这个字段的值是否已经在某个记录的这个字段里出现过了；如果是，MySQL将拒绝插入那条新记录。也就是说，唯一索引可以保证数据记录的唯一性。事实上，在许多场合，人们创建唯一索引的目的往往不是为了提高访问速度，而只是为了避免数据出现重复。<br>（1）创建唯一索可以使用关键字UNIQUE随表一同创建：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE &#96;wb_blog&#96; (</span><br><span class="line">     &#96;id&#96; smallint(8) unsigned NOT NULL,</span><br><span class="line">     &#96;catid&#96; smallint(5) unsigned NOT NULL DEFAULT &#39;0&#39;,</span><br><span class="line">     PRIMARY KEY (&#96;id&#96;),</span><br><span class="line">     UNIQUE KEY &#96;catename&#96; (&#96;catid&#96;)  ) ;</span><br></pre></td></tr></table></figure>
<p>为’catid’字段创建名为catename的唯一索引<br>（2）直接创建<code>CREATE UNIQUE INDEX catename ON wb_blog(catid);</code></p>
<p>参考：<a href="https://www.cnblogs.com/interdrp/p/8031087.html" target="_blank" rel="noopener">https://www.cnblogs.com/interdrp/p/8031087.html</a></p>
<h3 id="replace-into"><a href="#replace-into" class="headerlink" title="replace into"></a>replace into</h3><p><code>replace into t(id, update_time) values(1, now())</code>或者<br><code>replace into t(id, update_time) select 1, now()</code><br>replace into 跟 insert 功能类似，不同点在于：replace into 首先尝试插入数据到表中， 1. 如果发现表中已经有此行数据（根据主键或者唯一索引判断）则先删除此行数据，然后插入新的数据。 2. 否则，直接插入新数据。<br>要注意的是：插入数据的表必须有主键或者是唯一索引！否则的话，replace into 会直接插入数据，这将导致表中出现重复的数据。<br> MySQL replace into 有三种形式：</p>
<ol>
<li><code>replace into tbl_name(col_name, ...) values(...)</code></li>
<li><code>replace into tbl_name(col_name, ...) select ...</code></li>
<li><code>replace into tbl_name set col_name=value, ...</code></li>
</ol>
<p>第一种形式类似于insert into的用法，<br>第二种replace select的用法也类似于insert select，这种用法并不一定要求列名匹配，事实上，MYSQL甚至不关心select返回的列名，它需要的是列的位置。例如，replace into tb1(  name, title, mood) select  rname, rtitle, rmood from tb2; 这个例子使用replace into从 tb2中将所有数据导入tb1中。<br>第三种replace set用法类似于update set用法，使用一个例如“SET col_name = col_name + 1”的赋值，则对位于右侧的列名称的引用会被作为DEFAULT(col_name)处理。因此，该赋值相当于SET col_name = DEFAULT(col_name) + 1。<br>前两种形式用的多些。其中 “into” 关键字可以省略，不过最好加上 “into”，这样意思更加直观。另外，对于那些没有给予值的列，MySQL 将自动为这些列赋上默认值。</p>
<p>参考：<a href="https://blog.csdn.net/zmzwll1314/article/details/51550028" target="_blank" rel="noopener">https://blog.csdn.net/zmzwll1314/article/details/51550028</a></p>
<h3 id="CAST-CONVERT"><a href="#CAST-CONVERT" class="headerlink" title="CAST/CONVERT"></a>CAST/CONVERT</h3><p>转换数据类型</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CAST(value as type);</span><br><span class="line">CONVERT(value, type);</span><br></pre></td></tr></table></figure>
<p>就是<code>CAST(xxx AS 类型)</code>, <code>CONVERT(xxx,类型)</code>。<br>可以转换的类型是有限制的。这个类型可以是以下值其中的一个：<br>二进制，同带binary前缀的效果 : BINARY<br>字符型，可带参数 : <code>CHAR</code><br>日期 : <code>DATE</code><br>时间: <code>TIME</code><br>日期时间型 : <code>DATETIME</code><br>浮点数 : <code>DECIMAL</code><br>整数 : <code>SIGNED</code><br>无符号整数 : <code>UNSIGNED</code></p>
<h3 id="日期"><a href="#日期" class="headerlink" title="日期"></a>日期</h3><h4 id="DATE-FORMAT-date-format"><a href="#DATE-FORMAT-date-format" class="headerlink" title="DATE_FORMAT(date,format)"></a>DATE_FORMAT(date,format)</h4><p>date 参数是合法的日期。format 规定日期/时间的输出格式。</p>
<h4 id="DATE-SUB-date-INTERVAL-expr-type"><a href="#DATE-SUB-date-INTERVAL-expr-type" class="headerlink" title="DATE_SUB(date,INTERVAL expr type)"></a>DATE_SUB(date,INTERVAL expr type)</h4><p>DATE_ADD(date,INTERVAL expr type)<br>date 参数是合法的日期表达式。expr 参数是您希望添加的时间间隔。type 是MINUTE/HOUR/DAY/WEEK 等。</p>
<h4 id="DATEDIFF-date1-date2"><a href="#DATEDIFF-date1-date2" class="headerlink" title="DATEDIFF(date1,date2)"></a>DATEDIFF(date1,date2)</h4><p>date1 和 date2 参数是合法的日期或日期/时间表达式。 返回两个日期之间的天数。</p>
<h4 id="NOW-CURDATE-CURTIME"><a href="#NOW-CURDATE-CURTIME" class="headerlink" title="NOW(),CURDATE(),CURTIME()"></a>NOW(),CURDATE(),CURTIME()</h4><p>当前日期+时间，日期，时间</p>
<h3 id="format-number-length"><a href="#format-number-length" class="headerlink" title="format(number, length)"></a>format(number, length)</h3><p>number是浮点数。length约束浮点数小数位数。</p>
<h3 id="round-x-d"><a href="#round-x-d" class="headerlink" title="round(x[, d])"></a>round(x[, d])</h3><p>x指要处理的数，d是指保留几位小数。用于数据的四舍五入。d默认为0。d可以是负数，这时是指定小数点左边的d位整数位为0,同时小数位均为0。</p>
<h3 id="LEFT-str-len-RIGHT-str-len"><a href="#LEFT-str-len-RIGHT-str-len" class="headerlink" title="LEFT(str,len)/RIGHT(str,len)"></a>LEFT(str,len)/RIGHT(str,len)</h3><p><code>LEFT(str,len)</code><br>返回字符串str的最左面len个字符。<br><code>RIGHT(str,len)</code><br>返回字符串str的最右面len个字符。</p>
<h3 id="length-str"><a href="#length-str" class="headerlink" title="length(str)"></a>length(str)</h3><p>返回字符串的长度。</p>
<h3 id="substr-string-start-length"><a href="#substr-string-start-length" class="headerlink" title="substr(string, start, length)"></a>substr(string, start, length)</h3><p>从start开始截取string字符串，截取长度length。<br>从第一个开始截取start是1.</p>
<h3 id="grant"><a href="#grant" class="headerlink" title="grant"></a>grant</h3><p><code>show grants for username</code> 查看mysql用户权限</p>
<h3 id="group-concat"><a href="#group-concat" class="headerlink" title="group_concat"></a>group_concat</h3><p><code>group_concat([DISTINCT] 要连接的字段 [Order BY 排序字段 ASC/DESC] [Separator &#39;分隔符&#39;] )</code> 分割字符默认是逗号<br>有把一列数据变成一行的功效。 把<code>要连接的字段</code> 这个字段的 所有值由一列变成一行；可按照 <code>排序字段</code> 排序；<code>分隔符</code>是变成一行时的连接符。</p>
<h3 id="ERROR-1690"><a href="#ERROR-1690" class="headerlink" title="ERROR 1690"></a>ERROR 1690</h3><p><code>Mysql ERROR 1690 (22003): BIGINT UNSIGNED value is out of range in..</code><br>因为字段类型为unsigned,所以当相减结果为负值时会报错.<br>解决：使用cast()修改字段类型为signed</p>
]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql导入中文数据乱码</title>
    <url>/2018/04/19/mysql%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E4%B9%B1%E7%A0%81/</url>
    <content><![CDATA[<p>用 LOAD DATA INFILE 命令导入数据时中文是乱码。</p>
<p>解决：</p>
<p>（1）执行下面命令，看编码。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; SHOW VARIABLES LIKE &quot;%CHAR%&quot;;</span><br><span class="line">输出：</span><br><span class="line">+--------------------------+----------------------------+</span><br><span class="line">| Variable_name            | Value                      |</span><br><span class="line">+--------------------------+----------------------------+</span><br><span class="line">| character_set_client     | utf8                       |</span><br><span class="line">| character_set_connection | utf8                       |</span><br><span class="line">| character_set_database   | latin1                     |</span><br><span class="line">| character_set_filesystem | binary                     |</span><br><span class="line">| character_set_results    | utf8                       |</span><br><span class="line">| character_set_server     | latin1                     |</span><br><span class="line">| character_set_system     | utf8                       |</span><br><span class="line">| character_sets_dir       | &#x2F;usr&#x2F;share&#x2F;mysql&#x2F;charsets&#x2F; |</span><br><span class="line">+--------------------------+----------------------------+</span><br></pre></td></tr></table></figure>
<p>（2）把全部编码都改成utf8；我只改了character_set_database和character_set_server：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set character_set_database&#x3D;utf8；</span><br><span class="line">set character_set_server&#x3D;utf8；</span><br></pre></td></tr></table></figure>
<p>（3）再次执行（1）中命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; SHOW VARIABLES LIKE &quot;%CHAR%&quot;;</span><br><span class="line">输出：</span><br><span class="line">+--------------------------+----------------------------+</span><br><span class="line">| Variable_name            | Value                      |</span><br><span class="line">+--------------------------+----------------------------+</span><br><span class="line">| character_set_client     | utf8                       |</span><br><span class="line">| character_set_connection | utf8                       |</span><br><span class="line">| character_set_database   | utf8                       |</span><br><span class="line">| character_set_filesystem | binary                     |</span><br><span class="line">| character_set_results    | utf8                       |</span><br><span class="line">| character_set_server     | utf8                       |</span><br><span class="line">| character_set_system     | utf8                       |</span><br><span class="line">| character_sets_dir       | &#x2F;usr&#x2F;share&#x2F;mysql&#x2F;charsets&#x2F; |</span><br><span class="line">+--------------------------+----------------------------+</span><br></pre></td></tr></table></figure>
<p>这样再导入数据就不会有中文乱码了。</p>
<p>注：（2）中的命令。网上说可以先试用命令： set names utf8; 如果变量的输出仍然不是全部都是utf8，再使用上面的一个一个赋值的方法。</p>
<p>但是上面的办法只能用在一个终端里有效。我们可以通过修改数据库的编码永久解决这个问题：<br>修改数据库编码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter database you_data_base_name character set utf8;</span><br></pre></td></tr></table></figure>
<p>（我后来又试了一下只需要改set character_set_database=utf8;的字符编码就可以了）<br>原因：<br>通过命令 status 可以数据库中的状态：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; status;</span><br><span class="line">--------------</span><br><span class="line">mysql  Ver 14.14 Distrib 5.7.20, for Linux (x86_64) using  EditLine wrapper</span><br><span class="line"></span><br><span class="line">Connection id:        27</span><br><span class="line">Current database:    </span><br><span class="line">Current user:        chenliclchen@localhost</span><br><span class="line">SSL:            Not in use</span><br><span class="line">Current pager:        stdout</span><br><span class="line">Using outfile:        &#39;&#39;</span><br><span class="line">Using delimiter:    ;</span><br><span class="line">Server version:        5.7.20-0ubuntu0.16.04.1 (Ubuntu)</span><br><span class="line">Protocol version:    10</span><br><span class="line">Connection:        Localhost via UNIX socket</span><br><span class="line">Server characterset:    latin1  </span><br><span class="line">Db     characterset:    latin1  ####################################数据库的默认编码</span><br><span class="line">Client characterset:    utf8</span><br><span class="line">Conn.  characterset:    utf8</span><br><span class="line">UNIX socket:        &#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.sock</span><br><span class="line">Uptime:            4 hours 38 min 21 sec</span><br></pre></td></tr></table></figure>
<p>你在执行过上面的（2）（set character_set_database=utf8；）后再执行 status 可以看到 用“#”标出来的那行的‘latin1’变成了‘utf8’.</p>
<p>附：<br>– character_set_server：默认的内部操作字符集<br>– character_set_client：客户端来源数据使用的字符集<br>– character_set_connection：连接层字符集<br>– character_set_results：查询结果字符集<br>– character_set_database：当前选中数据库的默认字符集<br>– character_set_system：系统元数据(字段名等)字符集<br>– 还有以collation_开头的同上面对应的变量，用来描述字符序。</p>
<p>参考：<a href="https://www.2cto.com/database/201408/326102.html" target="_blank" rel="noopener">https://www.2cto.com/database/201408/326102.html</a></p>
]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>sudo cd为什么不能够执行</title>
    <url>/2018/04/19/sudo%20cd%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E8%83%BD%E5%A4%9F%E6%89%A7%E8%A1%8C/</url>
    <content><![CDATA[<p>问题复现： 开始想cd到某个文件夹，但是爆出“没有权限”；<br>         之后想直接用“sudo cd”，会爆出“找不到命令”</p>
<p>原因：cd不是一个应用程序而是Linux内建的命令，而sudo仅仅只对应用程序起作用。例如，sudo qtalk只意味着以root权限运行qtalk程序。</p>
<p>解决方法1：使用sudo -i命令提升用户权限</p>
<blockquote>
<p>sudo -i<br>cd /var/lib/mysql-files</p>
</blockquote>
<p>解决方法2：使用sudo -s命令打开特殊shell</p>
<blockquote>
<p>sudo -s<br>cd /var/lib/mysql-files</p>
</blockquote>
<p>上面都可以使用exit命令退出</p>
<p>参考：<a href="http://blog.csdn.net/u014717036/article/details/70338463" target="_blank" rel="noopener">http://blog.csdn.net/u014717036/article/details/70338463</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>spring aop的所遇到的问题</title>
    <url>/2018/04/08/SPRING%20AOP%E7%9A%84%E6%89%80%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Post-processing of the FactoryBean&#39;s object failed; nested exception is org.springframework.aop.framework.AopConfigException: Could not generate CGLIB subclass of class [class com.sun.proxy.$Proxy68]: Common causes of this problem include using a final class or a non-visible class; nested exception is java.lang.IllegalArgumentException: Cannot subclass final class class com.sun.proxy.$Proxy68: java.lang.IllegalArgumentException: Cannot subclass final class class com.sun.proxy.$Proxy68</span><br></pre></td></tr></table></figure>
<p>分析原因<br>代理了final修饰的类。可是哪里来的final类？ 原来，DAO层使用的是mybatis，可以只写接口不用写实现类。而我们项目中就是没有写实现类。<br>只在使用mybatis时，dao只有接口的方案时出了错，如果dao层仍然使用sqlSession的select方法不会出错。</p>
<p>详细：<a href="http://sparkgis.com/java/2017/08/%E8%AE%B0%E4%B8%80%E6%AC%A1spring-aop%E7%9A%84%E6%89%80%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">http://sparkgis.com/java/2017/08/%E8%AE%B0%E4%B8%80%E6%AC%A1spring-aop%E7%9A%84%E6%89%80%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</a></p>
]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>SPRING AOP的所遇到的问题</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring+Quartz实现定时任务的配置方法</title>
    <url>/2018/04/08/Spring+Quartz%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>实现比较麻烦，建议看<a href="/2018/04/08/spring用Scheduled注解方式实现定时任务">另一篇</a>实现使用注解的方式，更简洁。<br>普通类方法。例子主要功能，每分钟输出“everyMinute”，每天18点输出“hours”</p>
<h3 id="1-增加依赖库"><a href="#1-增加依赖库" class="headerlink" title="1.增加依赖库"></a>1.增加依赖库</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-context-support<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3.10<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.quartz-scheduler<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>quartz<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-普通类"><a href="#2-普通类" class="headerlink" title="2.普通类"></a>2.普通类</h3><p>输出类  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.qq.fresh.testqmq;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TestQuartz</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 17-11-2 下午5:19</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestQuartz</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">everyMinute</span><span class="params">()</span></span>&#123;</span><br><span class="line">    log.info(<span class="string">"everyMinute"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">hours</span><span class="params">()</span></span>&#123;</span><br><span class="line">    log.info(<span class="string">"hours"</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-业务类的配置"><a href="#3-业务类的配置" class="headerlink" title="3.业务类的配置"></a>3.业务类的配置</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--业务类的配置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"job"</span> <span class="attr">class</span>=<span class="string">"com.qq.fresh.testqmq.TestQuartz"</span>/&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-JobDetail的配置"><a href="#4-JobDetail的配置" class="headerlink" title="4.JobDetail的配置"></a>4.JobDetail的配置</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--JobDetail的配置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">name</span>=<span class="string">"minute"</span></span></span><br><span class="line"><span class="tag"> <span class="attr">class</span>=<span class="string">"org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"targetObject"</span> <span class="attr">ref</span>=<span class="string">"job"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"targetMethod"</span> <span class="attr">value</span>=<span class="string">"everyMinute"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">name</span>=<span class="string">"hours"</span></span></span><br><span class="line"><span class="tag"> <span class="attr">class</span>=<span class="string">"org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"targetObject"</span> <span class="attr">ref</span>=<span class="string">"job"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"targetMethod"</span> <span class="attr">value</span>=<span class="string">"hours"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="5-Trigger的配置"><a href="#5-Trigger的配置" class="headerlink" title="5.Trigger的配置"></a>5.Trigger的配置</h3><p>定时的cron定义 主要是  秒 分 时 日 月 周 年(可选) 特殊字符主要代表什么，可参看下面的2/3链接.  </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--Trigger的配置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"everyMinute"</span> <span class="attr">class</span>=<span class="string">"org.springframework.scheduling.quartz.CronTriggerFactoryBean"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"jobDetail"</span> <span class="attr">ref</span>=<span class="string">"minute"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"startDelay"</span> <span class="attr">value</span>=<span class="string">"0"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"cronExpression"</span> <span class="attr">value</span>=<span class="string">"0 */1 * * * ?"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"anHours"</span> <span class="attr">class</span>=<span class="string">"org.springframework.scheduling.quartz.CronTriggerFactoryBean"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"jobDetail"</span> <span class="attr">ref</span>=<span class="string">"hours"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"cronExpression"</span> <span class="attr">value</span>=<span class="string">"0 0 18 * * ?"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="6-Scheduler的配置"><a href="#6-Scheduler的配置" class="headerlink" title="6.Scheduler的配置"></a>6.Scheduler的配置</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--Scheduler的配置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">class</span>=<span class="string">"org.springframework.scheduling.quartz.SchedulerFactoryBean"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"triggers"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">list</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ref</span> <span class="attr">bean</span>=<span class="string">"everyMinute"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ref</span> <span class="attr">bean</span>=<span class="string">"anHours"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">list</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"quartzProperties"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">props</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">prop</span> <span class="attr">key</span>=<span class="string">"org.quartz.threadPool.threadCount"</span>&gt;</span>1<span class="tag">&lt;/<span class="name">prop</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">props</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>参考：  </p>
<p>1. <a href="http://blog.csdn.net/javawebxy/article/details/50492409" target="_blank" rel="noopener"> http://blog.csdn.net/javawebxy/article/details/50492409
</a></p>
<p>2. <a href="http://www.cnblogs.com/happyday56/p/4164877.html" target="_blank" rel="noopener"> http://www.cnblogs.com/happyday56/p/4164877.html
</a></p>
<p>3. <a href="http://www.cnblogs.com/henuyuxiang/p/4152805.html" target="_blank" rel="noopener"> http://www.cnblogs.com/henuyuxiang/p/4152805.html
</a></p>
]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>Spring Quartz</tag>
        <tag>定时任务</tag>
      </tags>
  </entry>
  <entry>
    <title>cmake安装遇到问题总结</title>
    <url>/2018/04/08/cmake%E5%AE%89%E8%A3%85%E9%81%87%E5%88%B0%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>推荐cmake安装文章 <a href="http://www.cnblogs.com/emouse/archive/2013/02/22/2922940.html" target="_blank" rel="noopener"> 点击打开链接
</a> 以及opencv例子执行。  </p>
<h3 id="qmake"><a href="#qmake" class="headerlink" title="qmake"></a>qmake</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">qmake: could not exec &#39;&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;qt4&#x2F;bin&#x2F;qmake&#39;: No such</span><br><span class="line">file or directory</span><br></pre></td></tr></table></figure>
<p>解决： </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install qt-sdk  (470M,so heavy)</span><br></pre></td></tr></table></figure>
<p>but some people use:  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install qt4-qmake</span><br></pre></td></tr></table></figure>
<p>don’t konw if is ok.you can try  </p>
<h3 id="numpy-distutils"><a href="#numpy-distutils" class="headerlink" title="numpy.distutils"></a>numpy.distutils</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ImportError: No module named numpy.distutils</span><br></pre></td></tr></table></figure>
<p>解决：  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-numpy</span><br></pre></td></tr></table></figure>
<h3 id="没有权限"><a href="#没有权限" class="headerlink" title="没有权限"></a>没有权限</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CMake Error at cmake_install.cmake:36 (FILE):  </span><br><span class="line">file INSTALL cannot set permissions on  </span><br><span class="line">&quot;&#x2F;usr&#x2F;local&#x2F;include&#x2F;opencv2&#x2F;opencv_modules.hpp&quot;</span><br></pre></td></tr></table></figure>
<p>原因：没有权限<br>解决：  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>caffe安装&amp;&amp;问题&amp;&amp;解决</category>
      </categories>
      <tags>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title>fast-rcnn安装及例子执行中的问题（一）</title>
    <url>/2018/04/08/fast-rcnn%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BE%8B%E5%AD%90%E6%89%A7%E8%A1%8C%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p><a href="http://weibo.com/p/230418855a82cd0102vnjq" target="_blank" rel="noopener"> 推荐安装文章 </a>  </p>
<h4 id="skimage-io"><a href="#skimage-io" class="headerlink" title="skimage.io"></a>skimage.io</h4><p>问题：ImportError: No module named skimage.io<br>解决：sudo pip install scikit-image<br>接着出错：ImportError: No module named scipy<br>再安装：sudo pip install scipy<br>出错：error: library dfftpack has Fortran sources but no Fortran compiler found<br>再安装：ruby -e “$(curl -fsSL <a href="https://raw.github.com/mxcl/homebrew/go)&quot;" target="_blank" rel="noopener">https://raw.github.com/mxcl/homebrew/go)&quot;</a><br>sudo apt-get install gfortran  </p>
<p><strong><em>补充</em></strong><br>今天（原文半年之后）再次遇到，解决：</p>
<p>先 [ 换源 ](/2018/04/08/ubuntu 安装numpy的烂问题libgfortran3依赖)<br>再执行：</p>
<pre><code>sudo apt-get install python-skimage</code></pre><h4 id="google-protobuf"><a href="#google-protobuf" class="headerlink" title="google.protobuf"></a>google.protobuf</h4><p>问题： </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from google.protobuf.internal import enum_type_wrapper  </span><br><span class="line">ImportError: No module named google.protobuf.internal</span><br></pre></td></tr></table></figure>
<p>解决(must use sudo,and when it perform,it will auto install mavproxy next.)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo pip install droneapi</span><br></pre></td></tr></table></figure>
<p>or you can look for this paper:<a href="https://github.com/dronekit/dronekit-python/issues/121" target="_blank" rel="noopener">https://github.com/dronekit/dronekit-python/issues/121</a></p>
<p>and there is another people say this:<a href="http://www.cnblogs.com/taokongcn/p/4341290.html" target="_blank" rel="noopener">http://www.cnblogs.com/taokongcn/p/4341290.html</a></p>
<h4 id="内存出错"><a href="#内存出错" class="headerlink" title="内存出错"></a>内存出错</h4><p>问题：在尝试demo时出现错误（具体英文没有记，大概意思是说多少多少内存不够，或者直接出现“已杀死”的字样）<br>解决：添加执行参数：./tools/demo.py –cpu –net caffenet<br>原因：猜测是因为内存的原因导致caffe崩溃。</p>
]]></content>
      <categories>
        <category>faster rcnn</category>
        <category>fast rcnn问题解决&amp;&amp;安装</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>fast rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title>fast-rcnn的例子执行和selective search中遇到的问题及解决（二）</title>
    <url>/2018/04/08/fast-rcnn%E7%9A%84%E4%BE%8B%E5%AD%90%E6%89%A7%E8%A1%8C%E5%92%8Cselective%20search%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<p>1.出现了EnvironmentError: MATLAB command ‘matlab’ not found.Please add ‘matlab’ to yourPATH.这种错误<br>原因：没有把matlab的路径添加到环境变量中<br>解决：设置环境变量 export PATH=$PATH:”/home/cl/install/MATLAB/bin”  </p>
<p>2.这个问题在selective search的执行过程中出的问题。github上的那个用python调用MATLAB的例子。在执行那句调用MATLAB时报的错。<br>原因：MATLAB没起来。<br>问题：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">selective_search_rcnn(&#123;<span class="string">'/home/cl/examples/images/00001.jpg'</span>&#125;,</span><br><span class="line"><span class="string">'/tmp/tmpZ0B5zO.mat'</span>)</span><br><span class="line"></span><br><span class="line">Traceback (most recent call last):  </span><br><span class="line">File <span class="string">"../python/detect.py"</span>, line <span class="number">168</span>, <span class="keyword">in</span>  </span><br><span class="line">main(sys.argv)  </span><br><span class="line">File <span class="string">"../python/detect.py"</span>, line <span class="number">139</span>, <span class="keyword">in</span> main  </span><br><span class="line">detections = detector.detect_selective_search(inputs)  </span><br><span class="line">File <span class="string">"/home/hank/Projects/caffe/python/caffe/detector.py"</span>, line <span class="number">119</span>, <span class="keyword">in</span></span><br><span class="line">detect_selective_search  </span><br><span class="line">cmd=<span class="string">'selective_search_rcnn'</span>  </span><br><span class="line">File</span><br><span class="line"><span class="string">"/usr/lib/python2.7/selective_search_ijcv_with_python/selective_search.py"</span>,</span><br><span class="line">line <span class="number">39</span>, <span class="keyword">in</span> get_windows  </span><br><span class="line">shlex.split(mc), stdout=open(<span class="string">'/dev/null'</span>, <span class="string">'w'</span>), cwd=script_dirname)  </span><br><span class="line">File <span class="string">"/usr/lib/python2.7/subprocess.py"</span>, line <span class="number">710</span>, <span class="keyword">in</span> init  </span><br><span class="line">errread, errwrite)  </span><br><span class="line">File <span class="string">"/usr/lib/python2.7/subprocess.py"</span>, line <span class="number">1327</span>, <span class="keyword">in</span> _execute_child  </span><br><span class="line"><span class="keyword">raise</span> child_exception  </span><br><span class="line">OSError: [Errno <span class="number">2</span>] No such file <span class="keyword">or</span> directory</span><br></pre></td></tr></table></figure>
<p>解决：在~/.bashrc中添加你的matlab的bin路径（例如，我的是exportPATH=$PATH:”/home/cl/install/Matlab/bin”）。<br>记得source是没有用的，需要重启才可以  </p>
]]></content>
      <categories>
        <category>faster rcnn</category>
        <category>fast rcnn问题解决&amp;&amp;安装</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>fast rcnn</tag>
        <tag>selective search</tag>
      </tags>
  </entry>
  <entry>
    <title>fast-rcnn训练自己数据集以及demo代码解读和总结（面向fast-rcnn初学者）</title>
    <url>/2018/04/08/fast-rcnn%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%A5%E5%8F%8Ademo%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%E5%92%8C%E6%80%BB%E7%BB%93%EF%BC%88%E9%9D%A2%E5%90%91fast-rcnn%E5%88%9D%E5%AD%A6%E8%80%85%EF%BC%89/</url>
    <content><![CDATA[<p>首先推荐 <a href="http://www.cnblogs.com/louyihang-loves-baiyan/p/4885659.html" target="_blank" rel="noopener"> 文章</a>里面有讲如何安装fast-rcnn，以及编译。</p>
<p>fast-rcnn的地址：<a href="https://github.com/rbgirshick/fast-rcnn" target="_blank" rel="noopener">https://github.com/rbgirshick/fast-rcnn</a></p>
<h2 id="demo-py代码解读："><a href="#demo-py代码解读：" class="headerlink" title="demo.py代码解读："></a><a href="地址：https://github.com/rbgirshick/fast-rcnn/blob/master/tools/demo.py">demo.py</a>代码解读：</h2><p>1.获取参数类型得到训练的类型，找到它的porototxt和model。<br>2.net=caffe.Net(c1,c2,c3)得到网络c1是porototxt，c2是model，c3固定。<br>3.进入demo（网络，图片名字，检测类别)方法<br>3.1soi加载“图片_boxes.mat”,这是用selective search做的预处理文件。<br>3.2cv2加载图片。<br>3.3fast_rcnn.test.im_detect(c1,c2,c3),c1是net网络，c2是加载后的图片，c3是加载后的预处理文件。但是fast_r<br>cnn.test不知道是什么。返回的是“分数，boxes”【】<br>4.画出结果<br>4.1.由程序可知，CLASSES里是根据voc写的，因此顺序不可改变。由类别名称得到索引。<br>4.2.把这个索引对应的scores里的分数定义一个阀值，把大于阀值的索引返回（where的语法仍然不对）<br>4.3.得到相应boxes的值，并把scores的值和boxes的值水平合并<br>4.4.送入画图函数<br>5.画图。  </p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>Fast RCNN中，提取OP的过程和训练过程仍然是分离的。<br>在训练过程中，需要用OP的方法先把图像OP提取好，再送入Fast RCNN中训练，<br>在检测过程中也是如此需要先把相应的测试图像的OP提取出来送入检测。</p>
<p>阀值很重要，太大会导致有些图识别得到object。太小又会识别太多（不必要的），哈哈，你试试就知道啦。</p>
<h2 id="直接用它的代码训练自己的数据（简单格式的）"><a href="#直接用它的代码训练自己的数据（简单格式的）" class="headerlink" title="直接用它的代码训练自己的数据（简单格式的）"></a>直接用它的代码训练自己的数据（简单格式的）</h2><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>由代码可以看出图片放在了fast-rcnn主目录下的/data/demo下，图片后缀是’jpg’;</p>
<p>另外我们还需要一个提取出来的op，即一个后缀名是mat的文件，它和图片放在同一个目录下。这个我们可以用网上的开源代码，<a href="https://github.com/sergeyk/selective_search_ijcv_with_python" target="_blank" rel="noopener">selective search</a><br>使用方法网站上有，如果使用中出错请参考我的[另一片文章](/2018/04/09/fast-rcnn的例子执行和selective search中遇到的问题及解决（二）)。</p>
<p>but，其实呢，我最初给的那个博主的可以识别出来很多车的情况，我这是没有的。大概忘了，貌似也就三四个的样子。</p>
<p>估计你还想知道怎么训练自己的model，那个还需要自己先动手标出真正的结果，因为我当时也就试试，没有真正的要用，所以就没用那个精力去做了。</p>
<p>名词解释：ROI : Region of Interest  </p>
]]></content>
      <categories>
        <category>faster rcnn</category>
        <category>fast rcnn</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>fast rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn修改demo.py保存网络中间结果</title>
    <url>/2018/04/08/faster%20rcnn%E4%BF%AE%E6%94%B9demo.py%E4%BF%9D%E5%AD%98%E7%BD%91%E7%BB%9C%E4%B8%AD%E9%97%B4%E7%BB%93%E6%9E%9C/</url>
    <content><![CDATA[<p>faster rcnn用python版本 <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="noopener"> https://github.com/rbgirshick/py-faster-rcnn
</a>  </p>
<p>以demo.py中默认网络VGG16.</p>
<p>原本demo.py地址 [ <a href="https://github.com/rbgirshick/py-faster-" target="_blank" rel="noopener">https://github.com/rbgirshick/py-faster-</a><br>rcnn/blob/master/tools/demo.py ](<a href="https://github.com/rbgirshick/py-faster-" target="_blank" rel="noopener">https://github.com/rbgirshick/py-faster-</a><br>rcnn/blob/master/tools/demo.py)</p>
<h3 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h3><p>图有点多，贴一个图的部分结果出来：</p>
<p><img src="17.jpeg" alt="">  </p>
<p>上图是原图；<br>下面第一张是网络中命名为“conv1_1”的结果图；<br>第二张是命名为“rpn_cls_prob_reshape”的结果图；<br>第三张是“rpnoutput”的结果图</p>
<p><img src="18.jpeg" alt=""> <img src="19.jpeg" alt=""> <img src="20.jpeg" alt=""></p>
<h3 id="修改后的代码"><a href="#修改后的代码" class="headerlink" title="修改后的代码"></a>修改后的代码</h3><p>看一下我修改后的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Faster R-CNN</span></span><br><span class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></span><br><span class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></span><br><span class="line"><span class="comment"># Written by Ross Girshick</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Demo script showing detections in sample images.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">See README.md for installation instructions before running.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> _init_paths</span><br><span class="line"><span class="keyword">from</span> fast_rcnn.config <span class="keyword">import</span> cfg</span><br><span class="line"><span class="keyword">from</span> fast_rcnn.test <span class="keyword">import</span> im_detect</span><br><span class="line"><span class="keyword">from</span> fast_rcnn.nms_wrapper <span class="keyword">import</span> nms</span><br><span class="line"><span class="keyword">from</span> utils.timer <span class="keyword">import</span> Timer</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">import</span> caffe, os, sys, cv2</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">CLASSES = (<span class="string">'__background__'</span>,</span><br><span class="line">           <span class="string">'aeroplane'</span>, <span class="string">'bicycle'</span>, <span class="string">'bird'</span>, <span class="string">'boat'</span>,</span><br><span class="line">           <span class="string">'bottle'</span>, <span class="string">'bus'</span>, <span class="string">'car'</span>, <span class="string">'cat'</span>, <span class="string">'chair'</span>,</span><br><span class="line">           <span class="string">'cow'</span>, <span class="string">'diningtable'</span>, <span class="string">'dog'</span>, <span class="string">'horse'</span>,</span><br><span class="line">           <span class="string">'motorbike'</span>, <span class="string">'person'</span>, <span class="string">'pottedplant'</span>,</span><br><span class="line">           <span class="string">'sheep'</span>, <span class="string">'sofa'</span>, <span class="string">'train'</span>, <span class="string">'tvmonitor'</span>)</span><br><span class="line"></span><br><span class="line">NETS = &#123;<span class="string">'vgg16'</span>: (<span class="string">'VGG16'</span>,</span><br><span class="line">                  <span class="string">'VGG16_faster_rcnn_final.caffemodel'</span>),</span><br><span class="line">        <span class="string">'zf'</span>: (<span class="string">'ZF'</span>,</span><br><span class="line">                  <span class="string">'ZF_faster_rcnn_final.caffemodel'</span>)&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vis_detections</span><span class="params">(im, class_name, dets, thresh=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Draw detected bounding boxes."""</span></span><br><span class="line">    inds = np.where(dets[:, <span class="number">-1</span>] &gt;= thresh)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> len(inds) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    im = im[:, :, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>)]</span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">    ax.imshow(im, aspect=<span class="string">'equal'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> inds:</span><br><span class="line">        bbox = dets[i, :<span class="number">4</span>]</span><br><span class="line">        score = dets[i, <span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        ax.add_patch(</span><br><span class="line">            plt.Rectangle((bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]),</span><br><span class="line">                          bbox[<span class="number">2</span>] - bbox[<span class="number">0</span>],</span><br><span class="line">                          bbox[<span class="number">3</span>] - bbox[<span class="number">1</span>], fill=<span class="literal">False</span>,</span><br><span class="line">                          edgecolor=<span class="string">'red'</span>, linewidth=<span class="number">3.5</span>)</span><br><span class="line">            )</span><br><span class="line">        ax.text(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>] - <span class="number">2</span>,</span><br><span class="line">                <span class="string">'&#123;:s&#125; &#123;:.3f&#125;'</span>.format(class_name, score),</span><br><span class="line">                bbox=dict(facecolor=<span class="string">'blue'</span>, alpha=<span class="number">0.5</span>),</span><br><span class="line">                fontsize=<span class="number">14</span>, color=<span class="string">'white'</span>)</span><br><span class="line"></span><br><span class="line">    ax.set_title((<span class="string">'&#123;&#125; detections with '</span></span><br><span class="line">                  <span class="string">'p(&#123;&#125; | box) &gt;= &#123;:.1f&#125;'</span>).format(class_name, class_name,</span><br><span class="line">                                                  thresh),</span><br><span class="line">                  fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    <span class="comment">#plt.draw()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_feature_picture</span><span class="params">(data, name, image_name=None, padsize = <span class="number">1</span>, padval = <span class="number">1</span>)</span>:</span></span><br><span class="line">    data = data[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">#print "data.shape1: ", data.shape</span></span><br><span class="line">    n = int(np.ceil(np.sqrt(data.shape[<span class="number">0</span>])))</span><br><span class="line">    padding = ((<span class="number">0</span>, n ** <span class="number">2</span> - data.shape[<span class="number">0</span>]), (<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, padsize)) + ((<span class="number">0</span>, <span class="number">0</span>),) * (data.ndim - <span class="number">3</span>)</span><br><span class="line">    <span class="comment">#print "padding: ", padding</span></span><br><span class="line">    data = np.pad(data, padding, mode=<span class="string">'constant'</span>, constant_values=(padval, padval))</span><br><span class="line">    <span class="comment">#print "data.shape2: ", data.shape</span></span><br><span class="line">    </span><br><span class="line">    data = data.reshape((n, n) + data.shape[<span class="number">1</span>:]).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>) + tuple(range(<span class="number">4</span>, data.ndim + <span class="number">1</span>)))</span><br><span class="line">    <span class="comment">#print "data.shape3: ", data.shape, n</span></span><br><span class="line">    data = data.reshape((n * data.shape[<span class="number">1</span>], n * data.shape[<span class="number">3</span>]) + data.shape[<span class="number">4</span>:])</span><br><span class="line">    <span class="comment">#print "data.shape4: ", data.shape</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.imshow(data,cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    <span class="comment">#plt.show()</span></span><br><span class="line">    <span class="keyword">if</span> image_name == <span class="literal">None</span>:</span><br><span class="line">        img_path = <span class="string">'./data/feature_picture/'</span> </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img_path = <span class="string">'./data/feature_picture/'</span> + image_name + <span class="string">"/"</span></span><br><span class="line">        check_file(img_path)</span><br><span class="line">    plt.savefig(img_path + name + <span class="string">".jpg"</span>, dpi = <span class="number">400</span>, bbox_inches = <span class="string">"tight"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_file</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">        os.mkdir(path)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(net, image_name)</span>:</span></span><br><span class="line">    <span class="string">"""Detect object classes in an image using pre-computed object proposals."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load the demo image</span></span><br><span class="line">    im_file = os.path.join(cfg.DATA_DIR, <span class="string">'demo'</span>, image_name)</span><br><span class="line">    im = cv2.imread(im_file)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Detect all object classes and regress object bounds</span></span><br><span class="line">    timer = Timer()</span><br><span class="line">    timer.tic()</span><br><span class="line">    scores, boxes = im_detect(net, im)</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> net.blobs.items():</span><br><span class="line">        <span class="keyword">if</span> k.find(<span class="string">"conv"</span>)&gt;<span class="number">-1</span> <span class="keyword">or</span> k.find(<span class="string">"pool"</span>)&gt;<span class="number">-1</span> <span class="keyword">or</span> k.find(<span class="string">"rpn"</span>)&gt;<span class="number">-1</span>:</span><br><span class="line">            save_feature_picture(v.data, k.replace(<span class="string">"/"</span>, <span class="string">""</span>), image_name)<span class="comment">#net.blobs["conv1_1"].data, "conv1_1") </span></span><br><span class="line">    timer.toc()</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'Detection took &#123;:.3f&#125;s for '</span></span><br><span class="line">           <span class="string">'&#123;:d&#125; object proposals'</span>).format(timer.total_time, boxes.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Visualize detections for each class</span></span><br><span class="line">    CONF_THRESH = <span class="number">0.8</span></span><br><span class="line">    NMS_THRESH = <span class="number">0.3</span></span><br><span class="line">    <span class="keyword">for</span> cls_ind, cls <span class="keyword">in</span> enumerate(CLASSES[<span class="number">1</span>:]):</span><br><span class="line">        cls_ind += <span class="number">1</span> <span class="comment"># because we skipped background</span></span><br><span class="line">        cls_boxes = boxes[:, <span class="number">4</span>*cls_ind:<span class="number">4</span>*(cls_ind + <span class="number">1</span>)]</span><br><span class="line">        cls_scores = scores[:, cls_ind]</span><br><span class="line">        dets = np.hstack((cls_boxes,</span><br><span class="line">                          cls_scores[:, np.newaxis])).astype(np.float32)</span><br><span class="line">        keep = nms(dets, NMS_THRESH)</span><br><span class="line">        dets = dets[keep, :]</span><br><span class="line">        vis_detections(im, cls, dets, thresh=CONF_THRESH)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Parse input arguments."""</span></span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">'Faster R-CNN demo'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--gpu'</span>, dest=<span class="string">'gpu_id'</span>, help=<span class="string">'GPU device id to use [0]'</span>,</span><br><span class="line">                        default=<span class="number">0</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--cpu'</span>, dest=<span class="string">'cpu_mode'</span>,</span><br><span class="line">                        help=<span class="string">'Use CPU mode (overrides --gpu)'</span>,</span><br><span class="line">                        action=<span class="string">'store_true'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--net'</span>, dest=<span class="string">'demo_net'</span>, help=<span class="string">'Network to use [vgg16]'</span>,</span><br><span class="line">                        choices=NETS.keys(), default=<span class="string">'vgg16'</span>)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> args</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_param</span><span class="params">(net)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> net.blobs.items():</span><br><span class="line">	<span class="keyword">print</span> (k, v.data.shape)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">""</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> net.params.items():</span><br><span class="line">	<span class="keyword">print</span> (k, v[<span class="number">0</span>].data.shape)  </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    cfg.TEST.HAS_RPN = <span class="literal">True</span>  <span class="comment"># Use RPN for proposals</span></span><br><span class="line"></span><br><span class="line">    args = parse_args()</span><br><span class="line"></span><br><span class="line">    prototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][<span class="number">0</span>],</span><br><span class="line">                            <span class="string">'faster_rcnn_alt_opt'</span>, <span class="string">'faster_rcnn_test.pt'</span>)</span><br><span class="line">    <span class="comment">#print "prototxt: ", prototxt</span></span><br><span class="line">    caffemodel = os.path.join(cfg.DATA_DIR, <span class="string">'faster_rcnn_models'</span>,</span><br><span class="line">                              NETS[args.demo_net][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(caffemodel):</span><br><span class="line">        <span class="keyword">raise</span> IOError((<span class="string">'&#123;:s&#125; not found.\nDid you run ./data/script/'</span></span><br><span class="line">                       <span class="string">'fetch_faster_rcnn_models.sh?'</span>).format(caffemodel))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.cpu_mode:</span><br><span class="line">        caffe.set_mode_cpu()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        caffe.set_mode_gpu()</span><br><span class="line">        caffe.set_device(args.gpu_id)</span><br><span class="line">        cfg.GPU_ID = args.gpu_id</span><br><span class="line">    net = caffe.Net(prototxt, caffemodel, caffe.TEST)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#print_param(net)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n\nLoaded network &#123;:s&#125;'</span>.format(caffemodel)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Warmup on a dummy image</span></span><br><span class="line">    im = <span class="number">128</span> * np.ones((<span class="number">300</span>, <span class="number">500</span>, <span class="number">3</span>), dtype=np.uint8)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">2</span>):</span><br><span class="line">        _, _= im_detect(net, im)</span><br><span class="line"></span><br><span class="line">    im_names = [<span class="string">'000456.jpg'</span>, <span class="string">'000542.jpg'</span>, <span class="string">'001150.jpg'</span>,</span><br><span class="line">                <span class="string">'001763.jpg'</span>, <span class="string">'004545.jpg'</span>]</span><br><span class="line">    <span class="keyword">for</span> im_name <span class="keyword">in</span> im_names:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Demo for data/demo/&#123;&#125;'</span>.format(im_name)</span><br><span class="line">        demo(net, im_name)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#plt.show()</span></span><br></pre></td></tr></table></figure>
<h3 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h3><p>1.在data下手动创建“feature_picture”文件夹就可以替换原来的demo使用了。</p>
<p>2.上面代码主要添加方法是：save_feature_picture，它会对网络测试的某些阶段的数据处理然后保存。</p>
<p>3.某些阶段是因为：if k.find(“conv”)&gt;-1 or k.find(“pool”)&gt;-1 or k.find(“rpn”)&gt;-1这行代码（110行），保证网络层name有这三个词的才会被保存，因为其他层无法用图片保存，如全连接（参数已经是二维的了）等层。</p>
<p>4.放开174行print_param(net)的注释，就可以看到网络参数的输出。</p>
<p>5.执行的最终结果 是在data/feature_picture产生以图片名字为文件夹名字的文件夹，文件夹下有以网络每层name为名字的图片。</p>
<p>6.另外部分网络的层name中有非法字符不能作为图片名字，我在代码的111行只是把‘字符/’剔除掉了，所以建议网络名字不要又其他字符。  </p>
<h3 id="图片下载和代码下载"><a href="#图片下载和代码下载" class="headerlink" title="图片下载和代码下载"></a>图片下载和代码下载</h3><pre><code>git clone https://github.com/meihuakaile/faster-rcnn.git</code></pre>]]></content>
      <categories>
        <category>faster rcnn</category>
        <category>faster cnn源码理解</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>faster rcnn中间层显示</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn源码解读总结</title>
    <url>/2018/04/08/faster%20rcnn%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>1. 初始数据通过imdb类的操作放在它的属性roidb里。</p>
<p>2. roidb只是一个字典，可以拿出来当做一个单独的字典，脱离imdb  。</p>
<p>3. roi_data_layer下的layer就是input-data。Forward中加载数据并控制一次一张图片的数据进入网络。送到rpn-data中三组数据：</p>
<ul>
<li>gt_boxes：大小（一张图片的xml中box个数, 5）；一张图中box的坐标以及类别</li>
<li>data：大小（1, 3, 高, 宽）；一张图的数据</li>
<li>im_info：大小（1, 3）；（高, 宽, 下面提到的比例）</li>
</ul>
<p>图片的大小与原图不同，每张图的高或宽被 rescale 成 600，另一边会按照相同的比例 rescale（代码出处未找到，且不懂这样的原因？？？？？？）</p>
<p>4. AnchorTargetLayer 就是rpn-data.计算 anchors,以及anchors是否合理（大小，overlap），并根据每个anchor与gt_box的重叠度判断labels；<br>anchors大小是卷积网络过来数据的高宽再乘9个（即，一个点有9个）.<br>最后产生四组数据（设k=len(anchors)）：</p>
<ul>
<li>labels：大小（k, 1）； 前景=1，背景=0，否则=-1</li>
<li>rpn_bbox_targets:  大小(k, 4)</li>
<li>bbox_inside_weights:  大小（k, 4）; 有前景=1，否则为0</li>
<li>bbox_outside_weights:  大小 (k, 4); 有前景或背景=1/（前景+背景），否则为0</li>
</ul>
<p>5. 区别 </p>
<ul>
<li>cfg.TRIAIN.IMS_PER_BATCH: 是训练proposals时的batch size, 在mini中被默认为1不能修改。</li>
<li>cfg.TRAIN.BATCH_SIZE: 是后面基于proposal训练的时候的batch size</li>
<li>cfg.TRAIN.RPN_BATCHSIZE: 是控制第一步训练的结果proposals的个数，在AnchorTargetLayer中被定义为256（如果想修改proposal个数可以修改）。</li>
</ul>
]]></content>
      <categories>
        <category>faster rcnn</category>
        <category>faster cnn源码理解</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>faster rcnn源码理解</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn源码理解（二）之AnchorTargetLayer（网络中的rpn_data）</title>
    <url>/2018/04/08/faster%20rcnn%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8BAnchorTargetLayer%EF%BC%88%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84rpn_data%EF%BC%89/</url>
    <content><![CDATA[<p>faster用python版本的 <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="noopener"> https://github.com/rbgirshick/py-faster-rcnn</a></p>
<p>AnchorTargetLayer源码在 <a href="https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/rpn/anchor_target_layer.py" target="_blank" rel="noopener"> https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/rpn/anchor_target_layer.py</a>  </p>
<h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Faster R-CNN</span></span><br><span class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></span><br><span class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></span><br><span class="line"><span class="comment"># Written by Ross Girshick and Sean Bell</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> caffe</span><br><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"><span class="keyword">from</span> fast_rcnn.config <span class="keyword">import</span> cfg</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy.random <span class="keyword">as</span> npr</span><br><span class="line"><span class="keyword">from</span> generate_anchors <span class="keyword">import</span> generate_anchors</span><br><span class="line"><span class="keyword">from</span> utils.cython_bbox <span class="keyword">import</span> bbox_overlaps</span><br><span class="line"><span class="keyword">from</span> fast_rcnn.bbox_transform <span class="keyword">import</span> bbox_transform</span><br><span class="line"></span><br><span class="line">DEBUG = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AnchorTargetLayer</span><span class="params">(caffe.Layer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Assign anchors to ground-truth targets. Produces anchor classification</span></span><br><span class="line"><span class="string">    labels and bounding-box regression targets.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setup</span><span class="params">(self, bottom, top)</span>:</span></span><br><span class="line">        layer_params = yaml.load(self.param_str_)</span><br><span class="line">        anchor_scales = layer_params.get(<span class="string">'scales'</span>, (<span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">        self._anchors = generate_anchors(scales=np.array(anchor_scales))<span class="comment">#九个anchor的w h x_cstr y_cstr，对原始的wh做横向纵向变化，并放大缩小得到九个</span></span><br><span class="line">        self._num_anchors = self._anchors.shape[<span class="number">0</span>]&lt;span style=<span class="string">"font-family: Arial, Helvetica, sans-serif;"</span>&gt;<span class="comment">#anchor的个数&lt;/span&gt;</span></span><br><span class="line">        self._feat_stride = layer_params[<span class="string">'feat_stride'</span>]<span class="comment">#网络中参数16</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> DEBUG:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'anchors:'</span></span><br><span class="line">            <span class="keyword">print</span> self._anchors</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'anchor shapes:'</span></span><br><span class="line">            <span class="keyword">print</span> np.hstack((</span><br><span class="line">                self._anchors[:, <span class="number">2</span>::<span class="number">4</span>] - self._anchors[:, <span class="number">0</span>::<span class="number">4</span>],</span><br><span class="line">                self._anchors[:, <span class="number">3</span>::<span class="number">4</span>] - self._anchors[:, <span class="number">1</span>::<span class="number">4</span>],</span><br><span class="line">            ))</span><br><span class="line">            self._counts = cfg.EPS</span><br><span class="line">            self._sums = np.zeros((<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">            self._squared_sums = np.zeros((<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">            self._fg_sum = <span class="number">0</span></span><br><span class="line">            self._bg_sum = <span class="number">0</span></span><br><span class="line">            self._count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># allow boxes to sit over the edge by a small amount</span></span><br><span class="line">        self._allowed_border = layer_params.get(<span class="string">'allowed_border'</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="comment">#bottom 长度为4；bottom[0],map；bottom[1],boxes,labels;bottom[2],im_fo;bottom[3],图片数据</span></span><br><span class="line">        height, width = bottom[<span class="number">0</span>].data.shape[<span class="number">-2</span>:]</span><br><span class="line">        <span class="keyword">if</span> DEBUG:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'AnchorTargetLayer: height'</span>, height, <span class="string">'width'</span>, width</span><br><span class="line"></span><br><span class="line">        A = self._num_anchors<span class="comment">#anchor的个数</span></span><br><span class="line">        <span class="comment"># labels</span></span><br><span class="line">        top[<span class="number">0</span>].reshape(<span class="number">1</span>, <span class="number">1</span>, A * height, width)</span><br><span class="line">        <span class="comment"># bbox_targets</span></span><br><span class="line">        top[<span class="number">1</span>].reshape(<span class="number">1</span>, A * <span class="number">4</span>, height, width)</span><br><span class="line">        <span class="comment"># bbox_inside_weights</span></span><br><span class="line">        top[<span class="number">2</span>].reshape(<span class="number">1</span>, A * <span class="number">4</span>, height, width)</span><br><span class="line">        <span class="comment"># bbox_outside_weights</span></span><br><span class="line">        top[<span class="number">3</span>].reshape(<span class="number">1</span>, A * <span class="number">4</span>, height, width)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, bottom, top)</span>:</span></span><br><span class="line">        <span class="comment"># Algorithm:</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># for each (H, W) location i</span></span><br><span class="line">        <span class="comment">#   generate 9 anchor boxes centered on cell i</span></span><br><span class="line">        <span class="comment">#   apply predicted bbox deltas at cell i to each of the 9 anchors</span></span><br><span class="line">        <span class="comment"># filter out-of-image anchors</span></span><br><span class="line">        <span class="comment"># measure GT overlap</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> bottom[<span class="number">0</span>].data.shape[<span class="number">0</span>] == <span class="number">1</span>, \</span><br><span class="line">            <span class="string">'Only single item batches are supported'</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># map of shape (..., H, W)</span></span><br><span class="line">        height, width = bottom[<span class="number">0</span>].data.shape[<span class="number">-2</span>:]</span><br><span class="line">        <span class="comment"># GT boxes (x1, y1, x2, y2, label)</span></span><br><span class="line">        gt_boxes = bottom[<span class="number">1</span>].data<span class="comment">#gt_boxes:长度不定</span></span><br><span class="line">        <span class="comment"># im_info</span></span><br><span class="line">        im_info = bottom[<span class="number">2</span>].data[<span class="number">0</span>, :]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> DEBUG:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">''</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">'im_size: (&#123;&#125;, &#123;&#125;)'</span>.format(im_info[<span class="number">0</span>], im_info[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'scale: &#123;&#125;'</span>.format(im_info[<span class="number">2</span>])</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'height, width: (&#123;&#125;, &#123;&#125;)'</span>.format(height, width)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'rpn: gt_boxes.shape'</span>, gt_boxes.shape</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'rpn: gt_boxes'</span>, gt_boxes</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. Generate proposals from bbox deltas and shifted anchors</span></span><br><span class="line">        shift_x = np.arange(<span class="number">0</span>, width) * self._feat_stride</span><br><span class="line">        shift_y = np.arange(<span class="number">0</span>, height) * self._feat_stride</span><br><span class="line">        shift_x, shift_y = np.meshgrid(shift_x, shift_y)</span><br><span class="line">        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),</span><br><span class="line">                            shift_x.ravel(), shift_y.ravel())).transpose()</span><br><span class="line">        <span class="comment"># add A anchors (1, A, 4) to</span></span><br><span class="line">        <span class="comment"># cell K shifts (K, 1, 4) to get</span></span><br><span class="line">        <span class="comment"># shift anchors (K, A, 4)</span></span><br><span class="line">        <span class="comment"># reshape to (K*A, 4) shifted anchors</span></span><br><span class="line">        A = self._num_anchors</span><br><span class="line">        K = shifts.shape[<span class="number">0</span>]</span><br><span class="line">        all_anchors = (self._anchors.reshape((<span class="number">1</span>, A, <span class="number">4</span>)) +</span><br><span class="line">                       shifts.reshape((<span class="number">1</span>, K, <span class="number">4</span>)).transpose((<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)))</span><br><span class="line">        all_anchors = all_anchors.reshape((K * A, <span class="number">4</span>))</span><br><span class="line">        total_anchors = int(K * A)<span class="comment">#K*A，所有anchors个数，包括越界的</span></span><br><span class="line">        <span class="comment">#K: width*height</span></span><br><span class="line">        <span class="comment">#A: 9</span></span><br><span class="line">        <span class="comment"># only keep anchors inside the image</span></span><br><span class="line">        inds_inside = np.where(</span><br><span class="line">            (all_anchors[:, <span class="number">0</span>] &gt;= -self._allowed_border) &amp;</span><br><span class="line">            (all_anchors[:, <span class="number">1</span>] &gt;= -self._allowed_border) &amp;</span><br><span class="line">            (all_anchors[:, <span class="number">2</span>] &lt; im_info[<span class="number">1</span>] + self._allowed_border) &amp;  <span class="comment"># width</span></span><br><span class="line">            (all_anchors[:, <span class="number">3</span>] &lt; im_info[<span class="number">0</span>] + self._allowed_border)    <span class="comment"># height</span></span><br><span class="line">        )[<span class="number">0</span>]<span class="comment">#没有过界的anchors索引</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> DEBUG:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'total_anchors'</span>, total_anchors</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'inds_inside'</span>, len(inds_inside)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># keep only inside anchors</span></span><br><span class="line">        anchors = all_anchors[inds_inside, :]<span class="comment">#没有过界的anchors</span></span><br><span class="line">        <span class="keyword">if</span> DEBUG:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'anchors.shape'</span>, anchors.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># label: 1 is positive, 0 is negative, -1 is dont care</span></span><br><span class="line">        labels = np.empty((len(inds_inside), ), dtype=np.float32)</span><br><span class="line">        labels.fill(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># overlaps between the anchors and the gt boxes</span></span><br><span class="line">        <span class="comment"># overlaps (ex, gt)</span></span><br><span class="line">        overlaps = bbox_overlaps(</span><br><span class="line">            np.ascontiguousarray(anchors, dtype=np.float),</span><br><span class="line">            np.ascontiguousarray(gt_boxes, dtype=np.float))</span><br><span class="line">        argmax_overlaps = overlaps.argmax(axis=<span class="number">1</span>)<span class="comment">#overlaps每行最大值索引</span></span><br><span class="line">        max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]</span><br><span class="line">        gt_argmax_overlaps = overlaps.argmax(axis=<span class="number">0</span>)</span><br><span class="line">        gt_max_overlaps = overlaps[gt_argmax_overlaps,</span><br><span class="line">                                   np.arange(overlaps.shape[<span class="number">1</span>])]</span><br><span class="line">        gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> cfg.TRAIN.RPN_CLOBBER_POSITIVES:</span><br><span class="line">            <span class="comment"># assign bg labels first so that positive labels can clobber them</span></span><br><span class="line">            labels[max_overlaps &lt; cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># fg label: for each gt, anchor with highest overlap</span></span><br><span class="line">        labels[gt_argmax_overlaps] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># fg label: above threshold IOU</span></span><br><span class="line">        labels[max_overlaps &gt;= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cfg.TRAIN.RPN_CLOBBER_POSITIVES:</span><br><span class="line">            <span class="comment"># assign bg labels last so that negative labels can clobber positives</span></span><br><span class="line">            labels[max_overlaps &lt; cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># subsample positive labels if we have too many</span></span><br><span class="line">        num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCHSIZE)</span><br><span class="line">        fg_inds = np.where(labels == <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> len(fg_inds) &gt; num_fg:</span><br><span class="line">            disable_inds = npr.choice(</span><br><span class="line">                fg_inds, size=(len(fg_inds) - num_fg), replace=<span class="literal">False</span>)</span><br><span class="line">            labels[disable_inds] = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># subsample negative labels if we have too many</span></span><br><span class="line">        num_bg = cfg.TRAIN.RPN_BATCHSIZE - np.sum(labels == <span class="number">1</span>)</span><br><span class="line">        bg_inds = np.where(labels == <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> len(bg_inds) &gt; num_bg:</span><br><span class="line">            disable_inds = npr.choice(</span><br><span class="line">                bg_inds, size=(len(bg_inds) - num_bg), replace=<span class="literal">False</span>)</span><br><span class="line">            labels[disable_inds] = <span class="number">-1</span></span><br><span class="line">            <span class="comment">#print "was %s inds, disabling %s, now %s inds" % (</span></span><br><span class="line">                <span class="comment">#len(bg_inds), len(disable_inds), np.sum(labels == 0))</span></span><br><span class="line"></span><br><span class="line">        bbox_targets = np.zeros((len(inds_inside), <span class="number">4</span>), dtype=np.float32)</span><br><span class="line">        bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])</span><br><span class="line"></span><br><span class="line">        bbox_inside_weights = np.zeros((len(inds_inside), <span class="number">4</span>), dtype=np.float32)</span><br><span class="line">        bbox_inside_weights[labels == <span class="number">1</span>, :] = np.array(cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS)</span><br><span class="line"></span><br><span class="line">        bbox_outside_weights = np.zeros((len(inds_inside), <span class="number">4</span>), dtype=np.float32)</span><br><span class="line">        <span class="keyword">if</span> cfg.TRAIN.RPN_POSITIVE_WEIGHT &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># uniform weighting of examples (given non-uniform sampling)</span></span><br><span class="line">            num_examples = np.sum(labels &gt;= <span class="number">0</span>)</span><br><span class="line">            positive_weights = np.ones((<span class="number">1</span>, <span class="number">4</span>)) * <span class="number">1.0</span> / num_examples</span><br><span class="line">            negative_weights = np.ones((<span class="number">1</span>, <span class="number">4</span>)) * <span class="number">1.0</span> / num_examples</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> ((cfg.TRAIN.RPN_POSITIVE_WEIGHT &gt; <span class="number">0</span>) &amp;</span><br><span class="line">                    (cfg.TRAIN.RPN_POSITIVE_WEIGHT &lt; <span class="number">1</span>))</span><br><span class="line">            positive_weights = (cfg.TRAIN.RPN_POSITIVE_WEIGHT /</span><br><span class="line">                                np.sum(labels == <span class="number">1</span>))</span><br><span class="line">            negative_weights = ((<span class="number">1.0</span> - cfg.TRAIN.RPN_POSITIVE_WEIGHT) /</span><br><span class="line">                                np.sum(labels == <span class="number">0</span>))</span><br><span class="line">        bbox_outside_weights[labels == <span class="number">1</span>, :] = positive_weights</span><br><span class="line">        bbox_outside_weights[labels == <span class="number">0</span>, :] = negative_weights</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> DEBUG:</span><br><span class="line">            self._sums += bbox_targets[labels == <span class="number">1</span>, :].sum(axis=<span class="number">0</span>)</span><br><span class="line">            self._squared_sums += (bbox_targets[labels == <span class="number">1</span>, :] ** <span class="number">2</span>).sum(axis=<span class="number">0</span>)</span><br><span class="line">            self._counts += np.sum(labels == <span class="number">1</span>)</span><br><span class="line">            means = self._sums / self._counts</span><br><span class="line">            stds = np.sqrt(self._squared_sums / self._counts - means ** <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'means:'</span></span><br><span class="line">            <span class="keyword">print</span> means</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'stdevs:'</span></span><br><span class="line">            <span class="keyword">print</span> stds</span><br><span class="line"></span><br><span class="line">        <span class="comment"># map up to original set of anchors</span></span><br><span class="line">        labels = _unmap(labels, total_anchors, inds_inside, fill=<span class="number">-1</span>)</span><br><span class="line">        bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=<span class="number">0</span>)</span><br><span class="line">        bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=<span class="number">0</span>)</span><br><span class="line">        bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, fill=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> DEBUG:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'rpn: max max_overlap'</span>, np.max(max_overlaps)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'rpn: num_positive'</span>, np.sum(labels == <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'rpn: num_negative'</span>, np.sum(labels == <span class="number">0</span>)</span><br><span class="line">            self._fg_sum += np.sum(labels == <span class="number">1</span>)</span><br><span class="line">            self._bg_sum += np.sum(labels == <span class="number">0</span>)</span><br><span class="line">            self._count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">print</span> <span class="string">'rpn: num_positive avg'</span>, self._fg_sum / self._count</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'rpn: num_negative avg'</span>, self._bg_sum / self._count</span><br><span class="line"></span><br><span class="line">        <span class="comment"># labels</span></span><br><span class="line">        labels = labels.reshape((<span class="number">1</span>, height, width, A)).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        labels = labels.reshape((<span class="number">1</span>, <span class="number">1</span>, A * height, width))</span><br><span class="line">        top[<span class="number">0</span>].reshape(*labels.shape)</span><br><span class="line">        top[<span class="number">0</span>].data[...] = labels</span><br><span class="line"></span><br><span class="line">        <span class="comment"># bbox_targets</span></span><br><span class="line">        bbox_targets = bbox_targets \</span><br><span class="line">            .reshape((<span class="number">1</span>, height, width, A * <span class="number">4</span>)).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        top[<span class="number">1</span>].reshape(*bbox_targets.shape)</span><br><span class="line">        top[<span class="number">1</span>].data[...] = bbox_targets</span><br><span class="line"></span><br><span class="line">        <span class="comment"># bbox_inside_weights</span></span><br><span class="line">        bbox_inside_weights = bbox_inside_weights \</span><br><span class="line">            .reshape((<span class="number">1</span>, height, width, A * <span class="number">4</span>)).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">assert</span> bbox_inside_weights.shape[<span class="number">2</span>] == height</span><br><span class="line">        <span class="keyword">assert</span> bbox_inside_weights.shape[<span class="number">3</span>] == width</span><br><span class="line">        top[<span class="number">2</span>].reshape(*bbox_inside_weights.shape)</span><br><span class="line">        top[<span class="number">2</span>].data[...] = bbox_inside_weights</span><br><span class="line"></span><br><span class="line">        <span class="comment"># bbox_outside_weights</span></span><br><span class="line">        bbox_outside_weights = bbox_outside_weights \</span><br><span class="line">            .reshape((<span class="number">1</span>, height, width, A * <span class="number">4</span>)).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">assert</span> bbox_outside_weights.shape[<span class="number">2</span>] == height</span><br><span class="line">        <span class="keyword">assert</span> bbox_outside_weights.shape[<span class="number">3</span>] == width</span><br><span class="line">        top[<span class="number">3</span>].reshape(*bbox_outside_weights.shape)</span><br><span class="line">        top[<span class="number">3</span>].data[...] = bbox_outside_weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, top, propagate_down, bottom)</span>:</span></span><br><span class="line">        <span class="string">"""This layer does not propagate gradients."""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reshape</span><span class="params">(self, bottom, top)</span>:</span></span><br><span class="line">        <span class="string">"""Reshaping happens during the call to forward."""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_unmap</span><span class="params">(data, count, inds, fill=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Unmap a subset of item (data) back to the original set of items (of</span></span><br><span class="line"><span class="string">    size count) """</span></span><br><span class="line">    <span class="keyword">if</span> len(data.shape) == <span class="number">1</span>:</span><br><span class="line">        ret = np.empty((count, ), dtype=np.float32)</span><br><span class="line">        ret.fill(fill)</span><br><span class="line">        ret[inds] = data</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ret = np.empty((count, ) + data.shape[<span class="number">1</span>:], dtype=np.float32)</span><br><span class="line">        ret.fill(fill)</span><br><span class="line">        ret[inds, :] = data</span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_compute_targets</span><span class="params">(ex_rois, gt_rois)</span>:</span></span><br><span class="line">    <span class="string">"""Compute bounding-box regression targets for an image."""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> ex_rois.shape[<span class="number">0</span>] == gt_rois.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">assert</span> ex_rois.shape[<span class="number">1</span>] == <span class="number">4</span></span><br><span class="line">    <span class="keyword">assert</span> gt_rois.shape[<span class="number">1</span>] == <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bbox_transform(ex_rois, gt_rois[:, :<span class="number">4</span>]).astype(np.float32, copy=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h3><p>rpn-data是AnchorTargetLayer<br>bottom长度为4；bottom[0],map；bottom[1],boxes,labels;bottom[2],im_fo;bottom[3],图片数据<br>self._feat_stride:网络中参数16<br>self._anchors：九个anchor的w h x_cstr y_cstr，对原始的wh做横向纵向变化，并放大缩小的到九个<br>self._num_anchors：anchor的个数</p>
<p>inds_inside：没有过界的anchors索引<br>anchors：没有过界的anchors<br>argmax_overlaps：overlaps每行最大值索引<br>total_anchors： K × A，所有anchors个数，包括越界的<br>K: width × height<br>A: 9  </p>
<p>gt_boxes:长度不定  </p>
<p>bbox_overlaps： 返回overlaps: (len(inds_inside)* len(gt_boxes))  </p>
<p>论文笔记<br>我们分配正标签给两类anchor：<br>（i）与某个ground truth（GT）包围盒有最高的IoU（Intersection-over-Union，交集并集之比）重叠的anchor（也许不到0.7）<br>（ii）与任意GT包围盒有大于0.7的IoU交叠的anchor  </p>
<p>labels:0,bg; 1,fg; -1, on care,(len(inds_inside)); over_laps列最大值对应行坐标=1；over_laps行最大值 &gt; 0.7,行=1； over_laps行最大值 &lt; 0.3，行=0<br>正样本数量由他们控制：cfg.TRAIN.RPN_FG_FRACTION × cfg.TRAIN.RPN_BATCHSIZE（128），小于等于<br>负样本数量。。。。。：cfg.TRAIN.RPN_BATCHSIZE - np.sum(labels == 1)<br>cfg.TRAIN.RPN_BATCHSIZE:  256，最终输出proposal数量控制<br><strong>多的proposal被随机搞成-1了。。。。。。随机</strong>  </p>
<p>bbox_inside_weights: label等于1的行，它的值等于cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS（1.0）；其他等于0;(len(inds_inside), 4)；相当于损失函数中的pi*<br>cfg.TRAIN.RPN_POSITIVE_WEIGHT: -1.0<br>bbox_outside_weights：fg,bg=np.ones((1, 4)) × 1.0 / sum(fg+bg)，其他为0;(len(inds_inside), 4)  </p>
<p>_unmap： 建立一个total_anchors × 第一个参数列的数组；全用fill填充；再把inds_inside对应的行用第一个参数对应的行填充</p>
]]></content>
      <categories>
        <category>faster rcnn</category>
        <category>faster cnn源码理解</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>faster rcnn源码理解</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn源码解读（三）train_faster_rcnn_alt_opt.py</title>
    <url>/2018/04/08/faster%20rcnn%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%B8%89%EF%BC%89train_faster_rcnn_alt_opt.py/</url>
    <content><![CDATA[<p>faster用python版本的  <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="noopener"> https://github.com/rbgirshick/py-faster-rcnn</a></p>
<p>train_faster_rcnn_alt_opt.py源码在 <a href="https://github.com/rbgirshick/py-faster-rcnn/blob/master/tools/train_faster_rcnn_alt_opt.py" target="_blank" rel="noopener"> https://github.com/rbgirshick/py-faster-rcnn/blob/master/tools/train_faster_rcnn_alt_opt.py
</a>  </p>
<p>faster rcnn训练的开始是：faster_rcnn_alt_opt.sh。下面命令是训练的，还有它的参数说明。</p>
<h3 id="调用最初脚本的说明"><a href="#调用最初脚本的说明" class="headerlink" title="调用最初脚本的说明"></a>调用最初脚本的说明</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">cd $FRCN_ROOT</span><br><span class="line"></span><br><span class="line"># .&#x2F;experiments&#x2F;scripts&#x2F;faster_rcnn_alt_opt.sh  GPU  NET  DATASET [options args to &#123;train,test&#125;_net.py]</span><br><span class="line"></span><br><span class="line"># GPU_ID is the GPU you want to train on</span><br><span class="line"># NET in &#123;ZF, VGG_CNN_M_1024, VGG16&#125; is the network arch to use</span><br><span class="line"># DATASET is only pascal_voc for now</span><br></pre></td></tr></table></figure>
<h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><p>train_faster_rcnn_alt_opt.py的源码：  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Faster R-CNN</span></span><br><span class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></span><br><span class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></span><br><span class="line"><span class="comment"># Written by Ross Girshick</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""Train a Faster R-CNN network using alternating optimization.</span></span><br><span class="line"><span class="string">This tool implements the alternating optimization algorithm described in our</span></span><br><span class="line"><span class="string">NIPS 2015 paper ("Faster R-CNN: Towards Real-time Object Detection with Region</span></span><br><span class="line"><span class="string">Proposal Networks." Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun.)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> _init_paths</span><br><span class="line"><span class="keyword">from</span> fast_rcnn.train <span class="keyword">import</span> get_training_roidb, train_net</span><br><span class="line"><span class="keyword">from</span> fast_rcnn.config <span class="keyword">import</span> cfg, cfg_from_file, cfg_from_list, get_output_dir</span><br><span class="line"><span class="keyword">from</span> datasets.factory <span class="keyword">import</span> get_imdb</span><br><span class="line"><span class="keyword">from</span> rpn.generate <span class="keyword">import</span> imdb_proposals</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> pprint</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> cPickle</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Parse input arguments</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">'Train a Faster R-CNN network'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--gpu'</span>, dest=<span class="string">'gpu_id'</span>,</span><br><span class="line">                        help=<span class="string">'GPU device id to use [0]'</span>,</span><br><span class="line">                        default=<span class="number">0</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--net_name'</span>, dest=<span class="string">'net_name'</span>,</span><br><span class="line">                        help=<span class="string">'network name (e.g., "ZF")'</span>,</span><br><span class="line">                        default=<span class="literal">None</span>, type=str)</span><br><span class="line">    parser.add_argument(<span class="string">'--weights'</span>, dest=<span class="string">'pretrained_model'</span>,</span><br><span class="line">                        help=<span class="string">'initialize with pretrained model weights'</span>,</span><br><span class="line">                        default=<span class="literal">None</span>, type=str)</span><br><span class="line">    parser.add_argument(<span class="string">'--cfg'</span>, dest=<span class="string">'cfg_file'</span>,</span><br><span class="line">                        help=<span class="string">'optional config file'</span>,</span><br><span class="line">                        default=<span class="literal">None</span>, type=str)</span><br><span class="line">    parser.add_argument(<span class="string">'--imdb'</span>, dest=<span class="string">'imdb_name'</span>,</span><br><span class="line">                        help=<span class="string">'dataset to train on'</span>,</span><br><span class="line">                        default=<span class="string">'voc_2007_trainval'</span>, type=str)</span><br><span class="line">    parser.add_argument(<span class="string">'--set'</span>, dest=<span class="string">'set_cfgs'</span>,</span><br><span class="line">                        help=<span class="string">'set config keys'</span>, default=<span class="literal">None</span>,</span><br><span class="line">                        nargs=argparse.REMAINDER)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) == <span class="number">1</span>:</span><br><span class="line">        parser.print_help()</span><br><span class="line">        sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="keyword">return</span> args</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_roidb</span><span class="params">(imdb_name, rpn_file=None)</span>:</span></span><br><span class="line">    imdb = get_imdb(imdb_name)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Loaded dataset `&#123;:s&#125;` for training'</span>.format(imdb.name)</span><br><span class="line">    imdb.set_proposal_method(cfg.TRAIN.PROPOSAL_METHOD)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Set proposal method: &#123;:s&#125;'</span>.format(cfg.TRAIN.PROPOSAL_METHOD)</span><br><span class="line">    <span class="keyword">if</span> rpn_file <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        imdb.config[<span class="string">'rpn_file'</span>] = rpn_file</span><br><span class="line">    roidb = get_training_roidb(imdb)</span><br><span class="line">    <span class="keyword">return</span> roidb, imdb</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_solvers</span><span class="params">(net_name)</span>:</span></span><br><span class="line">    <span class="comment"># Faster R-CNN Alternating Optimization</span></span><br><span class="line">    n = <span class="string">'faster_rcnn_alt_opt'</span></span><br><span class="line">    <span class="comment"># Solver for each training stage</span></span><br><span class="line">    solvers = [[net_name, n, <span class="string">'stage1_rpn_solver60k80k.pt'</span>],</span><br><span class="line">               [net_name, n, <span class="string">'stage1_fast_rcnn_solver30k40k.pt'</span>],</span><br><span class="line">               [net_name, n, <span class="string">'stage2_rpn_solver60k80k.pt'</span>],</span><br><span class="line">               [net_name, n, <span class="string">'stage2_fast_rcnn_solver30k40k.pt'</span>]]</span><br><span class="line">    solvers = [os.path.join(cfg.MODELS_DIR, *s) <span class="keyword">for</span> s <span class="keyword">in</span> solvers]</span><br><span class="line">    <span class="comment"># Iterations for each training stage</span></span><br><span class="line">    max_iters = [<span class="number">80000</span>, <span class="number">40000</span>, <span class="number">80000</span>, <span class="number">40000</span>]</span><br><span class="line">    <span class="comment"># max_iters = [100, 100, 100, 100]</span></span><br><span class="line">    <span class="comment"># Test prototxt for the RPN</span></span><br><span class="line">    rpn_test_prototxt = os.path.join(</span><br><span class="line">        cfg.MODELS_DIR, net_name, n, <span class="string">'rpn_test.pt'</span>)</span><br><span class="line">    <span class="keyword">return</span> solvers, max_iters, rpn_test_prototxt</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Pycaffe doesn't reliably free GPU memory when instantiated nets are discarded</span></span><br><span class="line"><span class="comment"># (e.g. "del net" in Python code). To work around this issue, each training</span></span><br><span class="line"><span class="comment"># stage is executed in a separate process using multiprocessing.Process.</span></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_init_caffe</span><span class="params">(cfg)</span>:</span></span><br><span class="line">    <span class="string">"""Initialize pycaffe in a training process.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> caffe</span><br><span class="line">    <span class="comment"># fix the random seeds (numpy and caffe) for reproducibility</span></span><br><span class="line">    np.random.seed(cfg.RNG_SEED)</span><br><span class="line">    caffe.set_random_seed(cfg.RNG_SEED)</span><br><span class="line">    <span class="comment"># set up caffe</span></span><br><span class="line">    caffe.set_mode_gpu()</span><br><span class="line">    caffe.set_device(cfg.GPU_ID)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_rpn</span><span class="params">(queue=None, imdb_name=None, init_model=None, solver=None,</span></span></span><br><span class="line"><span class="function"><span class="params">              max_iters=None, cfg=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train a Region Proposal Network in a separate training process.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Not using any proposals, just ground-truth boxes</span></span><br><span class="line">    cfg.TRAIN.HAS_RPN = <span class="literal">True</span></span><br><span class="line">    cfg.TRAIN.BBOX_REG = <span class="literal">False</span>  <span class="comment"># applies only to Fast R-CNN bbox regression</span></span><br><span class="line">    cfg.TRAIN.PROPOSAL_METHOD = <span class="string">'gt'</span></span><br><span class="line">    cfg.TRAIN.IMS_PER_BATCH = <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Init model: &#123;&#125;'</span>.format(init_model)</span><br><span class="line">    print(<span class="string">'Using config:'</span>)</span><br><span class="line">    pprint.pprint(cfg)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> caffe</span><br><span class="line">    _init_caffe(cfg)</span><br><span class="line"></span><br><span class="line">    roidb, imdb = get_roidb(imdb_name)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'roidb len: &#123;&#125;'</span>.format(len(roidb))</span><br><span class="line">    output_dir = get_output_dir(imdb)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Output will be saved to `&#123;:s&#125;`'</span>.format(output_dir)</span><br><span class="line"></span><br><span class="line">    model_paths = train_net(solver, roidb, output_dir,</span><br><span class="line">                            pretrained_model=init_model,</span><br><span class="line">                            max_iters=max_iters)</span><br><span class="line">    <span class="comment"># Cleanup all but the final model</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> model_paths[:<span class="number">-1</span>]:</span><br><span class="line">        os.remove(i)</span><br><span class="line">    rpn_model_path = model_paths[<span class="number">-1</span>]</span><br><span class="line">    <span class="comment"># Send final model path through the multiprocessing queue</span></span><br><span class="line">    queue.put(&#123;<span class="string">'model_path'</span>: rpn_model_path&#125;)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn_generate</span><span class="params">(queue=None, imdb_name=None, rpn_model_path=None, cfg=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 rpn_test_prototxt=None)</span>:</span></span><br><span class="line">    <span class="string">"""Use a trained RPN to generate proposals.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    cfg.TEST.RPN_PRE_NMS_TOP_N = <span class="number">-1</span>     <span class="comment"># no pre NMS filtering</span></span><br><span class="line">    cfg.TEST.RPN_POST_NMS_TOP_N = <span class="number">2000</span>  <span class="comment"># limit top boxes after NMS</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'RPN model: &#123;&#125;'</span>.format(rpn_model_path)</span><br><span class="line">    print(<span class="string">'Using config:'</span>)</span><br><span class="line">    pprint.pprint(cfg)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> caffe</span><br><span class="line">    _init_caffe(cfg)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> the matlab implementation computes proposals on flipped images, too.</span></span><br><span class="line">    <span class="comment"># We compute them on the image once and then flip the already computed</span></span><br><span class="line">    <span class="comment"># proposals. This might cause a minor loss in mAP (less proposal jittering).</span></span><br><span class="line">    imdb = get_imdb(imdb_name)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Loaded dataset `&#123;:s&#125;` for proposal generation'</span>.format(imdb.name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load RPN and configure output directory</span></span><br><span class="line">    rpn_net = caffe.Net(rpn_test_prototxt, rpn_model_path, caffe.TEST)</span><br><span class="line">    output_dir = get_output_dir(imdb)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Output will be saved to `&#123;:s&#125;`'</span>.format(output_dir)</span><br><span class="line">    <span class="comment"># Generate proposals on the imdb</span></span><br><span class="line">    rpn_proposals = imdb_proposals(rpn_net, imdb)</span><br><span class="line">    <span class="comment"># Write proposals to disk and send the proposal file path through the</span></span><br><span class="line">    <span class="comment"># multiprocessing queue</span></span><br><span class="line">    rpn_net_name = os.path.splitext(os.path.basename(rpn_model_path))[<span class="number">0</span>]</span><br><span class="line">    rpn_proposals_path = os.path.join(</span><br><span class="line">        output_dir, rpn_net_name + <span class="string">'_proposals.pkl'</span>)</span><br><span class="line">    <span class="keyword">with</span> open(rpn_proposals_path, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        cPickle.dump(rpn_proposals, f, cPickle.HIGHEST_PROTOCOL)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Wrote RPN proposals to &#123;&#125;'</span>.format(rpn_proposals_path)</span><br><span class="line">    queue.put(&#123;<span class="string">'proposal_path'</span>: rpn_proposals_path&#125;)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_fast_rcnn</span><span class="params">(queue=None, imdb_name=None, init_model=None, solver=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    max_iters=None, cfg=None, rpn_file=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train a Fast R-CNN using proposals generated by an RPN.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    cfg.TRAIN.HAS_RPN = <span class="literal">False</span>           <span class="comment"># not generating prosals on-the-fly</span></span><br><span class="line">    cfg.TRAIN.PROPOSAL_METHOD = <span class="string">'rpn'</span>   <span class="comment"># use pre-computed RPN proposals instead</span></span><br><span class="line">    cfg.TRAIN.IMS_PER_BATCH = <span class="number">2</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Init model: &#123;&#125;'</span>.format(init_model)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'RPN proposals: &#123;&#125;'</span>.format(rpn_file)</span><br><span class="line">    print(<span class="string">'Using config:'</span>)</span><br><span class="line">    pprint.pprint(cfg)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> caffe</span><br><span class="line">    _init_caffe(cfg)</span><br><span class="line"></span><br><span class="line">    roidb, imdb = get_roidb(imdb_name, rpn_file=rpn_file)</span><br><span class="line">    output_dir = get_output_dir(imdb)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Output will be saved to `&#123;:s&#125;`'</span>.format(output_dir)</span><br><span class="line">    <span class="comment"># Train Fast R-CNN</span></span><br><span class="line">    model_paths = train_net(solver, roidb, output_dir,</span><br><span class="line">                            pretrained_model=init_model,</span><br><span class="line">                            max_iters=max_iters)</span><br><span class="line">    <span class="comment"># Cleanup all but the final model</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> model_paths[:<span class="number">-1</span>]:</span><br><span class="line">        os.remove(i)</span><br><span class="line">    fast_rcnn_model_path = model_paths[<span class="number">-1</span>]</span><br><span class="line">    <span class="comment"># Send Fast R-CNN model path over the multiprocessing queue</span></span><br><span class="line">    queue.put(&#123;<span class="string">'model_path'</span>: fast_rcnn_model_path&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    args = parse_args()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Called with args:'</span>)</span><br><span class="line">    print(args)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.cfg_file <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        cfg_from_file(args.cfg_file)</span><br><span class="line">    <span class="keyword">if</span> args.set_cfgs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        cfg_from_list(args.set_cfgs)</span><br><span class="line">    cfg.GPU_ID = args.gpu_id</span><br><span class="line"></span><br><span class="line">    <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line">    <span class="comment"># Pycaffe doesn't reliably free GPU memory when instantiated nets are</span></span><br><span class="line">    <span class="comment"># discarded (e.g. "del net" in Python code). To work around this issue, each</span></span><br><span class="line">    <span class="comment"># training stage is executed in a separate process using</span></span><br><span class="line">    <span class="comment"># multiprocessing.Process.</span></span><br><span class="line">    <span class="comment"># --------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># queue for communicated results between processes</span></span><br><span class="line">    mp_queue = mp.Queue()</span><br><span class="line">    <span class="comment"># solves, iters, etc. for each training stage</span></span><br><span class="line">    solvers, max_iters, rpn_test_prototxt = get_solvers(args.net_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Stage 1 RPN, init from ImageNet model'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line"></span><br><span class="line">    cfg.TRAIN.SNAPSHOT_INFIX = <span class="string">'stage1'</span></span><br><span class="line">    mp_kwargs = dict(</span><br><span class="line">            queue=mp_queue,</span><br><span class="line">            imdb_name=args.imdb_name,</span><br><span class="line">            init_model=args.pretrained_model,</span><br><span class="line">            solver=solvers[<span class="number">0</span>],</span><br><span class="line">            max_iters=max_iters[<span class="number">0</span>],</span><br><span class="line">            cfg=cfg)</span><br><span class="line">    p = mp.Process(target=train_rpn, kwargs=mp_kwargs)</span><br><span class="line">    p.start()</span><br><span class="line">    rpn_stage1_out = mp_queue.get()</span><br><span class="line">    p.join()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Stage 1 RPN, generate proposals'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line"></span><br><span class="line">    mp_kwargs = dict(</span><br><span class="line">            queue=mp_queue,</span><br><span class="line">            imdb_name=args.imdb_name,</span><br><span class="line">            rpn_model_path=str(rpn_stage1_out[<span class="string">'model_path'</span>]),</span><br><span class="line">            cfg=cfg,</span><br><span class="line">            rpn_test_prototxt=rpn_test_prototxt)</span><br><span class="line">    p = mp.Process(target=rpn_generate, kwargs=mp_kwargs)</span><br><span class="line">    p.start()</span><br><span class="line">    rpn_stage1_out[<span class="string">'proposal_path'</span>] = mp_queue.get()[<span class="string">'proposal_path'</span>]</span><br><span class="line">    p.join()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Stage 1 Fast R-CNN using RPN proposals, init from ImageNet model'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line"></span><br><span class="line">    cfg.TRAIN.SNAPSHOT_INFIX = <span class="string">'stage1'</span></span><br><span class="line">    mp_kwargs = dict(</span><br><span class="line">            queue=mp_queue,</span><br><span class="line">            imdb_name=args.imdb_name,</span><br><span class="line">            init_model=args.pretrained_model,</span><br><span class="line">            solver=solvers[<span class="number">1</span>],</span><br><span class="line">            max_iters=max_iters[<span class="number">1</span>],</span><br><span class="line">            cfg=cfg,</span><br><span class="line">            rpn_file=rpn_stage1_out[<span class="string">'proposal_path'</span>])</span><br><span class="line">    p = mp.Process(target=train_fast_rcnn, kwargs=mp_kwargs)</span><br><span class="line">    p.start()</span><br><span class="line">    fast_rcnn_stage1_out = mp_queue.get()</span><br><span class="line">    p.join()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Stage 2 RPN, init from stage 1 Fast R-CNN model'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line"></span><br><span class="line">    cfg.TRAIN.SNAPSHOT_INFIX = <span class="string">'stage2'</span></span><br><span class="line">    mp_kwargs = dict(</span><br><span class="line">            queue=mp_queue,</span><br><span class="line">            imdb_name=args.imdb_name,</span><br><span class="line">            init_model=str(fast_rcnn_stage1_out[<span class="string">'model_path'</span>]),</span><br><span class="line">            solver=solvers[<span class="number">2</span>],</span><br><span class="line">            max_iters=max_iters[<span class="number">2</span>],</span><br><span class="line">            cfg=cfg)</span><br><span class="line">    p = mp.Process(target=train_rpn, kwargs=mp_kwargs)</span><br><span class="line">    p.start()</span><br><span class="line">    rpn_stage2_out = mp_queue.get()</span><br><span class="line">    p.join()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Stage 2 RPN, generate proposals'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line"></span><br><span class="line">    mp_kwargs = dict(</span><br><span class="line">            queue=mp_queue,</span><br><span class="line">            imdb_name=args.imdb_name,</span><br><span class="line">            rpn_model_path=str(rpn_stage2_out[<span class="string">'model_path'</span>]),</span><br><span class="line">            cfg=cfg,</span><br><span class="line">            rpn_test_prototxt=rpn_test_prototxt)</span><br><span class="line">    p = mp.Process(target=rpn_generate, kwargs=mp_kwargs)</span><br><span class="line">    p.start()</span><br><span class="line">    rpn_stage2_out[<span class="string">'proposal_path'</span>] = mp_queue.get()[<span class="string">'proposal_path'</span>]</span><br><span class="line">    p.join()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Stage 2 Fast R-CNN, init from stage 2 RPN R-CNN model'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></span><br><span class="line"></span><br><span class="line">    cfg.TRAIN.SNAPSHOT_INFIX = <span class="string">'stage2'</span></span><br><span class="line">    mp_kwargs = dict(</span><br><span class="line">            queue=mp_queue,</span><br><span class="line">            imdb_name=args.imdb_name,</span><br><span class="line">            init_model=str(rpn_stage2_out[<span class="string">'model_path'</span>]),</span><br><span class="line">            solver=solvers[<span class="number">3</span>],</span><br><span class="line">            max_iters=max_iters[<span class="number">3</span>],</span><br><span class="line">            cfg=cfg,</span><br><span class="line">            rpn_file=rpn_stage2_out[<span class="string">'proposal_path'</span>])</span><br><span class="line">    p = mp.Process(target=train_fast_rcnn, kwargs=mp_kwargs)</span><br><span class="line">    p.start()</span><br><span class="line">    fast_rcnn_stage2_out = mp_queue.get()</span><br><span class="line">    p.join()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create final model (just a copy of the last stage)</span></span><br><span class="line">    final_path = os.path.join(</span><br><span class="line">            os.path.dirname(fast_rcnn_stage2_out[<span class="string">'model_path'</span>]),</span><br><span class="line">            args.net_name + <span class="string">'_faster_rcnn_final.caffemodel'</span>)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'cp &#123;&#125; -&gt; &#123;&#125;'</span>.format(</span><br><span class="line">            fast_rcnn_stage2_out[<span class="string">'model_path'</span>], final_path)</span><br><span class="line">    shutil.copy(fast_rcnn_stage2_out[<span class="string">'model_path'</span>], final_path)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Final model: &#123;&#125;'</span>.format(final_path)</span><br></pre></td></tr></table></figure>
<h3 id="部分参数说明"><a href="#部分参数说明" class="headerlink" title="部分参数说明"></a>部分参数说明</h3><p>train_faster_rcnn_alt_opt.py  的部分参数说明:</p>
<ul>
<li>net_name:      {ZF, VGG_CNN_M_1024, VGG16}</li>
<li>pretrained_model:      data/imagenet_models/${net_name}.v2.caffemodel</li>
<li>cfg_file:     experiments/cfgs/faster_rcnn_alt_opt.yml</li>
<li>imdb_name:     “voc_2007_trainval” or “voc_2007_test”</li>
<li>cfg.TRAIN.HAS_RPN = True  表示用  xml  提供的  propoal</li>
<li>cfg是配置文件，它的默认值放在上面的cfg_file里，其他还可以自己写配置文件之后与默认配置文件融合。</li>
</ul>
<h4 id="net-name"><a href="#net-name" class="headerlink" title="net_name"></a>net_name</h4><p>net_name是用get_solvers()找到网络。还要用到cfg的参数  MODELS_DIR  ，<br>例子是join(MODELS_DIR， net_name， ‘faster_rcnn_alt_opt’,’stage1_rpn_solver60k80k.pt’)</p>
<h4 id="imdb-name"><a href="#imdb-name" class="headerlink" title="imdb_name"></a>imdb_name</h4><p>imdb_name在factory中被拆成‘2007’（year）和‘trainval’/‘test’（split）到类pascal_voc中产生相应的imdb</p>
<h4 id="整个step的大致流程"><a href="#整个step的大致流程" class="headerlink" title="整个step的大致流程"></a>整个step的大致流程</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(ImageNet model)-&gt;stage1_rpn_train-&gt;rpn_test</span><br><span class="line">                                      |(proposal_path)</span><br><span class="line">                      (ImageNetmodel)-&gt;stage1_fast_rcnn_train-&gt; stage2_rpn_train-&gt; rpn_test-&gt;stage2_fast_rcnn_train</span><br></pre></td></tr></table></figure>
<h4 id="数据imdb和roidb"><a href="#数据imdb和roidb" class="headerlink" title="数据imdb和roidb"></a>数据imdb和roidb</h4><p>roidb原本是imdb的一个属性，但imdb其实是为了计算roidb存在的，他所有的其他属性和方法都是为了计算roidb</p>
]]></content>
      <categories>
        <category>faster rcnn</category>
        <category>faster cnn源码理解</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>faster rcnn源码理解</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn源码解读（五）之layer（网络里的input-data）</title>
    <url>/2018/04/08/faster%20rcnn%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%BA%94%EF%BC%89%E4%B9%8Blayer%EF%BC%88%E7%BD%91%E7%BB%9C%E9%87%8C%E7%9A%84input-data%EF%BC%89/</url>
    <content><![CDATA[<p>faster rcnn用python版本的  <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="noopener"> https://github.com/rbgirshick/py-faster-rcnn</a></p>
<p>layer源码地址： <a href="https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/roi_data_layer/layer.py" target="_blank" rel="noopener"> https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/roi_data_layer/layer.py </a></p>
<h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Fast R-CNN</span></span><br><span class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></span><br><span class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></span><br><span class="line"><span class="comment"># Written by Ross Girshick</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""The data layer used during training to train a Fast R-CNN network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">RoIDataLayer implements a Caffe Python layer.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> caffe</span><br><span class="line"><span class="keyword">from</span> fast_rcnn.config <span class="keyword">import</span> cfg</span><br><span class="line"><span class="keyword">from</span> roi_data_layer.minibatch <span class="keyword">import</span> get_minibatch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Queue</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RoIDataLayer</span><span class="params">(caffe.Layer)</span>:</span></span><br><span class="line">    <span class="string">"""Fast R-CNN data layer used for training."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_shuffle_roidb_inds</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Randomly permute the training roidb."""</span></span><br><span class="line">        <span class="keyword">if</span> cfg.TRAIN.ASPECT_GROUPING:</span><br><span class="line">            widths = np.array([r[<span class="string">'width'</span>] <span class="keyword">for</span> r <span class="keyword">in</span> self._roidb])</span><br><span class="line">            heights = np.array([r[<span class="string">'height'</span>] <span class="keyword">for</span> r <span class="keyword">in</span> self._roidb])</span><br><span class="line">            horz = (widths &gt;= heights)</span><br><span class="line">            vert = np.logical_not(horz)</span><br><span class="line">            horz_inds = np.where(horz)[<span class="number">0</span>]</span><br><span class="line">            vert_inds = np.where(vert)[<span class="number">0</span>]</span><br><span class="line">            inds = np.hstack((</span><br><span class="line">                np.random.permutation(horz_inds),</span><br><span class="line">                np.random.permutation(vert_inds)))</span><br><span class="line">            inds = np.reshape(inds, (<span class="number">-1</span>, <span class="number">2</span>))</span><br><span class="line">            row_perm = np.random.permutation(np.arange(inds.shape[<span class="number">0</span>]))</span><br><span class="line">            inds = np.reshape(inds[row_perm, :], (<span class="number">-1</span>,))</span><br><span class="line">            self._perm = inds<span class="comment">#把roidb的索引打乱，造成的shuffle，打乱的索引存储的地方</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._perm = np.random.permutation(np.arange(len(self._roidb)))</span><br><span class="line">        self._cur = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_next_minibatch_inds</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the roidb indices for the next minibatch."""</span></span><br><span class="line">        <span class="keyword">if</span> self._cur + cfg.TRAIN.IMS_PER_BATCH &gt;= len(self._roidb):</span><br><span class="line">            self._shuffle_roidb_inds()</span><br><span class="line"></span><br><span class="line">        db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]</span><br><span class="line">        self._cur += cfg.TRAIN.IMS_PER_BATCH<span class="comment">#相当于一个指向_perm的指针，每次取走图片后，他会跟着变化#cfg.TRAIN.IMS_PER_BATCH： （猜测，每次取图片的数量）</span></span><br><span class="line">        <span class="keyword">return</span> db_inds<span class="comment">#本次取得图片的索引</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_next_minibatch</span><span class="params">(self)</span>:</span><span class="comment">#取得本次图片的索引，即db_inds</span></span><br><span class="line">        <span class="string">"""Return the blobs to be used for the next minibatch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If cfg.TRAIN.USE_PREFETCH is True, then blobs will be computed in a</span></span><br><span class="line"><span class="string">        separate process and made available through self._blob_queue.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> cfg.TRAIN.USE_PREFETCH:</span><br><span class="line">            <span class="keyword">return</span> self._blob_queue.get()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            db_inds = self._get_next_minibatch_inds()</span><br><span class="line">            minibatch_db = [self._roidb[i] <span class="keyword">for</span> i <span class="keyword">in</span> db_inds]<span class="comment">#本次的roidb</span></span><br><span class="line">            <span class="keyword">return</span> get_minibatch(minibatch_db, self._num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_roidb</span><span class="params">(self, roidb)</span>:</span></span><br><span class="line">        <span class="string">"""Set the roidb to be used by this layer during training."""</span></span><br><span class="line">        self._roidb = roidb</span><br><span class="line">        self._shuffle_roidb_inds()</span><br><span class="line">        <span class="keyword">if</span> cfg.TRAIN.USE_PREFETCH:</span><br><span class="line">            self._blob_queue = Queue(<span class="number">10</span>)</span><br><span class="line">            self._prefetch_process = BlobFetcher(self._blob_queue,</span><br><span class="line">                                                 self._roidb,</span><br><span class="line">                                                 self._num_classes)</span><br><span class="line">            self._prefetch_process.start()</span><br><span class="line">            <span class="comment"># Terminate the child process when the parent exists</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">cleanup</span><span class="params">()</span>:</span></span><br><span class="line">                <span class="keyword">print</span> <span class="string">'Terminating BlobFetcher'</span></span><br><span class="line">                self._prefetch_process.terminate()</span><br><span class="line">                self._prefetch_process.join()</span><br><span class="line">            <span class="keyword">import</span> atexit</span><br><span class="line">            atexit.register(cleanup)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setup</span><span class="params">(self, bottom, top)</span>:</span></span><br><span class="line">        <span class="string">"""Setup the RoIDataLayer."""</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># parse the layer parameter string, which must be valid YAML</span></span><br><span class="line">        layer_params = yaml.load(self.param_str_)</span><br><span class="line"></span><br><span class="line">        self._num_classes = layer_params[<span class="string">'num_classes'</span>]<span class="comment">#网络里的类别数值21</span></span><br><span class="line"></span><br><span class="line">        self._name_to_top_map = &#123;&#125;<span class="comment">#&#123;'gt_boxes': 2, 'data': 0, 'im_info': 1&#125;字典的value值是top的对应索引</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># data blob: holds a batch of N images, each with 3 channels</span></span><br><span class="line">        idx = <span class="number">0</span></span><br><span class="line">        top[idx].reshape(cfg.TRAIN.IMS_PER_BATCH, <span class="number">3</span>,</span><br><span class="line">            max(cfg.TRAIN.SCALES), cfg.TRAIN.MAX_SIZE)</span><br><span class="line">        self._name_to_top_map[<span class="string">'data'</span>] = idx</span><br><span class="line">        idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cfg.TRAIN.HAS_RPN:</span><br><span class="line">            top[idx].reshape(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">            self._name_to_top_map[<span class="string">'im_info'</span>] = idx</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            top[idx].reshape(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">            self._name_to_top_map[<span class="string">'gt_boxes'</span>] = idx</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># not using RPN</span></span><br><span class="line">            <span class="comment"># rois blob: holds R regions of interest, each is a 5-tuple</span></span><br><span class="line">            <span class="comment"># (n, x1, y1, x2, y2) specifying an image batch index n and a</span></span><br><span class="line">            <span class="comment"># rectangle (x1, y1, x2, y2)</span></span><br><span class="line">            top[idx].reshape(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">            self._name_to_top_map[<span class="string">'rois'</span>] = idx</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># labels blob: R categorical labels in [0, ..., K] for K foreground</span></span><br><span class="line">            <span class="comment"># classes plus background</span></span><br><span class="line">            top[idx].reshape(<span class="number">1</span>)</span><br><span class="line">            self._name_to_top_map[<span class="string">'labels'</span>] = idx</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cfg.TRAIN.BBOX_REG:</span><br><span class="line">                <span class="comment"># bbox_targets blob: R bounding-box regression targets with 4</span></span><br><span class="line">                <span class="comment"># targets per class</span></span><br><span class="line">                top[idx].reshape(<span class="number">1</span>, self._num_classes * <span class="number">4</span>)</span><br><span class="line">                self._name_to_top_map[<span class="string">'bbox_targets'</span>] = idx</span><br><span class="line">                idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># bbox_inside_weights blob: At most 4 targets per roi are active;</span></span><br><span class="line">                <span class="comment"># thisbinary vector sepcifies the subset of active targets</span></span><br><span class="line">                top[idx].reshape(<span class="number">1</span>, self._num_classes * <span class="number">4</span>)</span><br><span class="line">                self._name_to_top_map[<span class="string">'bbox_inside_weights'</span>] = idx</span><br><span class="line">                idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                top[idx].reshape(<span class="number">1</span>, self._num_classes * <span class="number">4</span>)</span><br><span class="line">                self._name_to_top_map[<span class="string">'bbox_outside_weights'</span>] = idx</span><br><span class="line">                idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'RoiDataLayer: name_to_top:'</span>, self._name_to_top_map</span><br><span class="line">        <span class="keyword">assert</span> len(top) == len(self._name_to_top_map)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, bottom, top)</span>:</span></span><br><span class="line">        <span class="string">"""Get blobs and copy them into this layer's top blob vector."""</span></span><br><span class="line">        blobs = self._get_next_minibatch()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> blob_name, blob <span class="keyword">in</span> blobs.iteritems():</span><br><span class="line">            top_ind = self._name_to_top_map[blob_name]</span><br><span class="line">            <span class="comment"># Reshape net's input blobs</span></span><br><span class="line">            top[top_ind].reshape(*(blob.shape))</span><br><span class="line">            <span class="comment"># Copy data into net's input blobs</span></span><br><span class="line">            top[top_ind].data[...] = blob.astype(np.float32, copy=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, top, propagate_down, bottom)</span>:</span></span><br><span class="line">        <span class="string">"""This layer does not propagate gradients."""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reshape</span><span class="params">(self, bottom, top)</span>:</span></span><br><span class="line">        <span class="string">"""Reshaping happens during the call to forward."""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlobFetcher</span><span class="params">(Process)</span>:</span></span><br><span class="line">    <span class="string">"""Experimental class for prefetching blobs in a separate process."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, queue, roidb, num_classes)</span>:</span></span><br><span class="line">        super(BlobFetcher, self).__init__()</span><br><span class="line">        self._queue = queue</span><br><span class="line">        self._roidb = roidb</span><br><span class="line">        self._num_classes = num_classes</span><br><span class="line">        self._perm = <span class="literal">None</span></span><br><span class="line">        self._cur = <span class="number">0</span></span><br><span class="line">        self._shuffle_roidb_inds()</span><br><span class="line">        <span class="comment"># fix the random seed for reproducibility</span></span><br><span class="line">        np.random.seed(cfg.RNG_SEED)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_shuffle_roidb_inds</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Randomly permute the training roidb."""</span></span><br><span class="line">        <span class="comment"># TODO(rbg): remove duplicated code</span></span><br><span class="line">        self._perm = np.random.permutation(np.arange(len(self._roidb)))</span><br><span class="line">        self._cur = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_next_minibatch_inds</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the roidb indices for the next minibatch."""</span></span><br><span class="line">        <span class="comment"># TODO(rbg): remove duplicated code</span></span><br><span class="line">        <span class="keyword">if</span> self._cur + cfg.TRAIN.IMS_PER_BATCH &gt;= len(self._roidb):</span><br><span class="line">            self._shuffle_roidb_inds()</span><br><span class="line"></span><br><span class="line">        db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]</span><br><span class="line">        self._cur += cfg.TRAIN.IMS_PER_BATCH</span><br><span class="line">        <span class="keyword">return</span> db_inds</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'BlobFetcher started'</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            db_inds = self._get_next_minibatch_inds()</span><br><span class="line">            minibatch_db = [self._roidb[i] <span class="keyword">for</span> i <span class="keyword">in</span> db_inds]</span><br><span class="line">            blobs = get_minibatch(minibatch_db, self._num_classes)</span><br><span class="line">            self._queue.put(blobs)</span><br></pre></td></tr></table></figure>
<h3 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h3><p>下面的roidb都只是一次batch的  </p>
<p>3.1 setup  在caffe.SGDSolver时调用；setup的top（list猜测是c++的vector）的每个项是caffe._caffe.Blob<br>（猜测，输出的Top shape就是上面的top,在setup中被shape；top[0],1 3 [600] 1000;top[1],1 3;top[2], 1 4)（疑问，在forward中blob的数据shape被重置，有时大小甚至会不定）</p>
<p>3.2 name_to_top: {‘gt_boxes’: 2, ‘data’: 0, ‘im_info’: 1}  字典的value值是top的对应索引</p>
<p>3.3 solver.step(1)  会调用layer的reshape、forward</p>
<p>3.4 self._perm： 把roidb的索引打乱，造成图片的shuffle，打乱的索引存储的地方</p>
<p>3.5 cfg.TRAIN.IMS_PER_BATCH：（猜测，每次取图片的数量）</p>
<p>3.6 self._cur： 相当于一个指向_perm的指针，每次取走图片后，他会跟着变化</p>
<p>3.7 db_inds： 本次取得图片的索引</p>
<p>3.8 def _get_next_minibatch_inds(self)： 取得本次图片的索引，即db_inds</p>
<p>3.9 minibatch_db： 本次的 roidb</p>
<p>3.10 _num_classes： 网络里的类别数值  21</p>
<p>3.11 forward（）： 得到blob并处理放进top</p>
<p>solver.step(1)-&gt;reshape-&gt;forward -&gt; _get_next_minbatch-&gt;_get_next_minbatch_inds-&gt; (前面在layers里,现在进入minibatch组建真正的blob)get_minibatch</p>
]]></content>
      <categories>
        <category>faster rcnn</category>
        <category>faster cnn源码理解</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>faster rcnn源码理解</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn源码解读（六）之minibatch</title>
    <url>/2018/04/08/faster%20rcnn%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E5%85%AD%EF%BC%89%E4%B9%8Bminibatch/</url>
    <content><![CDATA[<p>faster rcnn用python版本的  <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="noopener"> https://github.com/rbgirshick/py-faster-rcnn</a></p>
<p>minibatch源码： <a href="https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/roi_data_layer/minibatch.py" target="_blank" rel="noopener"> https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/roi_data_layer/minibatch.py</a></p>
<h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Fast R-CNN</span></span><br><span class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></span><br><span class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></span><br><span class="line"><span class="comment"># Written by Ross Girshick</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""Compute minibatch blobs for training a Fast R-CNN network."""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy.random <span class="keyword">as</span> npr</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> fast_rcnn.config <span class="keyword">import</span> cfg</span><br><span class="line"><span class="keyword">from</span> utils.blob <span class="keyword">import</span> prep_im_for_blob, im_list_to_blob</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_minibatch</span><span class="params">(roidb, num_classes)</span>:</span></span><br><span class="line">    <span class="string">"""Given a roidb, construct a minibatch sampled from it."""</span></span><br><span class="line">    num_images = len(roidb)</span><br><span class="line">    <span class="comment"># Sample random scales to use for each image in this batch</span></span><br><span class="line">    random_scale_inds = npr.randint(<span class="number">0</span>, high=len(cfg.TRAIN.SCALES),</span><br><span class="line">                                    size=num_images)<span class="comment">#随机索引组成的numpy，大小是roidb的长度</span></span><br><span class="line">    <span class="keyword">assert</span>(cfg.TRAIN.BATCH_SIZE % num_images == <span class="number">0</span>), \</span><br><span class="line">        <span class="string">'num_images (&#123;&#125;) must divide BATCH_SIZE (&#123;&#125;)'</span>. \</span><br><span class="line">        format(num_images, cfg.TRAIN.BATCH_SIZE)</span><br><span class="line">    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images</span><br><span class="line">    fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get the input image blob, formatted for caffe</span></span><br><span class="line">    im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)</span><br><span class="line"></span><br><span class="line">    blobs = &#123;<span class="string">'data'</span>: im_blob&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cfg.TRAIN.HAS_RPN:</span><br><span class="line">        <span class="keyword">assert</span> len(im_scales) == <span class="number">1</span>, <span class="string">"Single batch only"</span></span><br><span class="line">        <span class="keyword">assert</span> len(roidb) == <span class="number">1</span>, <span class="string">"Single batch only"</span></span><br><span class="line">        <span class="comment"># gt boxes: (x1, y1, x2, y2, cls)</span></span><br><span class="line">        gt_inds = np.where(roidb[<span class="number">0</span>][<span class="string">'gt_classes'</span>] != <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">        gt_boxes = np.empty((len(gt_inds), <span class="number">5</span>), dtype=np.float32)</span><br><span class="line">        gt_boxes[:, <span class="number">0</span>:<span class="number">4</span>] = roidb[<span class="number">0</span>][<span class="string">'boxes'</span>][gt_inds, :] * im_scales[<span class="number">0</span>]</span><br><span class="line">        gt_boxes[:, <span class="number">4</span>] = roidb[<span class="number">0</span>][<span class="string">'gt_classes'</span>][gt_inds]</span><br><span class="line">        blobs[<span class="string">'gt_boxes'</span>] = gt_boxes</span><br><span class="line">        blobs[<span class="string">'im_info'</span>] = np.array(</span><br><span class="line">            [[im_blob.shape[<span class="number">2</span>], im_blob.shape[<span class="number">3</span>], im_scales[<span class="number">0</span>]]],</span><br><span class="line">            dtype=np.float32)</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># not using RPN</span></span><br><span class="line">        <span class="comment"># Now, build the region of interest and label blobs</span></span><br><span class="line">        rois_blob = np.zeros((<span class="number">0</span>, <span class="number">5</span>), dtype=np.float32)</span><br><span class="line">        labels_blob = np.zeros((<span class="number">0</span>), dtype=np.float32)</span><br><span class="line">        bbox_targets_blob = np.zeros((<span class="number">0</span>, <span class="number">4</span> * num_classes), dtype=np.float32)</span><br><span class="line">        bbox_inside_blob = np.zeros(bbox_targets_blob.shape, dtype=np.float32)</span><br><span class="line">        <span class="comment"># all_overlaps = []</span></span><br><span class="line">        <span class="keyword">for</span> im_i <span class="keyword">in</span> xrange(num_images):</span><br><span class="line">            labels, overlaps, im_rois, bbox_targets, bbox_inside_weights \</span><br><span class="line">                = _sample_rois(roidb[im_i], fg_rois_per_image, rois_per_image,</span><br><span class="line">                               num_classes)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Add to RoIs blob</span></span><br><span class="line">            rois = _project_im_rois(im_rois, im_scales[im_i])</span><br><span class="line">            batch_ind = im_i * np.ones((rois.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">            rois_blob_this_image = np.hstack((batch_ind, rois))</span><br><span class="line">            rois_blob = np.vstack((rois_blob, rois_blob_this_image))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Add to labels, bbox targets, and bbox loss blobs</span></span><br><span class="line">            labels_blob = np.hstack((labels_blob, labels))</span><br><span class="line">            bbox_targets_blob = np.vstack((bbox_targets_blob, bbox_targets))</span><br><span class="line">            bbox_inside_blob = np.vstack((bbox_inside_blob, bbox_inside_weights))</span><br><span class="line">            <span class="comment"># all_overlaps = np.hstack((all_overlaps, overlaps))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># For debug visualizations</span></span><br><span class="line">        <span class="comment"># _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps)</span></span><br><span class="line"></span><br><span class="line">        blobs[<span class="string">'rois'</span>] = rois_blob</span><br><span class="line">        blobs[<span class="string">'labels'</span>] = labels_blob</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cfg.TRAIN.BBOX_REG:</span><br><span class="line">            blobs[<span class="string">'bbox_targets'</span>] = bbox_targets_blob</span><br><span class="line">            blobs[<span class="string">'bbox_inside_weights'</span>] = bbox_inside_blob</span><br><span class="line">            blobs[<span class="string">'bbox_outside_weights'</span>] = \</span><br><span class="line">                np.array(bbox_inside_blob &gt; <span class="number">0</span>).astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> blobs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sample_rois</span><span class="params">(roidb, fg_rois_per_image, rois_per_image, num_classes)</span>:</span></span><br><span class="line">    <span class="string">"""Generate a random sample of RoIs comprising foreground and background</span></span><br><span class="line"><span class="string">    examples.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># label = class RoI has max overlap with</span></span><br><span class="line">    labels = roidb[<span class="string">'max_classes'</span>]</span><br><span class="line">    overlaps = roidb[<span class="string">'max_overlaps'</span>]</span><br><span class="line">    rois = roidb[<span class="string">'boxes'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Select foreground RoIs as those with &gt;= FG_THRESH overlap</span></span><br><span class="line">    fg_inds = np.where(overlaps &gt;= cfg.TRAIN.FG_THRESH)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># Guard against the case when an image has fewer than fg_rois_per_image</span></span><br><span class="line">    <span class="comment"># foreground RoIs</span></span><br><span class="line">    fg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)</span><br><span class="line">    <span class="comment"># Sample foreground regions without replacement</span></span><br><span class="line">    <span class="keyword">if</span> fg_inds.size &gt; <span class="number">0</span>:</span><br><span class="line">        fg_inds = npr.choice(</span><br><span class="line">                fg_inds, size=fg_rois_per_this_image, replace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)</span></span><br><span class="line">    bg_inds = np.where((overlaps &lt; cfg.TRAIN.BG_THRESH_HI) &amp;</span><br><span class="line">                       (overlaps &gt;= cfg.TRAIN.BG_THRESH_LO))[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># Compute number of background RoIs to take from this image (guarding</span></span><br><span class="line">    <span class="comment"># against there being fewer than desired)</span></span><br><span class="line">    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image</span><br><span class="line">    bg_rois_per_this_image = np.minimum(bg_rois_per_this_image,</span><br><span class="line">                                        bg_inds.size)</span><br><span class="line">    <span class="comment"># Sample foreground regions without replacement</span></span><br><span class="line">    <span class="keyword">if</span> bg_inds.size &gt; <span class="number">0</span>:</span><br><span class="line">        bg_inds = npr.choice(</span><br><span class="line">                bg_inds, size=bg_rois_per_this_image, replace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The indices that we're selecting (both fg and bg)</span></span><br><span class="line">    keep_inds = np.append(fg_inds, bg_inds)</span><br><span class="line">    <span class="comment"># Select sampled values from various arrays:</span></span><br><span class="line">    labels = labels[keep_inds]</span><br><span class="line">    <span class="comment"># Clamp labels for the background RoIs to 0</span></span><br><span class="line">    labels[fg_rois_per_this_image:] = <span class="number">0</span></span><br><span class="line">    overlaps = overlaps[keep_inds]</span><br><span class="line">    rois = rois[keep_inds]</span><br><span class="line"></span><br><span class="line">    bbox_targets, bbox_inside_weights = _get_bbox_regression_labels(</span><br><span class="line">            roidb[<span class="string">'bbox_targets'</span>][keep_inds, :], num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> labels, overlaps, rois, bbox_targets, bbox_inside_weights</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_image_blob</span><span class="params">(roidb, scale_inds)</span>:</span></span><br><span class="line">    <span class="string">"""Builds an input blob from the images in the roidb at the specified</span></span><br><span class="line"><span class="string">    scales.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    num_images = len(roidb)</span><br><span class="line">    processed_ims = []</span><br><span class="line">    im_scales = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_images):</span><br><span class="line">        im = cv2.imread(roidb[i][<span class="string">'image'</span>])</span><br><span class="line">        <span class="keyword">if</span> roidb[i][<span class="string">'flipped'</span>]:</span><br><span class="line">            im = im[:, ::<span class="number">-1</span>, :]</span><br><span class="line">        target_size = cfg.TRAIN.SCALES[scale_inds[i]]</span><br><span class="line">        im, im_scale = prep_im_for_blob(im, cfg.PIXEL_MEANS, target_size,</span><br><span class="line">                                        cfg.TRAIN.MAX_SIZE)prep_im_for_blob： util的blob.py中；用于将图片平均后缩放。<span class="comment">#im_scales： 每张图片的缩放率</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#  cfg.PIXEL_MEANS： 原始图片会集体减去该值达到mean</span></span><br><span class="line">        im_scales.append(im_scale)</span><br><span class="line">        processed_ims.append(im)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a blob to hold the input images</span></span><br><span class="line">    blob = im_list_to_blob(processed_ims)<span class="comment">#将以list形式存放的图片数据处理成(batch elem, channel, height, width)的im_blob形式，height，width用的是此次计算所有图片的最大值</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> blob, im_scales<span class="comment">#blob是一个字典，与name_to_top对应，方便把blob数据放进top</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_project_im_rois</span><span class="params">(im_rois, im_scale_factor)</span>:</span></span><br><span class="line">    <span class="string">"""Project image RoIs into the rescaled training image."""</span></span><br><span class="line">    rois = im_rois * im_scale_factor</span><br><span class="line">    <span class="keyword">return</span> rois</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_bbox_regression_labels</span><span class="params">(bbox_target_data, num_classes)</span>:</span></span><br><span class="line">    <span class="string">"""Bounding-box regression targets are stored in a compact form in the</span></span><br><span class="line"><span class="string">    roidb.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This function expands those targets into the 4-of-4*K representation used</span></span><br><span class="line"><span class="string">    by the network (i.e. only one class has non-zero targets). The loss weights</span></span><br><span class="line"><span class="string">    are similarly expanded.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bbox_target_data (ndarray): N x 4K blob of regression targets</span></span><br><span class="line"><span class="string">        bbox_inside_weights (ndarray): N x 4K blob of loss weights</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    clss = bbox_target_data[:, <span class="number">0</span>]</span><br><span class="line">    bbox_targets = np.zeros((clss.size, <span class="number">4</span> * num_classes), dtype=np.float32)</span><br><span class="line">    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)</span><br><span class="line">    inds = np.where(clss &gt; <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> inds:</span><br><span class="line">        cls = clss[ind]</span><br><span class="line">        start = <span class="number">4</span> * cls</span><br><span class="line">        end = start + <span class="number">4</span></span><br><span class="line">        bbox_targets[ind, start:end] = bbox_target_data[ind, <span class="number">1</span>:]</span><br><span class="line">        bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS</span><br><span class="line">    <span class="keyword">return</span> bbox_targets, bbox_inside_weights</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_vis_minibatch</span><span class="params">(im_blob, rois_blob, labels_blob, overlaps)</span>:</span></span><br><span class="line">    <span class="string">"""Visualize a mini-batch for debugging."""</span></span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(rois_blob.shape[<span class="number">0</span>]):</span><br><span class="line">        rois = rois_blob[i, :]</span><br><span class="line">        im_ind = rois[<span class="number">0</span>]</span><br><span class="line">        roi = rois[<span class="number">1</span>:]</span><br><span class="line">        im = im_blob[im_ind, :, :, :].transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)).copy()</span><br><span class="line">        im += cfg.PIXEL_MEANS</span><br><span class="line">        im = im[:, :, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>)]</span><br><span class="line">        im = im.astype(np.uint8)</span><br><span class="line">        cls = labels_blob[i]</span><br><span class="line">        plt.imshow(im)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'class: '</span>, cls, <span class="string">' overlap: '</span>, overlaps[i]</span><br><span class="line">        plt.gca().add_patch(</span><br><span class="line">            plt.Rectangle((roi[<span class="number">0</span>], roi[<span class="number">1</span>]), roi[<span class="number">2</span>] - roi[<span class="number">0</span>],</span><br><span class="line">                          roi[<span class="number">3</span>] - roi[<span class="number">1</span>], fill=<span class="literal">False</span>,</span><br><span class="line">                          edgecolor=<span class="string">'r'</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">            )</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h3><p>solver.step(1)-&gt;reshape-》forward-&gt;_get_next_minbatch-&gt;_get_next_minbatch_inds-&gt;(前面在layers里,现在进入minibatch组建真正的blob)get_minibatch  </p>
<p>cfg.TRAIN.SCALES： 图片被缩放的target_size列表<br>random_scale_inds：列表的随机索引组成的numpy，大小是roidb的长度<br>cfg.PIXEL_MEANS： 原始图片会集体减去该值达到mean<br>im_scales  ： 每张图片的缩放率</p>
<p>缩放率的求法:  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">im_scales = target_size/min(width, height)</span><br><span class="line"><span class="keyword">if</span> im_scales * max(width, height) &gt; cfg.TRAIN.MAX_SIZE:</span><br><span class="line">    im_scales = cfg.TRAIN.MAX_SIZE * max(width, height)</span><br></pre></td></tr></table></figure>

<p>prep_im_for_blob： util的blob.py中；用于将图片平均后缩放。<br>im_list_to_blob(ims)： 将以list形式存放的图片数据处理成(batch elem, channel, height, width)的im_blob形式，height，width用的是此次计算所有图片的最大值</p>
<p>blob是一个字典, 与name_to_top对应，方便把blob数据放进top: </p>
<ul>
<li>data, 一个batch处理过的所有图片数据，即上面的im_blob</li>
<li>im_info, [im_blob.shape[2], im_blob.shape[3], im_scales[0]]</li>
<li>gt_boxes, 是前四列是  box  的值，第五列是  box  的类别。</li>
<li>box, =原box*im_scales</li>
</ul>
<p><strong>！！！  minibatch.py  中  34  行的代码表明，  batchsize  即  cfg.TRAIN.IMS_PER_BATCH  只能是1</strong></p>
]]></content>
      <categories>
        <category>faster rcnn</category>
        <category>faster cnn源码理解</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>faster rcnn源码理解</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn源码解读（四）之数据类型imdb.py和pascal_voc.py（主要是imdb和roidb数据类型的解说）</title>
    <url>/2018/04/08/faster%20rcnn%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8Bimdb.py%E5%92%8Cpascal_voc.py%EF%BC%88%E4%B8%BB%E8%A6%81%E6%98%AFimdb%E5%92%8Croidb%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E8%A7%A3%E8%AF%B4%EF%BC%89/</url>
    <content><![CDATA[<p>faster用python版本的  <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="noopener"> https://github.com/rbgirshick/py-faster-rcnn</a></p>
<p>imdb.py源码地址： <a href="https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/imdb.py" target="_blank" rel="noopener"> https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/imdb.py </a></p>
<h3 id="imdb源码"><a href="#imdb源码" class="headerlink" title="imdb源码"></a>imdb源码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Fast R-CNN</span></span><br><span class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></span><br><span class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></span><br><span class="line"><span class="comment"># Written by Ross Girshick</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"><span class="keyword">import</span> PIL</span><br><span class="line"><span class="keyword">from</span> utils.cython_bbox <span class="keyword">import</span> bbox_overlaps</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.sparse</span><br><span class="line"><span class="keyword">from</span> fast_rcnn.config <span class="keyword">import</span> cfg</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">imdb</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Image database."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self._name = name</span><br><span class="line">        self._num_classes = <span class="number">0</span><span class="comment">#类别的长度</span></span><br><span class="line">        self._classes = []<span class="comment">#类别定义</span></span><br><span class="line">        self._image_index = []<span class="comment">#a list of image name（read from eg：root/data + /VOCdevkit2007/VOC2007/ImageSets/Main/&#123;image_set&#125;.txt）</span></span><br><span class="line">        self._obj_proposer = <span class="string">'selective_search'</span></span><br><span class="line">        self._roidb = <span class="literal">None</span><span class="comment">#gt_roidb（cfg.TRAIN.PROPOSAL_METHOD=gt导致了此操作）</span></span><br><span class="line">        self._roidb_handler = self.default_roidb</span><br><span class="line">        <span class="comment"># Use this dict for storing dataset specific config options</span></span><br><span class="line">        self.config = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">name</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_classes</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._classes)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">classes</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._classes</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">image_index</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._image_index</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">roidb_handler</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._roidb_handler</span><br><span class="line"></span><br><span class="line"><span class="meta">    @roidb_handler.setter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">roidb_handler</span><span class="params">(self, val)</span>:</span></span><br><span class="line">        self._roidb_handler = val</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_proposal_method</span><span class="params">(self, method)</span>:</span></span><br><span class="line">        method = eval(<span class="string">'self.'</span> + method + <span class="string">'_roidb'</span>)</span><br><span class="line">        self.roidb_handler = method</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">roidb</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># A roidb is a list of dictionaries, each with the following keys:</span></span><br><span class="line">        <span class="comment">#   boxes</span></span><br><span class="line">        <span class="comment">#   gt_overlaps</span></span><br><span class="line">        <span class="comment">#   gt_classes</span></span><br><span class="line">        <span class="comment">#   flipped</span></span><br><span class="line">        <span class="keyword">if</span> self._roidb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self._roidb</span><br><span class="line">        self._roidb = self.roidb_handler()</span><br><span class="line">        <span class="keyword">return</span> self._roidb</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cache_path</span><span class="params">(self)</span>:</span></span><br><span class="line">        cache_path = osp.abspath(osp.join(cfg.DATA_DIR, <span class="string">'cache'</span>))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(cache_path):</span><br><span class="line">            os.makedirs(cache_path)</span><br><span class="line">        <span class="keyword">return</span> cache_path</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_images</span><span class="params">(self)</span>:</span></span><br><span class="line">      <span class="keyword">return</span> len(self.image_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">image_path_at</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">default_roidb</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate_detections</span><span class="params">(self, all_boxes, output_dir=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        all_boxes is a list of length number-of-classes.</span></span><br><span class="line"><span class="string">        Each list element is a list of length number-of-images.</span></span><br><span class="line"><span class="string">        Each of those list elements is either an empty list []</span></span><br><span class="line"><span class="string">        or a numpy array of detection.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        all_boxes[class][image] = [] or np.array of shape #dets x 5</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_widths</span><span class="params">(self)</span>:</span></span><br><span class="line">      <span class="keyword">return</span> [PIL.Image.open(self.image_path_at(i)).size[<span class="number">0</span>]</span><br><span class="line">              <span class="keyword">for</span> i <span class="keyword">in</span> xrange(self.num_images)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append_flipped_images</span><span class="params">(self)</span>:</span></span><br><span class="line">        num_images = self.num_images</span><br><span class="line">        widths = self._get_widths()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_images):</span><br><span class="line">            boxes = self.roidb[i][<span class="string">'boxes'</span>].copy()</span><br><span class="line">            oldx1 = boxes[:, <span class="number">0</span>].copy()</span><br><span class="line">            oldx2 = boxes[:, <span class="number">2</span>].copy()</span><br><span class="line">            boxes[:, <span class="number">0</span>] = widths[i] - oldx2 - <span class="number">1</span></span><br><span class="line">            boxes[:, <span class="number">2</span>] = widths[i] - oldx1 - <span class="number">1</span></span><br><span class="line">            <span class="keyword">assert</span> (boxes[:, <span class="number">2</span>] &gt;= boxes[:, <span class="number">0</span>]).all()</span><br><span class="line">            entry = &#123;<span class="string">'boxes'</span> : boxes,</span><br><span class="line">                     <span class="string">'gt_overlaps'</span> : self.roidb[i][<span class="string">'gt_overlaps'</span>],</span><br><span class="line">                     <span class="string">'gt_classes'</span> : self.roidb[i][<span class="string">'gt_classes'</span>],</span><br><span class="line">                     <span class="string">'flipped'</span> : <span class="literal">True</span>&#125;</span><br><span class="line">            self.roidb.append(entry)</span><br><span class="line">        self._image_index = self._image_index * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate_recall</span><span class="params">(self, candidate_boxes=None, thresholds=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                        area=<span class="string">'all'</span>, limit=None)</span>:</span></span><br><span class="line">        <span class="string">"""Evaluate detection proposal recall metrics.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            results: dictionary of results with keys</span></span><br><span class="line"><span class="string">                'ar': average recall</span></span><br><span class="line"><span class="string">                'recalls': vector recalls at each IoU overlap threshold</span></span><br><span class="line"><span class="string">                'thresholds': vector of IoU overlap thresholds</span></span><br><span class="line"><span class="string">                'gt_overlaps': vector of all ground-truth overlaps</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Record max overlap value for each gt box</span></span><br><span class="line">        <span class="comment"># Return vector of overlap values</span></span><br><span class="line">        areas = &#123; <span class="string">'all'</span>: <span class="number">0</span>, <span class="string">'small'</span>: <span class="number">1</span>, <span class="string">'medium'</span>: <span class="number">2</span>, <span class="string">'large'</span>: <span class="number">3</span>,</span><br><span class="line">                  <span class="string">'96-128'</span>: <span class="number">4</span>, <span class="string">'128-256'</span>: <span class="number">5</span>, <span class="string">'256-512'</span>: <span class="number">6</span>, <span class="string">'512-inf'</span>: <span class="number">7</span>&#125;</span><br><span class="line">        area_ranges = [ [<span class="number">0</span>**<span class="number">2</span>, <span class="number">1e5</span>**<span class="number">2</span>],    <span class="comment"># all</span></span><br><span class="line">                        [<span class="number">0</span>**<span class="number">2</span>, <span class="number">32</span>**<span class="number">2</span>],     <span class="comment"># small</span></span><br><span class="line">                        [<span class="number">32</span>**<span class="number">2</span>, <span class="number">96</span>**<span class="number">2</span>],    <span class="comment"># medium</span></span><br><span class="line">                        [<span class="number">96</span>**<span class="number">2</span>, <span class="number">1e5</span>**<span class="number">2</span>],   <span class="comment"># large</span></span><br><span class="line">                        [<span class="number">96</span>**<span class="number">2</span>, <span class="number">128</span>**<span class="number">2</span>],   <span class="comment"># 96-128</span></span><br><span class="line">                        [<span class="number">128</span>**<span class="number">2</span>, <span class="number">256</span>**<span class="number">2</span>],  <span class="comment"># 128-256</span></span><br><span class="line">                        [<span class="number">256</span>**<span class="number">2</span>, <span class="number">512</span>**<span class="number">2</span>],  <span class="comment"># 256-512</span></span><br><span class="line">                        [<span class="number">512</span>**<span class="number">2</span>, <span class="number">1e5</span>**<span class="number">2</span>],  <span class="comment"># 512-inf</span></span><br><span class="line">                      ]</span><br><span class="line">        <span class="keyword">assert</span> areas.has_key(area), <span class="string">'unknown area range: &#123;&#125;'</span>.format(area)</span><br><span class="line">        area_range = area_ranges[areas[area]]</span><br><span class="line">        gt_overlaps = np.zeros(<span class="number">0</span>)</span><br><span class="line">        num_pos = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(self.num_images):</span><br><span class="line">            <span class="comment"># Checking for max_overlaps == 1 avoids including crowd annotations</span></span><br><span class="line">            <span class="comment"># (...pretty hacking :/)</span></span><br><span class="line">            max_gt_overlaps = self.roidb[i][<span class="string">'gt_overlaps'</span>].toarray().max(axis=<span class="number">1</span>)</span><br><span class="line">            gt_inds = np.where((self.roidb[i][<span class="string">'gt_classes'</span>] &gt; <span class="number">0</span>) &amp;</span><br><span class="line">                               (max_gt_overlaps == <span class="number">1</span>))[<span class="number">0</span>]</span><br><span class="line">            gt_boxes = self.roidb[i][<span class="string">'boxes'</span>][gt_inds, :]</span><br><span class="line">            gt_areas = self.roidb[i][<span class="string">'seg_areas'</span>][gt_inds]</span><br><span class="line">            valid_gt_inds = np.where((gt_areas &gt;= area_range[<span class="number">0</span>]) &amp;</span><br><span class="line">                                     (gt_areas &lt;= area_range[<span class="number">1</span>]))[<span class="number">0</span>]</span><br><span class="line">            gt_boxes = gt_boxes[valid_gt_inds, :]</span><br><span class="line">            num_pos += len(valid_gt_inds)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> candidate_boxes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># If candidate_boxes is not supplied, the default is to use the</span></span><br><span class="line">                <span class="comment"># non-ground-truth boxes from this roidb</span></span><br><span class="line">                non_gt_inds = np.where(self.roidb[i][<span class="string">'gt_classes'</span>] == <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">                boxes = self.roidb[i][<span class="string">'boxes'</span>][non_gt_inds, :]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                boxes = candidate_boxes[i]</span><br><span class="line">            <span class="keyword">if</span> boxes.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> limit <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> boxes.shape[<span class="number">0</span>] &gt; limit:</span><br><span class="line">                boxes = boxes[:limit, :]</span><br><span class="line"></span><br><span class="line">            overlaps = bbox_overlaps(boxes.astype(np.float),</span><br><span class="line">                                     gt_boxes.astype(np.float))</span><br><span class="line"></span><br><span class="line">            _gt_overlaps = np.zeros((gt_boxes.shape[<span class="number">0</span>]))</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> xrange(gt_boxes.shape[<span class="number">0</span>]):</span><br><span class="line">                <span class="comment"># find which proposal box maximally covers each gt box</span></span><br><span class="line">                argmax_overlaps = overlaps.argmax(axis=<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># and get the iou amount of coverage for each gt box</span></span><br><span class="line">                max_overlaps = overlaps.max(axis=<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># find which gt box is 'best' covered (i.e. 'best' = most iou)</span></span><br><span class="line">                gt_ind = max_overlaps.argmax()</span><br><span class="line">                gt_ovr = max_overlaps.max()</span><br><span class="line">                <span class="keyword">assert</span>(gt_ovr &gt;= <span class="number">0</span>)</span><br><span class="line">                <span class="comment"># find the proposal box that covers the best covered gt box</span></span><br><span class="line">                box_ind = argmax_overlaps[gt_ind]</span><br><span class="line">                <span class="comment"># record the iou coverage of this gt box</span></span><br><span class="line">                _gt_overlaps[j] = overlaps[box_ind, gt_ind]</span><br><span class="line">                <span class="keyword">assert</span>(_gt_overlaps[j] == gt_ovr)</span><br><span class="line">                <span class="comment"># mark the proposal box and the gt box as used</span></span><br><span class="line">                overlaps[box_ind, :] = <span class="number">-1</span></span><br><span class="line">                overlaps[:, gt_ind] = <span class="number">-1</span></span><br><span class="line">            <span class="comment"># append recorded iou coverage level</span></span><br><span class="line">            gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))</span><br><span class="line"></span><br><span class="line">        gt_overlaps = np.sort(gt_overlaps)</span><br><span class="line">        <span class="keyword">if</span> thresholds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            step = <span class="number">0.05</span></span><br><span class="line">            thresholds = np.arange(<span class="number">0.5</span>, <span class="number">0.95</span> + <span class="number">1e-5</span>, step)</span><br><span class="line">        recalls = np.zeros_like(thresholds)</span><br><span class="line">        <span class="comment"># compute recall for each iou threshold</span></span><br><span class="line">        <span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(thresholds):</span><br><span class="line">            recalls[i] = (gt_overlaps &gt;= t).sum() / float(num_pos)</span><br><span class="line">        <span class="comment"># ar = 2 * np.trapz(recalls, thresholds)</span></span><br><span class="line">        ar = recalls.mean()</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'ar'</span>: ar, <span class="string">'recalls'</span>: recalls, <span class="string">'thresholds'</span>: thresholds,</span><br><span class="line">                <span class="string">'gt_overlaps'</span>: gt_overlaps&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_roidb_from_box_list</span><span class="params">(self, box_list, gt_roidb)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> len(box_list) == self.num_images, \</span><br><span class="line">                <span class="string">'Number of boxes must match number of ground-truth images'</span></span><br><span class="line">        roidb = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(self.num_images):</span><br><span class="line">            boxes = box_list[i]</span><br><span class="line">            num_boxes = boxes.shape[<span class="number">0</span>]</span><br><span class="line">            overlaps = np.zeros((num_boxes, self.num_classes), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> gt_roidb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> gt_roidb[i][<span class="string">'boxes'</span>].size &gt; <span class="number">0</span>:</span><br><span class="line">                gt_boxes = gt_roidb[i][<span class="string">'boxes'</span>]</span><br><span class="line">                gt_classes = gt_roidb[i][<span class="string">'gt_classes'</span>]</span><br><span class="line">                gt_overlaps = bbox_overlaps(boxes.astype(np.float),</span><br><span class="line">                                            gt_boxes.astype(np.float))</span><br><span class="line">                argmaxes = gt_overlaps.argmax(axis=<span class="number">1</span>)</span><br><span class="line">                maxes = gt_overlaps.max(axis=<span class="number">1</span>)</span><br><span class="line">                I = np.where(maxes &gt; <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">                overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]</span><br><span class="line"></span><br><span class="line">            overlaps = scipy.sparse.csr_matrix(overlaps)</span><br><span class="line">            roidb.append(&#123;</span><br><span class="line">                <span class="string">'boxes'</span> : boxes,</span><br><span class="line">                <span class="string">'gt_classes'</span> : np.zeros((num_boxes,), dtype=np.int32),</span><br><span class="line">                <span class="string">'gt_overlaps'</span> : overlaps,</span><br><span class="line">                <span class="string">'flipped'</span> : <span class="literal">False</span>,</span><br><span class="line">                <span class="string">'seg_areas'</span> : np.zeros((num_boxes,), dtype=np.float32),</span><br><span class="line">            &#125;)</span><br><span class="line">        <span class="keyword">return</span> roidb</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge_roidbs</span><span class="params">(a, b)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> len(a) == len(b)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(a)):</span><br><span class="line">            a[i][<span class="string">'boxes'</span>] = np.vstack((a[i][<span class="string">'boxes'</span>], b[i][<span class="string">'boxes'</span>]))</span><br><span class="line">            a[i][<span class="string">'gt_classes'</span>] = np.hstack((a[i][<span class="string">'gt_classes'</span>],</span><br><span class="line">                                            b[i][<span class="string">'gt_classes'</span>]))</span><br><span class="line">            a[i][<span class="string">'gt_overlaps'</span>] = scipy.sparse.vstack([a[i][<span class="string">'gt_overlaps'</span>],</span><br><span class="line">                                                       b[i][<span class="string">'gt_overlaps'</span>]])</span><br><span class="line">            a[i][<span class="string">'seg_areas'</span>] = np.hstack((a[i][<span class="string">'seg_areas'</span>],</span><br><span class="line">                                           b[i][<span class="string">'seg_areas'</span>]))</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">competition_mode</span><span class="params">(self, on)</span>:</span></span><br><span class="line">        <span class="string">"""Turn competition mode on or off."""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h3 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h3><p>get_imdb-&gt;factory-&gt;pascal_voc-&gt;(继承)imdb</p>
<h4 id="factory"><a href="#factory" class="headerlink" title="factory"></a>factory</h4><p>year = [‘2007’, ‘2012’]<br>split = [‘train’, ‘val’, ‘trainval’, ‘test’]</p>
<h4 id="imdb"><a href="#imdb" class="headerlink" title="imdb"></a>imdb</h4><p>image_set: split<br>devkit_path: config.DATA_DIR(root/data/) + VOCdevkit + year<br>data_path: devkit_path + ‘/‘ + ‘VOC’ + year<br>image_index: a list read image name from 如，root/data + /VOCdevkit2007/VOC2007/ImageSets/Main/{image_set}.txt</p>
<p>roidb: gt_roidb得到（cfg.TRAIN.PROPOSAL_METHOD=gt导致了此操作）<br>classes： 类别定义<br>num_classes： 类别的长度<br>class_to_ind：  {类别名：类别索引}字典</p>
<p>num_images(): image_index’length，数据库中图片个数<br>image_path_at（index）： 得到第index图片的地址， data_path + ‘/‘ +’JPEGImages’ + image_index[index] + image_ext(.jpg)</p>
<p>在train_faster_rcnn_alt_opt.py的imdb.set_proposal_method之后一旦用imdb.roidb都会用gt_roidb读取xml中的内容中得到部分信息<br>xml的地址： data_path + ‘/‘ + ‘Annotations’ + ‘/‘ + index + ‘.xml’<br>(root/data/) + VOCdevkit + year  + ‘/‘ + ‘VOC’ + year + ‘/‘ + ‘Annotations’ +’/‘ + index + ‘.xml’</p>
<h4 id="roidb"><a href="#roidb" class="headerlink" title="roidb"></a>roidb</h4><p>get_training_roidb： 对得到的roi做是否反转（参见roidb的flipped，为了扩充数据库）和到roidb.py的prepare_roidb中计算得到roidb的其他数据<br>一张图有一个roidb  ，每个roidb是一个字典<br>boxes: four rows. the proposal. left-up, right-down<br>gt_overlaps: len（box）× 类别数（即，每个box对应的类别。初始化时，从xml读出来的类别对应类别值是1.0，被压缩保存）<br>gt_classes: 每个box的类别索引<br>flipped: true,代表图片被水平反转，改变了boxes里第一、三列的值（所有原图都这样的操作，imdb.image_index × 2）(cfg.TRAIN.USE_FLIPPED会导致此操作的发生，见train.py 116行)<br>seg_areas： box的面积</p>
<h4 id="prepare-roidb中得到"><a href="#prepare-roidb中得到" class="headerlink" title="prepare_roidb中得到"></a>prepare_roidb中得到</h4><p>（下面的值在roidb.py的prepare_roidb中得到）<br>image： image_path_at（index），此roi的图片地址<br>width：此图片的宽<br>height：高<br>max_classes: box的类别=labels（gt_overlaps行最大值索引）<br>max_overlaps: （gt_overlaps行最大值）（max_overlaps=0，max_classes=0，即都是背景，否则不正确）<br>output_dir： ROOT_DIR + ‘output’ + EXP_DIR(‘faster_rcnn_alt_opt’) + imdb.name(“voc_2007_trainval” or “voc_2007_test”)</p>
]]></content>
      <categories>
        <category>faster rcnn</category>
        <category>faster cnn源码理解</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>faster rcnn源码理解</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop1.2.1在linux中配置安装独立运行Standalone Operation，伪分布Pseudo-Distributed Operation，集群配置三种配置和测试</title>
    <url>/2018/04/08/hadoop1.2.1%E5%9C%A8linux%E4%B8%AD%E9%85%8D%E7%BD%AE%E5%AE%89%E8%A3%85%E7%8B%AC%E7%AB%8B%E8%BF%90%E8%A1%8CStandalone%20Operation%EF%BC%8C%E4%BC%AA%E5%88%86%E5%B8%83Pseudo-Distributed%20Operation%EF%BC%8C%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E4%B8%89%E7%A7%8D%E9%85%8D%E7%BD%AE%E5%92%8C%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>由于安装已经很长时间了，有些问题我已记不得太清，如果哪里有手误敬请指出。但是要记住遇到问题可以直接上网查，就算你不知道为什么有问题你让可以把报错的那句话复制<br>直接查。</p>
<p>我这个大概适用的是1.X版本。要提醒各位，一定要学到东西，即使只是安装。不要犯直接复制别人代码ip/别人版本的问题！！！  </p>
<h2 id="安装-sun-jdk"><a href="#安装-sun-jdk" class="headerlink" title="安装  sun jdk"></a>安装  sun jdk</h2><p>要求不能安装这个一定要是sunjdk，否则后面会后问题.</p>
<h2 id="安装-ssh"><a href="#安装-ssh" class="headerlink" title="安装 ssh"></a>安装 ssh</h2><p>（hadoop使用ssh来实现cluster中各node的登录认证，即Namenode是通过SSH来启动和停止各个datanode上的各种守护进程的，所以一定要实现免密码登录，免密码ssh的设置在后文中有介绍）  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install ssh</span><br></pre></td></tr></table></figure>
<h2 id="安装rsync"><a href="#安装rsync" class="headerlink" title="安装rsync"></a>安装rsync</h2><p>（Ubuntu12.10已自带rsync） </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install rsync</span><br></pre></td></tr></table></figure>
<h2 id="安装vim"><a href="#安装vim" class="headerlink" title="安装vim"></a>安装vim</h2><p>只是为了方便，可以不安装.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ubuntu系统：</span><br><span class="line">普通用户下输入命令：  sudo apt-get install vim-gtk</span><br><span class="line">centos系统：</span><br><span class="line">普通用户下输入命令：  yum -y install vim*</span><br></pre></td></tr></table></figure>

<h1 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>1. 创建hadoop用户组以及用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo addgroup hadoop  </span><br><span class="line">sudo adduser --ingroup hadoop hadoop</span><br></pre></td></tr></table></figure>
<p>在  /home/下会有一个新的hadoop文件夹，此时最好切换至新建的hadoop用户登陆Ubuntu。 </p>
<p>2.  将下载的hadoop拷贝至该新建文件夹下：<br>注意此处命令的目录，这是别人的目录，你的下载目录并不一定就在/mnt/hgfs下。  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;mnt&#x2F;hgfs&#x2F;hadoop-1.2.1-bin.tar.gz &#x2F;home&#x2F;hadoop&#x2F;</span><br></pre></td></tr></table></figure>
<p>3.  进入该目录（cd /home/hadoop/）之后，解压该文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar xzf hadoop-  1.2.1-bin.tar.gz</span><br></pre></td></tr></table></figure>
<p>4.  进入hadoop-env.sh 所在目录（hadoop-1.2.1/conf/），对该文件进行如下内容的修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.6.0_07</span><br></pre></td></tr></table></figure>
<p>注：（/usr/java/jdk1.6.0_07 为 jdk安装目录）</p>
<p>5.  为了方便执行Hadoop命令，修改/etc/profiles,在最后面加上：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.6.0_07  </span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-1.2.1  </span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin  </span><br><span class="line">export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar:$JAVA_HOME&#x2F;lib&#x2F;htmlconverter.jar:$JAVA_HOME&#x2F;lib&#x2F;jconsole.jar:$JAVA_HOME&#x2F;lib&#x2F;sa-jdi.jar</span><br></pre></td></tr></table></figure>
<p>重新启动或者直接执行source etc/profile，使得/etc/profiles生效。<br>其实这时你的hadoop已经简单安装单机版。</p>
<h2 id="测试及文件配置"><a href="#测试及文件配置" class="headerlink" title="测试及文件配置"></a>测试及文件配置</h2><h3 id="单机版测试"><a href="#单机版测试" class="headerlink" title="单机版测试"></a>单机版测试</h3><p>hadoop默认是Standalone Operation。按照官方文档进行测试。<br>1、在  /home/hadoop  目录下建立HadoopStandaloneTest目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir  HadoopStandaloneTest</span><br></pre></td></tr></table></figure>
<p>2、在 /home/hadoop/HadoopStandaloneTest目录下执行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir input  </span><br><span class="line">cp $HADOOP_HOME&#x2F;conf&#x2F;*.xml input  </span><br><span class="line">hadoop jar $HADOOP_HOME&#x2F;hadoop-examples-1.2.1.jar grep input output &#39;dfs[a-z.]+&#39;</span><br></pre></td></tr></table></figure>
<p>命令解释： bin/hadoop jar（使用hadoop运行jar包） hadoop-*_examples.jar（jar包的名字） grep （要使用的类，后边的是参数） input output ‘dfs[a-z.]+’<br>就是运行hadoop示例程序中的grep，对应的hdfs上的输入目录为input、输出目录为output<br>3、<strong><em>运行成功标志</em></strong>就是在你的主目录下HadoopStandaloneTest有input，output两个文件夹，且output里会有两个文件，显示执行的结果。建议下个java得反编译的工具查看/hadoop-examples-1.2.1.jar的代码，更能理解这个栗子。</p>
<h3 id="伪分布配置、测试"><a href="#伪分布配置、测试" class="headerlink" title="伪分布配置、测试"></a>伪分布配置、测试</h3><h4 id="ssh-配置"><a href="#ssh-配置" class="headerlink" title="ssh 配置"></a>ssh 配置</h4><h5 id="启动ssh服务器、客户端"><a href="#启动ssh服务器、客户端" class="headerlink" title="启动ssh服务器、客户端"></a>启动ssh服务器、客户端</h5><p>首先查看ssh服务器和ssh客户端是否启动 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -e|grep ssh</span><br></pre></td></tr></table></figure>
<p>如看到如下二个进程则  OK<br><img src="25.png" alt=""><br>注！！！如果此处你的ssh-agent没有启动成功，执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">eval ‘ssh-agent’</span><br></pre></td></tr></table></figure>
<h5 id="实现ssh免密码登录"><a href="#实现ssh免密码登录" class="headerlink" title="实现ssh免密码登录"></a>实现ssh免密码登录</h5><p>测试可否使用ssh登陆localhost<br>$ ssh localhost<br>如果发现需要输入密码则执行一下步骤，否则跳过ssh免密码登陆  </p>
<p>假设A为客户机器，B为目标机；<br>要达到的目的：  A机器ssh登录B机器无需输入密码；<br>加密方式选rsa|dsa均可以，默认dsa<br>做法：<br>1、登录A机器<br>2、ssh-keygen -t [rsa|dsa]，将会生成密钥文件和私钥文件id_rsa, id_rsa.pub或id_dsa, id_dsa.pub<br>3、将.pub文件复制到B机器的.ssh目录, cat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>4、大功告成，从A机器登录B机器的目标账户，不再需要密码了；<br>$ ssh-keygen -t dsa –P ‘’ -f ~/.ssh/id_dsa  (先不要着急执行，看下面)<br>其中  </p>
<ul>
<li>-t dsa  指定密码算法为  dsa   </li>
<li>-P ‘’  指不需要  passphrase </li>
<li>-f ~/.ssh/id_dsa  指定秘钥输出文件 </li>
</ul>
<p>其实在真正执行时，上面的那句命令会说太长无法执行，所以其实只输入前面“$ ssh-keygen -t dsa”即可，输入之后会有其他的提示，提示你输入密码时，<strong><em>直接回车就代表密码回空</em></strong>，具体如下：<br><img src="26.png" alt=""><br>之后可以看到  ~/.ssh  目录下多了二个文件：<br><img src="27.png" alt=""><br>将  ssh  公钥追加到  authorized_keys  后面，即可实现免密钥登陆。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat ~&#x2F;.ssh&#x2F;id_dsa.pub &gt;&gt; ~&#x2F;.ssh&#x2F;authorized_keys</span><br></pre></td></tr></table></figure>
<p>修改  ~/.ssh/authorized_keys的权限，要保证.ssh和authorized_keys都只有用户自己有写权限。否则验证无效。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ chmod 600 ~&#x2F;.ssh&#x2F;authorized_keys</span><br></pre></td></tr></table></figure>
<p>~/.ssh 目录的权限为700，因此不用修改。<br>再利用ssh登录，发现不再需要输入密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh localhost</span><br></pre></td></tr></table></figure>
<h4 id="文件配置"><a href="#文件配置" class="headerlink" title="文件配置"></a>文件配置</h4><p>在/home/hadoop目录下建立HadoopPseudoDistributeTest目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mkdir HadoopPseudoDistributeTest  </span><br><span class="line">$ cd HadoopPseudoDistributeTest&#x2F;  </span><br><span class="line">$ mkdir conf  </span><br><span class="line">$ cp $HADOOP_HOME&#x2F;conf&#x2F;* conf  （ 复制  conf  目录下的所有文件）</span><br></pre></td></tr></table></figure>
<p>编辑HadoopPseudoDistributeTest/conf/下的配置文件  </p>
<ul>
<li>core-site.xml:  用于配置Common组件的属性  </li>
<li>hdfs-site.xml:  用于配置HDFS的属性  </li>
<li>mapred-site.xml:  用于配置MapReduce的属性  </li>
<li>masters  指定master节点  </li>
<li>slaves  指定slave节点</li>
</ul>
<p>注：<em><strong>下面配置文件中有两次用到ip地址（ifconfig命令查询IP地址），一定记得改成你自己的ip地址。</strong></em><br>不要在HadoopPseudoDistributeTest创建以下目录tmpdir/hdfs/name tmpdir/hdfs/data【rm -rf 文件夹 命令用来删除非空文件夹】</p>
<p>core-site.xml:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--最好不要用localhsot，否则Eclipse插件会出问题 ip地址换成自己的--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.231.111:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--A base for other temporary directories(用来存储其他临时目录的根目录) --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/HadoopPseudoDistributeTest/tmpdir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>hdfs-site.xml  </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">！--指定namenode存储文件系统元数据的目录</span> <span class="attr">--</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/HadoopPseudoDistributeTest/tmpdir/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--指定datanode存储数据的目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/HadoopPseudoDistributeTest/tmpdir/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>mapred-site.xml   </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">!-ip地址记得换成自己的--</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.231.111:9001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>masters:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure>
<p>slave:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure>
<h4 id="运行、测试"><a href="#运行、测试" class="headerlink" title="运行、测试"></a>运行、测试</h4><p>1、格式化HDFS的namenode（管理元数据）创建一个空的文件系统【可以直接不执行这句话，在下面2中第二句命令后执行“hadoop namenode -format”】  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hadoop --config ~&#x2F;HadoopPseudoDistributeTest&#x2F;conf namenode  -format</span><br></pre></td></tr></table></figure>
<p>注意  --config  后面一定用绝对路径指定配置文件所在的路径<br>2、运行hadoop（切记：首先使用ssh登陆localhost  ）  </p>
<pre><code>ssh localhost
export HADOOP_CONF_DIR=~/HadoopPseudoDistributeTest/conf(这样后面就不用带—config选项)
$HADOOP_HOME/bin/start-all.sh</code></pre><p>3、打开浏览器<br>** NameNode ** - <a href="http://localhost:50070/" target="_blank" rel="noopener">http://localhost:50070/</a><br>** JobTracker ** -  <a href="http://localhost:50030/" target="_blank" rel="noopener">http://localhost:50030/</a> </p>
<p>4、运行jps命令看相应服务是否启动：</p>
<p><img src="28.png" alt=""></p>
<p>5、在伪分布式的模式下运行前面的例子<br>将本地文件系统目录conf拷贝到分布式文件系统的input下（为了执行例子）  </p>
<pre><code>cd /home/hadoop/HadoopPseudoDistributeTest
hadoop fs -put conf/*xml input</code></pre><p>查看分布式文件系统的内容</p>
<pre><code>hadoop fs -ls</code></pre><p>执行例子</p>
<pre><code>cd ~/HadoopPseudoDistributeTest/
hadoop jar $HADOOP_HOME/hadoop-examples-1.0.4.jar grep input output &apos;dfs[a-z.]+&apos;
hadoop fs –ls output   # 查看执行结果</code></pre><p>执行正确效果，在  HadoopPseudoDistributeTest  的目录下有input和output文件夹，output中存放两个文件：<br><img src="29.png" alt=""><br>6、停止daemon</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">stop-all.sh</span><br></pre></td></tr></table></figure>

<h3 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h3><h4 id="hadoop的基础知识"><a href="#hadoop的基础知识" class="headerlink" title="hadoop的基础知识"></a>hadoop的基础知识</h4><p>Master: NameNode、JobTracker，负责总管分布式数据、分解任务的执行<br>Slave: DataNode、TaskTracker，负责分布式数据存储、任务的执行</p>
<p>主从结构<br>主节点，只有一个namenode<br>从节点，有很多个datanodes<br>namenode: 接收用户操作请求；维护文件系统的目录结构；管理文件与block之间关系，block与datanode之间关系<br>datanode负责：存储文件；文件被分成block存储在磁盘上；为保证数据安全，文件会有多个副本<br>namenode会有一个离线备份：SecondaryNamenode</p>
<p>JobTracker负责： 接收客户提交的计算任务， 把计算任务分给TaskTrackers执行， 监控TaskTracker的执行情况<br>TaskTrackers负责：执行JobTracker分配的计算任务</p>
<h4 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h4><p>现在介绍集群分布之前我的环境： 我是把之前一直安装好的ubuntu虚拟机直接无联系的克隆出来两个ubuntu，也就是说我现在要用三个ubuntu。两个也是可以的。<br>要说明的如果不是克隆出来的，还需要实现几台机器之间ssh登录无密码登录。因为我自己没有做这个不能确保给你们是否正确，可以看看  <a href="http://blog.csdn.net/hguisu/article/details/7237395" target="_blank" rel="noopener"> 这篇文章的4部分</a><br>如果是为了确保也可以使用克隆的方法。之后尝试ssh 到另两台ubuntu上，不需密码即可，如果需要密码，请重新确保不需密码。</p>
<p>我现在有三台ubuntu，他们的分配：  </p>
<ul>
<li>node1：namenode 192.168.231.129  </li>
<li>node2：datanode 192.168.231.130  </li>
<li>node3：datanode 192.168.231.131</li>
</ul>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p>1、在主目录下创建文件夹HadoopClusterTest，并且如上面2.2.2.2中把conf拷贝到这个文件下。 </p>
<p>2、配置conf下的文件<br>core-site.xml ip是node1的。    </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> 		<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://192.168.231.129:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> 		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/HadoopClusterTest/tmpdir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>hdfs-site.xml:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/HadoopClusterTest/tmpdir/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/HadoopClusterTest/tmpdir/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>mapred-site.xml ip地址改成自己的！！  </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>192.168.231.129:9001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.child.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx512m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.tasktracker.map.tasks.maximum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.tasktracker.reduce.tasks.maximum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.reuse.jvm.num.tasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>master</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.231.129</span><br></pre></td></tr></table></figure>
<p>slaves</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.231.130</span><br><span class="line">192.168.231.131</span><br></pre></td></tr></table></figure>
<p>3、用scp命令把HadoopClusterTest拷贝到另外两台ubuntu上：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -r &#x2F;home&#x2F;hadoop&#x2F;HadoopClusterTest 192.168.231.130:&#x2F;home&#x2F;hadoop&#x2F;</span><br></pre></td></tr></table></figure>
<p><img src="30.png" alt=""><br>然后改变ip地址用相同命令拷到另一台里。这样三台电脑的配置就完成了。</p>
<h4 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h4><p>下面是它的使用方法（9是我后期稍微熟练之后的开启hadoop的方法）：  </p>
<p>4、在三个ubuntu都执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HADOOP_CONF_DIR&#x3D;~&#x2F;HadoopClusterTest&#x2F;conf</span><br></pre></td></tr></table></figure>
<p>确保三个ubuntu上HadoopClusterTest都没有的tmpdir文件夹然后执行下面的  </p>
<p>5、在node1上，即我的192.168.231.129上执行【执行之前确保node1，node2，node3上没有tmpdir文件夹，否则启动namenode和datanode会有问题】：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure>
<p>这时在HadoopClusterTest的目录会有tmpdir文件夹，但是node2和node3没有。</p>
<p>6、在node1上，即我的192.168.231.129上执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$HADOOP_HOME&#x2F;bin&#x2F;start-all.sh</span><br></pre></td></tr></table></figure>
<p>然后在node1上执行下面的：<br><img src="31.png" alt=""><br>7、在node1上执行jps查看启动服务：<br><img src="32.png" alt=""><br>在node2、node3上jps查看：<br><img src="33.png" alt=""><br>8、浏览NameNode和JobTracker的网络接口，它们的地址默认为(我的node1为192.168.231.129)：</p>
<p>NameNode - <a href="http://node1:50070/" target="_blank" rel="noopener">http://node1:50070/</a><br>JobTracker - <a href="http://node1:50030/" target="_blank" rel="noopener">http://node1:50030/</a></p>
<p>9、我自己后期开启集群方法：打开三个虚拟机（但其实下面的命令是只在一台虚拟机上执行的）</p>
<p>打开192.168.231.129的终端。依次输入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh 192.168.231.130</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;~&#x2F;HadoopClusterTest&#x2F;conf</span><br><span class="line">ssh 192.168.231.131</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;~&#x2F;HadoopClusterTest&#x2F;conf</span><br><span class="line">ssh 192.168.231.129</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;~&#x2F;HadoopCluterTest&#x2F;conf</span><br><span class="line">$HADOOP_HOME&#x2F;bin&#x2F;start-all.sh</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>ignore-resource-not-found和ignore-unresolvable</title>
    <url>/2018/04/08/ignore-resource-not-found%E5%92%8Cignore-unresolvable/</url>
    <content><![CDATA[<p>.ignore-resource-not-found和ignore-unresolvable两个属性是类似的作用（网上说推荐配对使用，但很少看到配对使用的）</p>
<p>如果location中的文件指向了一个不存在的文件（在没有指定上面两个参数的情况下，spring也并不会报错），那么也极有可能意味着有属性无法解析（虽然存在其他属性文件中存在重名，但是这个是应该避免的，  所以当ignore-resource-not-found设为true时，ignore-unresolvable也必须设为true。）<br>                       -————————————来自网络<br>其实当ignore-unresolvable设为true时，ignore-resource-not-found的值true或false，并不影响异常的抛出。<br>如果设置为ture，后属性值无法解析成功，将赋值为${属性名}</p>
<p>** ignore-resource-not-found： ** 如果属性文件找不到，是否忽略，默认false，即不忽略，找不到文件并不会抛出异常。<br>** ignore-unresolvable： ** 是否忽略解析不到的属性，如果不忽略，找不到将抛出异常。但它设置为true的主要原因是： </p>
<p>** 理解：ignore-unresolvable为true时，配置文件${}找不到对应占位符的值 不会报错，会直接赋值’${}’；如果设为false，会直接报错。<br>设置它为true的主要原因，是一个xml中有多个配置文件时的情况： **<br>同个模块中如果出现多个context:property-placeholder ，location properties文件后，运行时出现<br>_ *<em>Could not resolve placeholder ‘key’ in string value${key1} *</em> _<br>原因是在加载第一个context:property-placeholder时会扫描所有的bean，而有的bean里面出现第二个 context:property-placeholder引入的properties的占位符${key2}，但此时还没有加载第二个property-placeholder，所以解析不了${key2}。</p>
<p>办法一，可以将通过模块的多个property-placeholder合并为一个，将初始化放在一起(不推荐)。<br>方法二，添加ignore-unresolvable=”true”，这样可以在加载第一个property-placeholder时出现解析不了的占位符进行忽略掉。</p>
<p>参考：  <a href="http://blog.csdn.net/Rickesy/article/details/50791534" target="_blank" rel="noopener"> http://blog.csdn.net/Rickesy/article/details/50791534
</a> (比较多乱)  </p>
]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>ignore-unresolvable</tag>
        <tag>ignore-resource-not-found</tag>
      </tags>
  </entry>
  <entry>
    <title>python+caffe做图片处理可能会用到的方法</title>
    <url>/2018/04/08/python+caffe%E5%81%9A%E5%9B%BE%E7%89%87%E5%A4%84%E7%90%86%E5%8F%AF%E8%83%BD%E4%BC%9A%E7%94%A8%E5%88%B0%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>另外参考我的 <a href="/2018/04/08/python图片处理Image和skimage的不同"> 这篇 </a> 和 <a href="/2018/04/08/python的Image和skimage处理图片"> 这篇
</a> ，了解更多的图片处理。</p>
<h3 id="图片和numpy数据的转化"><a href="#图片和numpy数据的转化" class="headerlink" title="图片和numpy数据的转化"></a>图片和numpy数据的转化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以实现图片和numpy数据的转化。</span></span><br><span class="line">numpy.array(image)和Image.fromarray(np_data)</span><br></pre></td></tr></table></figure>
<h3 id="随机"><a href="#随机" class="headerlink" title="随机"></a>随机</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#start到end间随机数。start=&lt;num&lt;=end</span></span><br><span class="line">random.randint(start, end)</span><br><span class="line"><span class="comment">#list以行随机打乱，用于存入数据库时的txt根据（即，常常看到的train.txt）</span></span><br><span class="line">random.shuffle(list)</span><br><span class="line"><span class="comment">#numpy；产生一个size大小的随机numpy，数值在start和end之间</span></span><br><span class="line">numpy.random.randint(start, end, size=(height,width))</span><br></pre></td></tr></table></figure>
<h3 id="矩阵相乘"><a href="#矩阵相乘" class="headerlink" title="矩阵相乘"></a>矩阵相乘</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">numpy.dot(numpyA, numpyB) <span class="comment">#两个矩阵相乘</span></span><br></pre></td></tr></table></figure>
<p>不断更新中，总是忘，目前就能记起这些了。  </p>
]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>图片处理</tag>
      </tags>
  </entry>
  <entry>
    <title>python 二进制、十六进制、ascii码互转</title>
    <url>/2018/04/08/python%20%E4%BA%8C%E8%BF%9B%E5%88%B6%E3%80%81%E5%8D%81%E5%85%AD%E8%BF%9B%E5%88%B6%E3%80%81ascii%E7%A0%81%E4%BA%92%E8%BD%AC/</url>
    <content><![CDATA[<p>1.bin(数字)十进制-》二进制，有0b，用replace(‘0b’,’’)</p>
<pre><code>&gt;&gt;&gt; print bin(97).replace(&apos;0b&apos;,&apos;&apos;)
1100001</code></pre><p>2.int(浮点型数字) float-》int  </p>
<pre><code>&gt;&gt;&gt; print int(12.3)
12</code></pre><p>3.chr(数字a)a在0~255之间。int-》ascii码（即只有8位）  </p>
<pre><code>&gt;&gt;&gt; print chr(97)
a</code></pre><p>4.ard(字符a)ascii码-》int。3的反向  </p>
<pre><code>&gt;&gt;&gt; print ord(&apos;a&apos;)
97</code></pre><p>5.hex(数字a)十进制-》十六进制  </p>
<pre><code>&gt;&gt;&gt; print hex(10)
0xa</code></pre><p>6.binascii.b2a_hex(字符串)字符串-》十六进制<br>7.binascii.a2b_hex(十六进制数)十六进制-》字符串。6的反向  </p>
<pre><code>&gt;&gt;&gt; import binascii as ba
&gt;&gt;&gt; print ba.b2a_hex(&apos;nihao&apos;)
6e6968616f
&gt;&gt;&gt; import binascii as ba
&gt;&gt;&gt; b =  ba.b2a_hex(&apos;nihao&apos;)
&gt;&gt;&gt; a = ba.a2b_hex(b)
&gt;&gt;&gt; print b
6e6968616f
&gt;&gt;&gt; print a
nihao</code></pre><p>6,7我不常用，其他常用。  </p>
]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>图片处理</tag>
        <tag>python进制转换</tag>
      </tags>
  </entry>
  <entry>
    <title>python图片处理Image和skimage的不同</title>
    <url>/2018/04/08/python%E5%9B%BE%E7%89%87%E5%A4%84%E7%90%86Image%E5%92%8Cskimage%E7%9A%84%E4%B8%8D%E5%90%8C/</url>
    <content><![CDATA[<p>做cnn的难免要做大量的图片处理。由于接手项目时间不长，且是新项目，前段时间写代码都很赶，现在稍微总结（恩，总结是个好习惯）。</p>
<p>首先安装python-Image和python-skimage、python-matplotlib。</p>
<h4 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Image <span class="keyword">as</span> img</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plot</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io,transform</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    fig = plot.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">121</span>)</span><br><span class="line">    ax.imshow(data, cmap=<span class="string">'gray'</span>)</span><br><span class="line">    ax2 = fig.add_subplot(<span class="number">122</span>)</span><br><span class="line">    ax2.imshow(data)</span><br><span class="line">    plot.show()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    parse = argparse.ArgumentParser()</span><br><span class="line">    parse.add_argument(<span class="string">'--picpath'</span>, help = <span class="string">"the picture' path"</span>)</span><br><span class="line">    args = parse.parse_args()</span><br><span class="line">    img_file1 = img.open(args.picpath)<span class="comment">#Image读图片</span></span><br><span class="line">    one_pixel = img_file1.getpixel((<span class="number">0</span>,<span class="number">0</span>))[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"picture's first pixe: "</span>,one_pixel  </span><br><span class="line">    <span class="keyword">print</span> <span class="string">"the picture's size: "</span>, img_file1.size<span class="comment">#Image读出来的size是高宽</span></span><br><span class="line">    show_data(img_file1)</span><br><span class="line">    img_file2 = io.imread(args.picpath)<span class="comment">#skimage读图片</span></span><br><span class="line">    show_data(img_file2)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"picture's first pixel: "</span>, img_file2[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"the picture's shape: "</span>, img_file2.shape<span class="comment">#skimage读出来的shape是高，宽， 通道</span></span><br></pre></td></tr></table></figure>
<p>调用及输出：</p>
<p><img src="1.png" alt="">  </p>
<p>Image读出来的是PIL什么的类型，<br>skimage.io读出来的数据是numpy的格式。<br>想看Image和skimage读出来图片的区别，可以直接输出它们读图片以后的返回结果。  </p>
<h4 id="Image和skimage读图片"><a href="#Image和skimage读图片" class="headerlink" title="Image和skimage读图片"></a>Image和skimage读图片</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_file1 = img.open(args.picpath)</span><br><span class="line">img_file2 = io.imread(args.picpath)</span><br></pre></td></tr></table></figure>
<h4 id="读图片后数据的大小"><a href="#读图片后数据的大小" class="headerlink" title="读图片后数据的大小"></a>读图片后数据的大小</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"the picture's size: "</span>, img_file1.size</span><br><span class="line"><span class="keyword">print</span> <span class="string">"the picture's shape: "</span>, img_file2.shape</span><br></pre></td></tr></table></figure>
<p>从输出可以看出<br>img读图片的大小是图片的（height,width）；<br>skimage的是(height,width, channel)<br>[这也是为什么caffe在单独测试时要要在代码中设置：transformer.set_transpose  (‘data’,  (  2, 0, 1))，因为caffe可以处理的图片的数据格式是(channel,height,width)，所以要转换数据啊]</p>
<h4 id="得到像素"><a href="#得到像素" class="headerlink" title="得到像素"></a>得到像素</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">one_pixel = img_file1.getpixel((<span class="number">0</span>,<span class="number">0</span>))[<span class="number">0</span>]</span><br><span class="line">img_file2[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>img读出来的图片获得某点像素用getpixel((h,w))可以直接返回这个点三个通道的像素值<br>skimage读出来的图片直接img_file2[0][0][0]获得某点某通道像素值，但是一定记住它的格式，并不是你想的(channel,height,width)</p>
<p>关于matplotlib简单的画图请关注下篇～  </p>
]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>图片处理</tag>
        <tag>skimage</tag>
      </tags>
  </entry>
  <entry>
    <title>python数据处理之列表、集合、字典推导式</title>
    <url>/2018/04/08/python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B9%8B%E5%88%97%E8%A1%A8%E3%80%81%E9%9B%86%E5%90%88%E3%80%81%E5%AD%97%E5%85%B8%E6%8E%A8%E5%AF%BC%E5%BC%8F/</url>
    <content><![CDATA[<h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><p>_ [expr for item in collection if condition] _</p>
<p>举例：</p>
<pre><code>&gt;&gt;&gt; result = []
&gt;&gt;&gt; [result.append(item) for item in fruit if len(item) &gt; 5]
[None, None]
&gt;&gt;&gt; result
[&apos;banana&apos;, &apos;orange&apos;]</code></pre><p>效果与下面类似：  </p>
<pre><code>&gt;&gt;&gt; result = []
&gt;&gt;&gt; fruit = [&apos;apple&apos;, &apos;banana&apos;, &apos;orange&apos;]
&gt;&gt;&gt; for item in fruit:
...     if len(item)&gt;5:
...         result.append(item)
... 
&gt;&gt;&gt; result
[&apos;banana&apos;, &apos;orange&apos;]</code></pre><h3 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h3><p>_ (expr for item in collection if condition) _ 与列表只有外面括号的差别。</p>
<p>！！！多谢下面的指出，集合外面的括号应该是大括号{}，即{ _ expr for item in collection if condition _ }</p>
<h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><p>_ {key : value for item in collectio if condition} _</p>
<p>例子：  </p>
<pre><code>&gt;&gt;&gt; fruit = [&apos;apple&apos;, &apos;banana&apos;, &apos;orange&apos;]
&gt;&gt;&gt; dictresult = {}
&gt;&gt;&gt; dictresult = {key: value for key, value in enumerate(fruit) if len(value) &gt; 5}
&gt;&gt;&gt; dictresult
{1: &apos;banana&apos;, 2: &apos;orange&apos;}</code></pre><p>相同效果：</p>
<pre><code>&gt;&gt;&gt; fruit = [&apos;apple&apos;, &apos;banana&apos;, &apos;orange&apos;]
&gt;&gt;&gt; dictresult = {}
&gt;&gt;&gt; for key, value in enumerate(fruit):
...     if len(value) &gt; 5:
...         dictresult[key] = value
... 
&gt;&gt;&gt; dictresult
{1: &apos;banana&apos;, 2: &apos;orange&apos;}</code></pre>]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>图片处理</tag>
      </tags>
  </entry>
  <entry>
    <title>python数据分析之numpy初始化</title>
    <url>/2018/04/08/python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8Bnumpy%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>假设 ：  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<p>numpy是同构数据多维容器，同构即数据类型相同  </p>
<h3 id="np-arange"><a href="#np-arange" class="headerlink" title="np.arange"></a>np.arange</h3><p>np.arange([start,] end [, step])  #与list的range相似</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.arange(<span class="number">10</span>)</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.arange(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br></pre></td></tr></table></figure>

<h3 id="np-zeros"><a href="#np-zeros" class="headerlink" title="np.zeros"></a>np.zeros</h3><p>np.zeros(tupleA)  #产生一个tupleA维度大小的矩阵，且初始全为0  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.zeros((<span class="number">4</span>))</span><br><span class="line">array([ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.zeros((<span class="number">4</span>,<span class="number">2</span>))</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="np-ones"><a href="#np-ones" class="headerlink" title="np.ones"></a>np.ones</h3><p>np.ones(tupleA)  #与上面类似，只是初始化全为1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.ones((<span class="number">4</span>))</span><br><span class="line">array([ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.ones((<span class="number">4</span>,<span class="number">2</span>))</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="np-empty"><a href="#np-empty" class="headerlink" title="np.empty"></a>np.empty</h3><p>np.empty(tupleA)  #与上面类似，只是初始化值是不确定的（并不是你以为的0！！！！）  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.empty((<span class="number">4</span>))</span><br><span class="line">array([  <span class="number">1.73154357e-316</span>,   <span class="number">4.71627160e-317</span>,   <span class="number">0.00000000e+000</span>,</span><br><span class="line">         <span class="number">4.94065646e-324</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.empty((<span class="number">3</span>,<span class="number">2</span>))</span><br><span class="line">array([[  <span class="number">0.00000000e+000</span>,   <span class="number">0.00000000e+000</span>],</span><br><span class="line">       [  <span class="number">6.94647584e-310</span>,   <span class="number">6.94647586e-310</span>],</span><br><span class="line">       [  <span class="number">6.94647586e-310</span>,   <span class="number">6.94647586e-310</span>],</span><br></pre></td></tr></table></figure>
<h3 id="np-array"><a href="#np-array" class="headerlink" title="np.array"></a>np.array</h3><p>np.array(listA)  #把listA转成np，listA只是一个统称，只要是序列化的都可以，还可以是其他np  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>npA = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>npA</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>npB = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2.0</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>npB</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">       [ <span class="number">4.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>]])</span><br></pre></td></tr></table></figure>
<p>np.array会自动找到最适合listA数据类型转给np:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>npA.dtype</span><br><span class="line">dtype(<span class="string">'int64'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>npB.dtype</span><br><span class="line">dtype(<span class="string">'float64'</span>)</span><br></pre></td></tr></table></figure>
<p>但其实，np初始化时没有特别说明都会被默认是float64，如前四种</p>
<h3 id="ones-like-npA-zeros-like-npA-empty-like-npA"><a href="#ones-like-npA-zeros-like-npA-empty-like-npA" class="headerlink" title="ones_like(npA);zeros_like(npA);empty_like(npA)"></a>ones_like(npA);zeros_like(npA);empty_like(npA)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>npB = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2.0</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.ones_like(npB)</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.zeros_like(npB)</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.empty_like(npB)</span><br><span class="line">array([[  <span class="number">0.00000000e+000</span>,   <span class="number">0.00000000e+000</span>,   <span class="number">1.56491143e-316</span>],</span><br><span class="line">       [  <span class="number">6.94647850e-310</span>,   <span class="number">6.94635322e-310</span>,   <span class="number">1.72361006e-316</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.identity(<span class="number">3</span>)</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.eye(<span class="number">3</span>, k = <span class="number">-1</span>)<span class="comment">#变化k的值试试看</span></span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>numpy</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title>spring 自定义注解实现登陆拦截</title>
    <url>/2018/04/08/spring%20%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%E5%AE%9E%E7%8E%B0%E7%99%BB%E9%99%86%E6%8B%A6%E6%88%AA/</url>
    <content><![CDATA[<p>需求：<br>自定义一个注解，使得controller层的类或者方法在写上这个注解后，会有登陆验证。  </p>
<p>实现原理：<br>（1）先写一个自定义注解，注解上可以通过注释确定是类/方法可以加此注解。<br>（2）之后，写一个拦截器，拦截器内可以通过handler得到被拦截器拦截的类或者方法，之后可以通过这个类/方法得知它是否有之前写的注解，如果有，就需要登陆校<br>验。<br>（3）之后要把这个拦截器配置到spring-mvc的配置文件中，需要让spring知道有哪些请求需要被拦截器拦截，一般是“/**”，是所有请求。<br>（4）在controller的类或者方法上加上步骤一中的的自定义注解就可以轻易实现是否需要登陆校验了。  </p>
<p>例子：  </p>
<h5 id="1-自定义注解"><a href="#1-自定义注解" class="headerlink" title="1.自定义注解"></a>1.自定义注解</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.fresh.annotation;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.ElementType;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.Retention;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.RetentionPolicy;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.Target;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* LoginRequired</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment">* <span class="doctag">@date</span> 17-10-11 下午8:30</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@Target</span>(&#123;ElementType.METHOD, ElementType.TYPE&#125;)</span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> LoginRequired &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>@Target({ElementType.METHOD, ElementType.TYPE}) 表示类和方法都可以加此注解<br>Retention(保留)注解说明,这种类型的注解会被保留到那个阶段. 有三个值:<br>1.RetentionPolicy.SOURCE —— 这种类型的Annotations只在源代码级别保留,编译时就会被忽略<br>2.RetentionPolicy.CLASS —— 这种类型的Annotations编译时被保留,在class文件中存在,但JVM将会忽略<br>3.RetentionPolicy.RUNTIME —— 这种类型的Annotations将被JVM保留,所以他们能在运行时被JVM或其他使用反射机制的代码所读取和使用.<br>还可以再写上@Documented注解：<br> Documented 注解表明这个注解应该被 javadoc工具记录. 默认情况下,javadoc是不包括注解的. 但如果声明注解时指定了 @Documented,则它会被 javadoc 之类的工具处理, 所以注解类型信息也会被包括在生成的文档中. </p>
<p>其他相关注解：<a href="https://www.jb51.net/article/55371.htm" target="_blank" rel="noopener">https://www.jb51.net/article/55371.htm</a></p>
<h5 id="2-写一个拦截器"><a href="#2-写一个拦截器" class="headerlink" title="2.写一个拦截器"></a>2.写一个拦截器</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.fresh.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.fresh.annotation.LoginRequired;</span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.method.HandlerMethod;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.servlet.handler.HandlerInterceptorAdapter;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.servlet.http.HttpServletRequest;</span><br><span class="line"><span class="keyword">import</span> javax.servlet.http.HttpServletResponse;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * AuthInterceptor</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 17-10-11 下午8:38</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AuthInterceptor</span> <span class="keyword">extends</span> <span class="title">HandlerInterceptorAdapter</span></span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="meta">@Override</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">preHandle</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object handler)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="comment">// return super.preHandle(request, response, handler);</span></span><br><span class="line"> 	<span class="keyword">if</span>(handler <span class="keyword">instanceof</span> HandlerMethod)&#123;</span><br><span class="line"> 		HandlerMethod myHandlerMethod = (HandlerMethod) handler;</span><br><span class="line"> 		Object bean = myHandlerMethod.getBean();</span><br><span class="line"> 		Method method = myHandlerMethod.getMethod();</span><br><span class="line"> 		LoginRequired classAnnotation = bean.getClass().getAnnotation(LoginRequired<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> 		LoginRequired methodAnnotation = method.getAnnotation(LoginRequired<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"> 		<span class="keyword">if</span>(classAnnotation != <span class="keyword">null</span> || methodAnnotation != <span class="keyword">null</span>)&#123;</span><br><span class="line"> 			<span class="keyword">boolean</span> loginState = isLogin(request, response);</span><br><span class="line"> 			<span class="keyword">if</span>(loginState)&#123;</span><br><span class="line"> 				<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"> 			&#125;</span><br><span class="line"> 			<span class="keyword">if</span>(isAjax(request))&#123;</span><br><span class="line"> 				<span class="comment">//ajax 请求需要返回json</span></span><br><span class="line"> 				log.info(<span class="string">"ajax 请求，没有登陆"</span>);</span><br><span class="line"> 			&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"> 				log.info(<span class="string">"普通请求，没有登陆"</span>);</span><br><span class="line"> 			&#125;</span><br><span class="line"> 			<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"> 		&#125;</span><br><span class="line"> 	&#125;</span><br><span class="line"> 	<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isLogin</span><span class="params">(HttpServletRequest request, HttpServletResponse response)</span></span>&#123;</span><br><span class="line"> 	<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isAjax</span><span class="params">(HttpServletRequest request)</span></span>&#123;</span><br><span class="line"> 	String requestType = request.getHeader(<span class="string">"X-Requested-With"</span>);</span><br><span class="line"> 	<span class="keyword">if</span>(requestType != <span class="keyword">null</span> &amp;&amp; requestType.equals(<span class="string">"XMLHttpRequest"</span>))&#123;</span><br><span class="line"> 		<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"> 	&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"> 		<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"> 	&#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在isLogin方法中写上是否登陆的验证。  </p>
<p>原理是先通过myHandlerMethod.getBean()得到bean，<br>再通过bean.getClass().getAnnotation(LoginRequired.class)的返回值确定是否有LoginRequired（即步骤1）中的自定义注解。<br>通过myHandlerMethod.getMethod()也可以得到method。<br>通过method.getAnnotation(LoginRequired.class)得到此方法是否有自定义注解。  </p>
<p>也可以通过下面的方法得到是否有LoginRequired 自定义注解。  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">HandlerMethod handlerMethod = (HandlerMethod) handler;</span><br><span class="line"><span class="keyword">if</span> (!handlerMethod.getBeanType().isAnnotationPresent(LoginRequired<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class"> &amp;&amp; !<span class="title">handlerMethod</span>.<span class="title">getMethod</span>().<span class="title">isAnnotationPresent</span>(<span class="title">LoginRequired</span>.<span class="title">class</span>)) </span>&#123;</span><br><span class="line"> 	<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br></pre></td></tr></table></figure>
<h5 id="3-配置springmvc文件"><a href="#3-配置springmvc文件" class="headerlink" title="3.配置springmvc文件"></a>3.配置springmvc文件</h5><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mvc:interceptors</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">mvc:interceptor</span>&gt;</span></span><br><span class="line"> 		<span class="tag">&lt;<span class="name">mvc:mapping</span> <span class="attr">path</span>=<span class="string">"/**"</span>/&gt;</span></span><br><span class="line"> 		<span class="tag">&lt;<span class="name">ref</span> <span class="attr">bean</span>=<span class="string">"authInterceptor"</span>/&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;/<span class="name">mvc:interceptor</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mvc:interceptors</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="4-使用例子"><a href="#4-使用例子" class="headerlink" title="4.使用例子"></a>4.使用例子</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.fresh.controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.fresh.annotation.LoginRequired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Controller;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TestController</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 17-10-11 下午9:05</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@LoginRequired</span></span><br><span class="line"><span class="meta">@Controller</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/test"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="meta">@RequestMapping</span>(<span class="string">"/one"</span>)</span><br><span class="line"> <span class="function"><span class="keyword">public</span> String <span class="title">test</span><span class="params">()</span></span>&#123;</span><br><span class="line"> 	<span class="keyword">return</span> <span class="string">"test"</span>;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样，访问以/test开头的所有url都会有是否登陆的验证。</p>
]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring 自定义注解</tag>
        <tag>登陆验证</tag>
      </tags>
  </entry>
  <entry>
    <title>spring 自定义注解实现日志统一处理</title>
    <url>/2018/04/08/spring%20%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97%E7%BB%9F%E4%B8%80%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p>需求：<br>通过注解的方式 统一处理controller和service的日志（实现上可能不太严谨，主要是实现流程）</p>
<p>原理：<br>先自定义注解。用aop切面拦截方法的使用，看是否有对应的自定义的注解，如果有，在切面中进行日志的统一打印，可以获取到加了注解方法的类名、方法名、参数。</p>
<p>如果想每个方法传进来不同信息，可以在自定义注解里写上参数，这样在使用时就可以带进来不同信息。例如，spring自带的注解@Resource（name=“”）。</p>
<h5 id="1-controller、service自定义注解"><a href="#1-controller、service自定义注解" class="headerlink" title="1.controller、service自定义注解"></a>1.controller、service自定义注解</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.fresh.annotation;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.ElementType;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.Retention;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.RetentionPolicy;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.Target;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ControllerLog</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 17-10-12 上午11:35</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Target</span>(&#123;ElementType.METHOD, ElementType.TYPE&#125;)</span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> ControllerLog &#123;</span><br><span class="line">    <span class="function">String <span class="title">description</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.fresh.annotation;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.ElementType;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.Retention;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.RetentionPolicy;</span><br><span class="line"><span class="keyword">import</span> java.lang.annotation.Target;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ServiceLog</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 17-10-12 上午11:34</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Target</span>(&#123;ElementType.METHOD, ElementType.TYPE&#125;)</span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> ServiceLog &#123;</span><br><span class="line">    <span class="function">String <span class="title">description</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以通过需求加多个如同description的注解。  </p>
<h5 id="2-写aop进行切面"><a href="#2-写aop进行切面" class="headerlink" title="2.写aop进行切面"></a>2.写aop进行切面</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.fresh.aop;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> com.fresh.annotation.ControllerLog;</span><br><span class="line"><span class="keyword">import</span> com.fresh.annotation.ServiceLog;</span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.text.StrBuilder;</span><br><span class="line"><span class="keyword">import</span> org.aspectj.lang.JoinPoint;</span><br><span class="line"><span class="keyword">import</span> org.aspectj.lang.ProceedingJoinPoint;</span><br><span class="line"><span class="keyword">import</span> org.aspectj.lang.annotation.*;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * LogAop</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 17-10-12 上午11:40</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@Aspect</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogAop</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line"> <span class="meta">@Pointcut</span>(<span class="string">"@annotation(com.fresh.annotation.ServiceLog)"</span>)</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serviceAspect</span><span class="params">()</span></span>&#123;</span><br><span class="line">    log.info(<span class="string">"service 日志"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="meta">@Pointcut</span>(<span class="string">"@annotation(com.fresh.annotation.ControllerLog)"</span>) <span class="comment">//</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">controllerAspect</span><span class="params">()</span></span>&#123;</span><br><span class="line">    log.info(<span class="string">"controller 日志"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="meta">@Before</span>(<span class="string">"controllerAspect() || serviceAspect()"</span>)</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doBefore</span><span class="params">(JoinPoint joinPoint)</span></span>&#123;</span><br><span class="line">    String className = joinPoint.getTarget().getClass().getName();</span><br><span class="line">    String methodName = joinPoint.getSignature().getName();</span><br><span class="line">    String description = getDescription(joinPoint);</span><br><span class="line">    Object[] args = joinPoint.getArgs();</span><br><span class="line">    String logResult = <span class="keyword">new</span> StringBuilder().append(description).append(<span class="string">"操作: "</span>)</span><br><span class="line">    .append(className).append(<span class="string">"类的"</span>).append(methodName).append(<span class="string">"()方法被请求，参数为： "</span>)</span><br><span class="line">    .append(Arrays.toString(args)).toString();</span><br><span class="line"> </span><br><span class="line">    log.info(logResult);</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// @AfterThrowing(pointcut = "controllerAspect() || serviceAspect()", throwing = "e")</span></span><br><span class="line"><span class="comment">// public void doAfterThrowing(JoinPoint joinPoint, Throwable e)&#123;</span></span><br><span class="line"><span class="comment">//      log.error("统一处理异常日志 &#123;&#125;", e);</span></span><br><span class="line"><span class="comment">// &#125;</span></span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">private</span> String <span class="title">getDescription</span><span class="params">(JoinPoint joinPoint)</span></span>&#123;</span><br><span class="line">    Class&lt;?&gt; className = joinPoint.getTarget().getClass();</span><br><span class="line">    <span class="comment">//得到类的所有方法</span></span><br><span class="line">    Method[] methods = className.getMethods();</span><br><span class="line">    String methodName = joinPoint.getSignature().getName();</span><br><span class="line">    Object[] args = joinPoint.getArgs();</span><br><span class="line">    <span class="comment">//在类所有方法中找到对应的被拦截到的方法，通过方法的名字和参数的个数。</span></span><br><span class="line">    <span class="keyword">for</span>(Method method: methods)&#123;</span><br><span class="line">        <span class="keyword">if</span>(method.getName().equals(methodName))&#123;</span><br><span class="line">            Class&lt;?&gt;[] parameterTypes = method.getParameterTypes();</span><br><span class="line">            <span class="comment">//参数个数是否相等。</span></span><br><span class="line">            <span class="keyword">if</span>(parameterTypes.length == args.length)&#123;</span><br><span class="line">                ControllerLog controllerLog = method.getAnnotation(ControllerLog<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">                <span class="keyword">if</span>(controllerLog != <span class="keyword">null</span>)&#123;</span><br><span class="line">                    <span class="keyword">return</span> controllerLog.description();</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    ServiceLog serviceLog = method.getAnnotation(ServiceLog<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">                    <span class="keyword">return</span> serviceLog != <span class="keyword">null</span>? serviceLog.description(): <span class="string">""</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>只写了before，如果有需求还可以写上after等。  </p>
<h5 id="3-xml配置"><a href="#3-xml配置" class="headerlink" title="3.xml配置"></a>3.xml配置</h5><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">aop:aspectj-autoproxy</span> <span class="attr">proxy-target-class</span>=<span class="string">"true"</span>/&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>_需要注意的是，如果项目的spring-mvc.xml和applicationContext.xml是两个文件，而你的切面是同时面向controller和service的，两个配置文件里都需要加上这行配置。</strong>_</p>
<h5 id="4-使用"><a href="#4-使用" class="headerlink" title="4.使用"></a>4.使用</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.fresh.controller;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> com.fresh.annotation.ControllerLog;</span><br><span class="line"><span class="keyword">import</span> com.fresh.annotation.LoginRequired;</span><br><span class="line"><span class="keyword">import</span> com.fresh.service.TestService;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Controller;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.PathVariable;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> javax.annotation.Resource;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TestController</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 17-10-11 下午9:05</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//@LoginRequired</span></span><br><span class="line"><span class="meta">@Controller</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/test"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestController</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line"> <span class="meta">@Resource</span></span><br><span class="line"> TestService testService;</span><br><span class="line"> </span><br><span class="line"> <span class="meta">@RequestMapping</span>(<span class="string">"/one/&#123;name&#125;"</span>)</span><br><span class="line"> <span class="meta">@ControllerLog</span>(description=<span class="string">"测试用aop统一处理日志"</span>)</span><br><span class="line"> <span class="function"><span class="keyword">public</span> String <span class="title">test</span><span class="params">(@PathVariable String name)</span></span>&#123;</span><br><span class="line">    testService.printHello(name);</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"test"</span>;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.fresh.service;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TestService</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 17-10-12 下午6:32</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TestService</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">void</span> <span class="title">printHello</span><span class="params">( String name)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">package</span> com.fresh.service.impl;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> com.fresh.annotation.ServiceLog;</span><br><span class="line"><span class="keyword">import</span> com.fresh.service.TestService;</span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Service;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TestServiceImpl</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 17-10-12 下午6:33</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestServiceImpl</span> <span class="keyword">implements</span> <span class="title">TestService</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line"> <span class="meta">@Override</span></span><br><span class="line"> <span class="meta">@ServiceLog</span>(description = <span class="string">"测试aop统一处理service日志输出"</span>)</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">printHello</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">"hello &#123;&#125;"</span>, name);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样，当浏览器输入：  <a href="http://localhost:8080/test/one/who" target="_blank" rel="noopener"> http://localhost:8080/test/one/who
</a> 时，会有如下输出日志：  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO com.fresh.aop.LogAop - 测试用aop统一处理日志操作: com.fresh.controller.TestController类的test()方法被请求，参数为： [who]</span><br><span class="line">INFO com.fresh.aop.LogAop - 测试aop统一处理service日志输出操作: com.fresh.service.impl.TestServiceImpl类的printHello()方法被请求，参数为： [who]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring 自定义注解</tag>
        <tag>日志统一处理</tag>
      </tags>
  </entry>
  <entry>
    <title>spring用Scheduled注解方式实现定时任务</title>
    <url>/2018/04/08/spring%E7%94%A8Scheduled%E6%B3%A8%E8%A7%A3%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h5 id="1-spring配置文件"><a href="#1-spring配置文件" class="headerlink" title="1.spring配置文件"></a>1.spring配置文件</h5><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">!-spring扫描注解包的配置--</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">context:component-scan</span> <span class="attr">base-package</span>=<span class="string">"XXX"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">!—开启这个配置，spring才能识别@Scheduled注解</span> <span class="attr">--</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">task:annotation-driven</span>/&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="2-定时任务的类"><a href="#2-定时任务的类" class="headerlink" title="2.定时任务的类"></a>2.定时任务的类</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* TestQuartz</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* <span class="doctag">@author</span> chenliclchen</span></span><br><span class="line"><span class="comment">* <span class="doctag">@date</span> 17-11-2 下午5:19</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestQuartz</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="meta">@Scheduled</span>(cron = <span class="string">"*/1 0 0 * * ?"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">everyMinute</span><span class="params">()</span></span>&#123;</span><br><span class="line">        log.info(<span class="string">"everyMinute"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Scheduled</span>(cron = <span class="string">"0 0 19 * * ?"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">hours</span><span class="params">()</span></span>&#123;</span><br><span class="line">        log.info(<span class="string">"hours"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样项目启动之后，每分钟会输出“everyMinute”，每天19点输出“hours”。  </p>
]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>定时任务</tag>
        <tag>spring 自定义注解</tag>
        <tag>Scheduled</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu 64位安装wps（亲测可用）</title>
    <url>/2018/04/08/ubuntu%2064%E4%BD%8D%E5%AE%89%E8%A3%85wps%EF%BC%88%E4%BA%B2%E6%B5%8B%E5%8F%AF%E7%94%A8%EF%BC%89/</url>
    <content><![CDATA[<p>ubuntu 14.04 64位安装wps<br>1.在<a href="http://community.wps.cn/download/" target="_blank" rel="noopener">官网</a>下载.我的是wps-office_9.1.0.4751<del>a15_i386.deb<br>2.切换到下载目录<br>3.sudo apt-get install ia32-libs*<br>4.sudo dpkg -i –force-architecture  wps-office_9.1.0.4751</del>a15_i386.deb<br>5.会提示错误，错误信息：<br>Selecting previously unselected package wps-office.<br>。。。<br>Errors were encountered while processing:<br>wps-office<br>6.命令修正：sudo apt-get -f install  </p>
<p>注释：(dpkg -i *.deb)deb文件安装   （-r）是文件卸载</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu wps安装</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu storm安装配置</title>
    <url>/2018/04/08/ubuntu%20storm%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>一、安装准备：</p>
<p>JDK、ssh、python。安装都比较简单，我之前做别的时候已经安装，此处不再详述。</p>
<p>ssh的安装和服务启动可以参见我的另一个讲 [ hadoop安装的文章<br>](/2018/04/08/hadoop1.2.1在linux中配置安装独立运行Standalone Operation，伪分布Pseudo-Distributed Operation，集群配置三种配置和测试)。而且我的storm集群和hadoop相同，都是配置在三台虚拟机上的，用户组及用户名和hadoop那个一样，都要求用户名是hadoop</p>
<p>二、安装zookeeper集群（三台虚拟机）</p>
<p>前：创建三个文件夹：  </p>
<pre><code>#zookeeper安放路径
mkdir -p /opt/modules
#zookeeper日志存放路径
mkdir -p /var/log/zookeeper
#zookeeper数据存放路径
mkdir -p /tmp/zookeeper</code></pre><p>1.下载zookeeper源码</p>
<p>官网下载zookeeper</p>
<pre><code>mv zookeeper-3.4.8.tar.gz /opt/modules
tar zxvf zookeeper-3.4.8.tar.gz
ln -s zookeeper-3.4.8 zookeeper</code></pre><p>2.配置zookeeper属性文件</p>
<p>进入zookeeper根目录，之后</p>
<pre><code>cd conf
cp zoo_sample.cfg zoo.cfg</code></pre><p>之后vim打开zoo.cfg，把下面内容追加到里面：</p>
<pre><code>tickTime=2000
clientPort=2181
initLimit=5
syncLimit=2
server.1=ip1:2888:3888
server.2=ip2:2888:3888
server.3=ip3:2888:3888</code></pre><p>!!!!!!ip1,ip2,ip3用你的ip地址替换。</p>
<p>server.A=B:C:D，A是一个数字。B是ip地址，C首选端口，D是防止对方挂掉的预防端口。  </p>
<p>如果只是一台是虚拟机，那就要分配不同的端口了。</p>
<p>zoo.cfg里datadir的默认值是/tmp/zookeeper，我们要在这个目录下创建myid文件。三个虚拟机里这个文件只有一行，分别是上面对应的A值</p>
<pre><code>#创建myid文件
vim /tmp/zookeeper/myid
#添加对应数字
1</code></pre><p>3、配置日志打印文件</p>
<p>把输出日志放到指定文件夹下，便于日志回收和查看。下面在zookeeper根目录下进行：</p>
<p>打开bin/zkEnv.sh ，把下面代码添加到脚本主体（新手注意，不要放在有!/的第一行）的开头部分：  </p>
<pre><code>ZOO_LOG_DIR=/var/log/zookeeper</code></pre><p>4.修改三个主要的文件夹的用户：  </p>
<pre><code>sudo chown -R hadoop:hadoop /opt/modules/zookeeper*
sudo chown -R hadoop:hadoop /var/log/zookeeper
sudo chown -R hadoop:hadoop /tmp/zookeeper</code></pre><p>5.启动zookeeper集群</p>
<p>三台虚拟机进入zookeeper的根目录，执行下面命令：  </p>
<pre><code>bin/zkServer.sh start</code></pre><p>再执行：</p>
<pre><code>bin/zkCli.sh -server 127.0.0.1:2181</code></pre><p>如果安装成功会有[zk:127.0.0.1:2181(CONNETED) 1]的字样（忘记截图），可以输入help，ls /等命令。</p>
<p>！！！！！datadir下的日志和快照不会自动清理，需要通过其他方式定期清理。</p>
<p>三、storm安装：</p>
<p>1.安装storm依赖库</p>
<p>ZeroMQ</p>
<p>JZMQ</p>
<p>1.1安装zeromq：  </p>
<pre><code>wget http://download.zeromq.org/zeromq-2.1.7.tar.gz
tar zxvf zeromq-2.1.7.tar.gz
cd zeromq-2.1.7
./configure
make
sudo make install</code></pre><p>安装出错：</p>
<p>（1）.安装过程中会出现cannot link with -luuid, install uuid-dev  c++错误，执行如下命令：  </p>
<pre><code>sudo apt-get install g++ build-essential gawk zlib1g-dev uuid-dev</code></pre><p>（2）.configure: error: Unable to find a working C++ compiler  </p>
<pre><code>sudo apt-get install g++</code></pre><p>1.2安装jzmq：</p>
<pre><code>sudo apt-get install libtool autoconf
git clone https://github.com/nathanmarz/jzmq.git
cd jzmq
./autogen.sh
./configure
make
sudo make install</code></pre><p>安装出错：</p>
<p>（1）安装 JZMQ出错 (1).make[1]: *<br>没有规则可以创建“org/zeromq/ZMQ.class”需要的目标“classdist_noinst.stamp”。 停止  </p>
<pre><code>#修正方法，创建classdist_noinst.stamp文件
touch src/classdist_noinst.stamp</code></pre><p>（2）make[1]: <strong>* 没有规则可以创建“all”需要的目标“org/zeromq/ZMQ$Context.class”。 停止。<br>make[1]:正在离开目录 `/home/hadoop/Storm/jzmq/src’<br>make: *</strong> [all-recursive] 错误 1  </p>
<pre><code>cd src/
javac -d . org/zeromq/*.java
#手动编译</code></pre><p>2.安装storm集群：</p>
<p>下载storm： <a href="http://storm.incubator.apache.org/" target="_blank" rel="noopener"> 点击打开链接 </a></p>
<p>配置storm.yaml文件，以下都是在storm根目录下。</p>
<p>打开conf/storm.yaml文件</p>
<pre><code>#1.去掉下面代码的#并修改
storm.zookeeper.servers:
        - &quot;ip1&quot;
        - &quot;ip2&quot;
        - &quot;ip3&quot;
#如果zookeeper没有用默认端口，还要改的storm.zookeeper.port
#2.storm.local.dir用于存少量nimbus，supervisor进程的少量状态
storm.local.dir: &quot;/var/storm&quot;
#3.nimbus.host,storm集群nimbus的机器地址，各个supervisor需要知道哪个是nimbus
nimbus.host: &quot;ip1&quot;</code></pre><p>上面只是少量的属性配置，其他的属性都在defaults.yaml文件里。</p>
<p>3.启动storm集群</p>
<pre><code>mkdir /var/storm
sudo chown -R hadoop:hadoop /var/storm
sudo chown -R hadoop:hadoop 你的storm地址</code></pre><p>nimbus：在主控节点上运行，即ip1。在ip1虚拟机上，启动ip1并放在后台执行：</p>
<pre><code>bin/storm nimbus &lt;/dev/null 2&lt;&amp;1 &amp;</code></pre><p>supervise:在Storm工作节点上运行，在ip2,ip3上启动并放在后台执行：</p>
<pre><code>bin/storm supervisor &lt;/dev/null 2&lt;&amp;1 &amp;</code></pre><p>UI:必须在ip1上，因为它会去找nimbus连接</p>
<pre><code>bin/storm ui &lt;/dev/null 2&lt;&amp;1 &amp;</code></pre><p>之后在浏览器打开<a href="http://ip1:8080,会出现下面界面：">http://ip1:8080,会出现下面界面：</a></p>
<p>4.停止storm</p>
<p>集群storm停止需要一个一个的杀死进程。  </p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>storm安装配置</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu 安装numpy的烂问题libgfortran3依赖</title>
    <url>/2018/04/08/ubuntu%20%E5%AE%89%E8%A3%85numpy%E7%9A%84%E7%83%82%E9%97%AE%E9%A2%98libgfortran3%E4%BE%9D%E8%B5%96/</url>
    <content><![CDATA[<h3 id="尝试安装—失败"><a href="#尝试安装—失败" class="headerlink" title="尝试安装—失败"></a>尝试安装—失败</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-numpy</span><br></pre></td></tr></table></figure>
<p>然后出错：  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">错误 http:&#x2F;&#x2F;cn.archive.ubuntu.com&#x2F;ubuntu&#x2F; trusty-updates&#x2F;main libgfortran3 i386 4.8.4-2ubuntu1~14.04  </span><br><span class="line">404  Not Found [IP: 112.124.140.210 80]</span><br></pre></td></tr></table></figure>
<p>然后尝试用它的提示“  apt-get update  或者加上  --fix-missing”来修复。没有作用。<br>之后提示：  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">“libblas3gf : 依赖: libblas3 但是它将不会被安装  </span><br><span class="line">libgfortran3 : 依赖: gcc-4.8-base (&#x3D; 4.8.2-19ubuntu1) 但是 4.8.4-2ubuntu1~14.04正要被安装  </span><br><span class="line">E: 有未能满足的依赖关系。请尝试不指明软件包的名字来运行“apt-get -f install”(也可以指定一个解决办法)。”</span><br></pre></td></tr></table></figure>
<p>用  sudo apt-get install libblas3 libblas3gf  可以修复第一个依赖，第二个仍然不行。</p>
<h3 id="step1-换源"><a href="#step1-换源" class="headerlink" title="step1 换源"></a>step1 换源</h3><p>开始换了163的源，出现下面错误，这种情况最好换源。 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">W: 无法下载 bzip2:&#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;partial&#x2F;mirrors.163.com_ubuntu_dists_trusty-security_main_i18n_Translation-en  Hash 校验和不符  </span><br><span class="line">W: 无法下载 bzip2:&#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;partial&#x2F;mirrors.163.com_ubuntu_dists_trusty-security_universe_i18n_Translation-en  Hash 校验和不符  </span><br><span class="line">W: 无法下载 bzip2:&#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;partial&#x2F;mirrors.163.com_ubuntu_dists_trusty-updates_main_source_Sources  Hash 校验和不符”</span><br></pre></td></tr></table></figure>
<p>换源：  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su -登录root用户。</span><br><span class="line">cd &#x2F;etc&#x2F;apt</span><br><span class="line">cp sources.list sources.list.old</span><br></pre></td></tr></table></figure>
<p>打开sources.list，删除原来所有内容，把<a href="http://chenrongya.blog.163.com/blog/static/8747419620143185103297" target="_blank" rel="noopener">网站</a>中清华大学的源拷进去。之后执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-get update</span><br></pre></td></tr></table></figure>
<h3 id="step2-安装numpy"><a href="#step2-安装numpy" class="headerlink" title="step2 安装numpy"></a>step2 安装numpy</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su XX(你的用户名  )</span><br><span class="line">sudo apt-get -f install</span><br><span class="line">sudo apt-get install gcc-4.8-base libgfortran3 </span><br><span class="line">sudo apt-get install python-numpy</span><br></pre></td></tr></table></figure>
<p>好了，python一下就可以import numpy as np。  </p>
<p>numpy的效率很高，是用c++写的。python里可以添加c++代码，大家都知道的吧，所以中间才有依赖g++的情况（我是这样想得）。  </p>
<p>在vim里删除文件全部内容的命令(‘:%d’)</p>
]]></content>
      <categories>
        <category>caffe安装&amp;&amp;问题&amp;&amp;解决</category>
      </categories>
      <tags>
        <tag>libgfortran3依赖</tag>
        <tag>numpy安装</tag>
      </tags>
  </entry>
  <entry>
    <title>做java时一些问题的解决</title>
    <url>/2018/04/08/%E5%81%9Ajava%E6%97%B6%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[<p>1、java.lang.classnotfoundexception（这个是在做hadoop时遇到）<br>因为我的jdk安装了openJDK，重新安装sunJDk就行了。  </p>
<p>2.改了java home，但是java -version仍然是原来的版本<br>解决：其实java -version是不准的，它只会显示你电脑安装的最新版本。  </p>
<p>3.这个也是hadoop时出现的错</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">16&#x2F;01&#x2F;01 21:48:01 WARN mapred.LocalJobRunner: job_local_0001</span><br><span class="line"></span><br><span class="line">java.lang.NullPointerException  </span><br><span class="line">at org.apache.hadoop.io.serializer.SerializationFactory.getSerializer(Serializ</span><br><span class="line">ationFactory.java:73)  </span><br><span class="line">at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.&lt;init&gt;(MapTask.java:965)  </span><br><span class="line">at</span><br><span class="line">org.apache.hadoop.mapred.MapTask$NewOutputCollector.&lt;init&gt;(MapTask.java:674)  </span><br><span class="line">at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:756)  </span><br><span class="line">at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)  </span><br><span class="line">at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)  </span><br><span class="line">16&#x2F;01&#x2F;01 21:48:01 INFO mapred.JobClient:  map 0% reduce 0%  </span><br><span class="line">16&#x2F;01&#x2F;01 21:48:01 INFO mapred.JobClient: Job complete: job_local_0001  </span><br><span class="line">16&#x2F;01&#x2F;01 21:48:01 INFO mapred.JobClient: Counters: 0</span><br></pre></td></tr></table></figure>
<p>解决：问题出的很挫，主要是因为把输出的intWritable写成了integer。  </p>
<p>4.对字符串使用replaceAll()方法替换 * ? + / | 等字符的时候会报以下异常：Dangling meta character ‘*‘ near index 0<br>这主要是因为这些符号在正则表达示中有相应意义。<br>只需将其改为 [*] 或 //* 即可。（但其实我在split中是\\*）</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title>ubuntu14.04+GPU+caffe</title>
    <url>/2018/04/08/ubuntu14.04+GPU+caffe/</url>
    <content><![CDATA[<p>非常建议ubuntu14.04的系统  </p>
<h1 id="验证硬件支持GPU-CUDA"><a href="#验证硬件支持GPU-CUDA" class="headerlink" title="验证硬件支持GPU CUDA"></a>验证硬件支持GPU CUDA</h1><p>执行下面的操作，然后验证硬件支持GPU CUDA，只要型号存在于<a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">nvidia-cuda-gpus</a>，就没问题了  </p>
<pre><code>$ lspci | grep -i nvidia</code></pre><p>（GPU买之前是专门查过的，这个地方不太重要）  </p>
<h1 id="确定系统是否支持"><a href="#确定系统是否支持" class="headerlink" title="确定系统是否支持"></a>确定系统是否支持</h1><pre><code>$ uname -m &amp;&amp; cat /etc/*release</code></pre><p>重点是“x86_64”这一项，保证是x86架构，64bit系统  </p>
<h1 id="确定有gcc"><a href="#确定有gcc" class="headerlink" title="确定有gcc"></a>确定有gcc</h1><pre><code>$ gcc --version</code></pre><p>（网上说，没有的话就先安装吧，这个是必须的用来编译CUDA Toolkit，不过Ubuntu 14.04是默认有的）  </p>
<h1 id="下载cuda和nvidia驱动（建议都下载-run格式的）"><a href="#下载cuda和nvidia驱动（建议都下载-run格式的）" class="headerlink" title="下载cuda和nvidia驱动（建议都下载.run格式的）"></a>下载cuda和nvidia驱动（建议都下载.run格式的）</h1><p>nvidia驱动： <a href="http://www.geforce.cn/drivers" target="_blank" rel="noopener"> http://www.geforce.cn/drivers </a><br>cuda下载： <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener"> https://developer.nvidia.com/cuda-downloads
</a><br>cuda由于比较大而且网内访问较慢，需要时间有点多  </p>
<h1 id="安装前准备，在tty中显示中文"><a href="#安装前准备，在tty中显示中文" class="headerlink" title="安装前准备，在tty中显示中文"></a>安装前准备，在tty中显示中文</h1><p>因为待会的安装可能会有很多问题，报错是中文的话会显示乱码，影响找错。  </p>
<h2 id="5-1安装fbterm"><a href="#5-1安装fbterm" class="headerlink" title="5.1安装fbterm"></a>5.1安装fbterm</h2><pre><code>sudo apt-get install fbterm</code></pre><h2 id="5-2编辑-fbtermrc文件"><a href="#5-2编辑-fbtermrc文件" class="headerlink" title="5.2编辑.fbtermrc文件"></a>5.2编辑.fbtermrc文件</h2><pre><code>sudo vi .fbtermrc</code></pre><p>，加入：  </p>
<pre><code>font-size=16
text-codings=utf8</code></pre><h1 id="安装nvidia显卡和cuda"><a href="#安装nvidia显卡和cuda" class="headerlink" title="安装nvidia显卡和cuda"></a>安装nvidia显卡和cuda</h1><p> （建议把他们两个放在同一个目录里，待会方面找）  </p>
<h2 id="6-1进入tty（ctrl-alt-f1）后输入如下指令进入中文tty"><a href="#6-1进入tty（ctrl-alt-f1）后输入如下指令进入中文tty" class="headerlink" title="6.1进入tty（ctrl+alt+f1）后输入如下指令进入中文tty"></a>6.1进入tty（ctrl+alt+f1）后输入如下指令进入中文tty</h2><pre><code>sudo fbterm</code></pre><h2 id="6-2退出GUI，"><a href="#6-2退出GUI，" class="headerlink" title="6.2退出GUI，"></a>6.2退出GUI，</h2><pre><code>sudo stop lightdm</code></pre><h2 id="6-3禁用Ubuntu系统自带显卡驱动，"><a href="#6-3禁用Ubuntu系统自带显卡驱动，" class="headerlink" title="6.3禁用Ubuntu系统自带显卡驱动，"></a>6.3禁用Ubuntu系统自带显卡驱动，</h2><pre><code>sudo vim/etc/modprobe.d/nvidia-graphics-drivers.conf</code></pre><p>在文件输入：</p>
<pre><code>blacklist nouveau</code></pre><p>保存退出。  </p>
<pre><code>sudo vim /etc/default/grub</code></pre><p>在文件末尾添加：</p>
<pre><code>rdblacklist=nouveau nouveau.modeset=0</code></pre><p>保存退出。  </p>
<h2 id="6-4其他操作："><a href="#6-4其他操作：" class="headerlink" title="6.4其他操作："></a>6.4其他操作：</h2><pre><code>sudo mv /boot/initramfs-$(uname -r).img/boot/initramfs-$(uname -r)-nouveau.img
sudo dracut/boot/initramfs-$(uname -r).img $(uname -r)
sudo update-initramfs –u</code></pre><p>第一条就提示没有这个文件，后面的就忘了，但是最最后的安装成功，这个地方就忽略了  </p>
<h2 id="6-5-安装驱动"><a href="#6-5-安装驱动" class="headerlink" title="6.5 安装驱动"></a>6.5 安装驱动</h2><p>cd进入nvidia驱动的目录，安装驱动：  </p>
<pre><code>ls -l 查看目录里的内容
 sudo sh ./nvidia驱动名字</code></pre><p>之后安装  </p>
<h2 id="6-6-安装cuda"><a href="#6-6-安装cuda" class="headerlink" title="6.6 安装cuda"></a>6.6 安装cuda</h2><p>（不要用sudo，我的用了之后没法跑samples，即用了之后检测不到cuda）。另外其实这个cuda里还有一个nvidia，因为上面已<br>经安装过nvidia了，安装cuda时就不要选它了，把软连接（问是否要建一个/usr/local/cuda的目录）选上，把samples选上（后面检测是否安<br>装上cuda）：  </p>
<pre><code>sh ./cuda名字</code></pre><h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><h2 id="7-1回到图形界面"><a href="#7-1回到图形界面" class="headerlink" title="7.1回到图形界面"></a>7.1回到图形界面</h2><pre><code>sudo start lightdm</code></pre><h2 id="7-2修改系统环境变量"><a href="#7-2修改系统环境变量" class="headerlink" title="7.2修改系统环境变量"></a>7.2修改系统环境变量</h2><pre><code>sudo vim ~/.bashrc</code></pre><p>在最后加上：</p>
<pre><code>export PATH=/usr/local/cuda-6.5/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-6.5/lib64:$LD_LIBRARY_PATH
sudo ldconfig（如果这句话的作用还是没有用，重启）</code></pre><h2 id="7-3检测cuda是否安装成功，编译cuda-samples并执行"><a href="#7-3检测cuda是否安装成功，编译cuda-samples并执行" class="headerlink" title="7.3检测cuda是否安装成功，编译cuda-samples并执行"></a>7.3检测cuda是否安装成功，编译cuda-samples并执行</h2><pre><code>进入你刚刚选择cuda sample安装的目录后make（如果提示没有make命令，请安装cmake。sudo apt-get install cmake)
编译完毕，切换release目录cd ./bin/x86_64/linux/release
运行实例 ./deviceQuery</code></pre><h1 id="caffe安装"><a href="#caffe安装" class="headerlink" title="caffe安装:"></a>caffe安装:</h1><h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><p>进入这个网址按照它的指令，安装依赖，里面要求的cuda上面已经安装：<br><a href="http://caffe.berkeleyvision.org/install_apt.html" target="_blank" rel="noopener"> http://caffe.berkeleyvision.org/install_apt.htm
</a> l  </p>
<h2 id="下载caffe："><a href="#下载caffe：" class="headerlink" title="下载caffe："></a>下载caffe：</h2><p><a href="https://github.com/BVLC/Caffe/" target="_blank" rel="noopener"> https://github.com/BVLC/Caffe/ </a>  </p>
<h2 id="安装python模块"><a href="#安装python模块" class="headerlink" title="安装python模块"></a>安装python模块</h2><p>（不影响编译，可以略过，但是你最终还是要用到，最好安装）<br>官网：</p>
<p><a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="noopener"> http://caffe.berkeleyvision.org/installation.html
</a></p>
<p>因为官网提供方法安装不可行，可以参考我的： <a href="/2018/04/08/caffe用python时可能需要的模块安装">caffe用python时可能需要的模块安装</a>  </p>
<h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><p>官网： <a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="noopener"> http://caffe.berkeleyvision.org/installation.htm
</a> l</p>
<p>看到那个大大的Compilation以及CMake Build没有，因为我们是GPU，而且也没有安装cudnn，所以可以直接执行它的命令，不用改动配置文件（如果是cpu，就还要把CPU_ONLY :=1的注释放开）  </p>
<h2 id="其他的"><a href="#其他的" class="headerlink" title="其他的"></a>其他的</h2><p><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener"> http://caffe.berkeleyvision.org/ </a><br>官网有很多例子等着你去看  </p>
]]></content>
      <categories>
        <category>caffe安装&amp;&amp;问题&amp;&amp;解决</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>cuda</tag>
        <tag>gpu</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu 安装mono Fiddler后The proxy server is refusing connections</title>
    <url>/2018/04/08/ubuntu%20%E5%AE%89%E8%A3%85mono%20Fiddler%E5%90%8EThe%20proxy%20server%20is%20refusing%20connections/</url>
    <content><![CDATA[<p>ubuntu 安装mono Fiddler后The proxy server is refusing connections火狐拒绝联网：</p>
<p>edit—&gt;preference-&gt;advanced-&gt;Network-&gt;settings选择‘no proxy’点击下边‘ok’即可。</p>
<p>原理参考（<a href="http://www.prweb.com/releases/2013/9/prweb11168005.htm）" target="_blank" rel="noopener">http://www.prweb.com/releases/2013/9/prweb11168005.htm）</a>  </p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>fiddler 火狐 proxy server</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu下的所有解压文件命令</title>
    <url>/2018/04/08/ubuntu%E4%B8%8B%E7%9A%84%E6%89%80%E6%9C%89%E8%A7%A3%E5%8E%8B%E6%96%87%E4%BB%B6%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h4 id="tar"><a href="#tar" class="headerlink" title="tar"></a>tar</h4><p>解包：tar xvf FileName.tar  </p>
<p>打包：tar cvf FileName.tar DirName 注：tar是打包，不是压缩！</p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del> .</p>
<h4 id="gz"><a href="#gz" class="headerlink" title="gz"></a>gz</h4><p>解压1：gunzip FileName.gz  </p>
<p>解压2：gzip -d FileName.gz 压缩：gzip FileName  </p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del> .</p>
<h4 id="tar-gz-和-tgz"><a href="#tar-gz-和-tgz" class="headerlink" title="tar.gz 和 .tgz"></a>tar.gz 和 .tgz</h4><p>解压：tar zxvf FileName.tar.gz  </p>
<p>压缩：tar zcvf FileName.tar.gz DirName  </p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del> .</p>
<h4 id="bz2"><a href="#bz2" class="headerlink" title="bz2"></a>bz2</h4><p>解压1：bzip2 -d FileName.bz2  </p>
<p>解压2：bunzip2 FileName.bz2  </p>
<p>压缩： bzip2 -z FileName  </p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del> .</p>
<h4 id="tar-bz2"><a href="#tar-bz2" class="headerlink" title="tar.bz2"></a>tar.bz2</h4><p>解压：tar jxvf FileName.tar.bz2  </p>
<p>压缩：tar jcvf FileName.tar.bz2 DirName</p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del> .</p>
<h4 id="bz"><a href="#bz" class="headerlink" title="bz"></a>bz</h4><p>解压1：bzip2 -d FileName.bz  </p>
<p>解压2：bunzip2 FileName.bz  </p>
<p>压缩：未知 .</p>
<h4 id="tar-bz"><a href="#tar-bz" class="headerlink" title="tar.bz"></a>tar.bz</h4><p>解压：tar jxvf FileName.tar.bz  </p>
<p>压缩：未知  </p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del> .</p>
<h4 id="Z"><a href="#Z" class="headerlink" title="Z"></a>Z</h4><p>解压：uncompress FileName.Z  </p>
<p>压缩：compress FileName .tar.Z  </p>
<p>解压：tar Zxvf FileName.tar.Z  </p>
<p>压缩：tar Zcvf FileName.tar.Z DirName  </p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del> .</p>
<h4 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h4><p>解压：unzip FileName.zip  </p>
<p>压缩：zip FileName.zip DirName  </p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del> .</p>
<h4 id="rar"><a href="#rar" class="headerlink" title="rar"></a>rar</h4><p>解压：rar x FileName.rar  </p>
<p>压缩：rar a FileName.rar DirName  </p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del> .</p>
<h4 id="lha"><a href="#lha" class="headerlink" title="lha"></a>lha</h4><p>解压：lha -e FileName.lha  </p>
<p>压缩：lha -a FileName.lha FileName  </p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del> .</p>
<h4 id="rpm"><a href="#rpm" class="headerlink" title="rpm"></a>rpm</h4><p>解包：rpm2cpio FileName.rpm | cpio -div</p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del> .</p>
<h4 id="deb"><a href="#deb" class="headerlink" title="deb"></a>deb</h4><p>解包：ar p FileName.deb data.tar.gz | tar zxf -</p>
<p><del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.<del>.</del>.</p>
<p>tar .tgz .tar.gz .tar.Z .tar.bz .tar.bz2 .zip .cpio .rpm .deb .slp .arj .rar.ace .lha .lzh .lzx .lzs .arc .sda .sfx .lnx .zoo .cab .kar .cpt .pit .sit.sea  </p>
<p>解压：sEx x FileName.*</p>
<p>压缩：sEx a FileName.* FileName</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>关于Class.getResource和ClassLoader.getResource的路径问题</title>
    <url>/2018/04/08/%E5%85%B3%E4%BA%8EClass.getResource%E5%92%8CClassLoader.getResource%E7%9A%84%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>参考：<br><a href="http://www.cnblogs.com/yejg1212/p/3270152.html" target="_blank" rel="noopener"> http://www.cnblogs.com/yejg1212/p/3270152.html
</a> （有详细例子，建议看）<br><a href="http://blog.csdn.net/netbug_nb/article/details/46121037" target="_blank" rel="noopener"> http://blog.csdn.net/netbug_nb/article/details/46121037
</a> （有详细例子）</p>
<h3 id="Class-getResource（“”）"><a href="#Class-getResource（“”）" class="headerlink" title="Class.getResource（“”）"></a>Class.getResource（“”）</h3><p>括号中最前面加不加/的效果不同，总结是有/就会取根目录下找，没有就在当前路径下找。</p>
<p>加/ ：是取得class根目录下的路径，即编译以后target/classes的路径，还有maven项目java资源文件和resources目录在同一层时，那层的路径。<br>不加/： 就是当前类的路径，编译以后的在target下的该class文件的路径</p>
<h3 id="ClassLoader-getResource（“”）"><a href="#ClassLoader-getResource（“”）" class="headerlink" title="ClassLoader.getResource（“”）"></a>ClassLoader.getResource（“”）</h3><p>括号中最前面不能加/</p>
<p>不加/  的效果和1中加了/的效果一样<br>加了/ 输出是null。</p>
<h3 id="ClassLoader-getResourceAsStream（）"><a href="#ClassLoader-getResourceAsStream（）" class="headerlink" title="ClassLoader.getResourceAsStream（）"></a>ClassLoader.getResourceAsStream（）</h3><p>和2一样。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>Class.getResource</tag>
        <tag>ClassLoader.getResource</tag>
        <tag>ClassLoader.getResourceAsStream</tag>
      </tags>
  </entry>
  <entry>
    <title>初级cnn研究辅助：python的matplotlib显示图片</title>
    <url>/2018/04/08/%E5%88%9D%E7%BA%A7cnn%E7%A0%94%E7%A9%B6%E8%BE%85%E5%8A%A9%EF%BC%9Apython%E7%9A%84matplotlib%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<h3 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding=UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    img = Image.open(<span class="string">"./Alex.jpg"</span>)</span><br><span class="line">    img_gray = img.convert(<span class="string">"L"</span>)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">121</span>)</span><br><span class="line">    ax.imshow(img)</span><br><span class="line">    ax = fig.add_subplot(<span class="number">122</span>)</span><br><span class="line">    ax.imshow(img_gray, cmap=<span class="string">"gray"</span>)<span class="comment">#以灰度图显示图片</span></span><br><span class="line">    ax.set_title(<span class="string">"hei,i'am the title"</span>)<span class="comment">#给图片加titile</span></span><br><span class="line">    <span class="comment">#plt.axis("off")#不显示刻度</span></span><br><span class="line">    plt.show()<span class="comment">#显示刚才所画的所有操作</span></span><br></pre></td></tr></table></figure>
<p>图片的其他处理，可以查看我的前几篇文章。</p>
<h3 id="add-subplot图片位置"><a href="#add-subplot图片位置" class="headerlink" title="add_subplot图片位置"></a>add_subplot图片位置</h3><p>add_subplot的参数由三个数字组成mnq。代表画布里有m行，n列位置，当前图片将要放在q位置。<br>q的计算方式以行为主：如果是四张图且显示是一个2*2的矩阵，q的排序是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1   2</span><br><span class="line">3   4</span><br></pre></td></tr></table></figure>
<p>以上面代码为例，</p>
<pre><code>ax = fig.add_subplot(121)</code></pre><p>里的121.第一个“1”代表画布只有一行；第一个“2”代表有两列；第二个“1”代表图片将放在1行2列的矩阵中的位置。</p>
<p>当然还会出现这样的需求，左边一张图右边两张图：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding=UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    img = Image.open(<span class="string">"./Alex.jpg"</span>)</span><br><span class="line">    img_gray = img.convert(<span class="string">"L"</span>)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">121</span>)</span><br><span class="line">    ax.imshow(img)</span><br><span class="line">    ax.set_title(<span class="string">"hei,i'am the first"</span>)</span><br><span class="line"></span><br><span class="line">    ax = fig.add_subplot(<span class="number">222</span>)</span><br><span class="line">    ax.imshow(img_gray, cmap=<span class="string">"gray"</span>)<span class="comment">#以灰度图显示图片</span></span><br><span class="line">    ax.set_title(<span class="string">"hei,i'am the second"</span>)<span class="comment">#给图片加titile</span></span><br><span class="line"></span><br><span class="line">    ax = fig.add_subplot(<span class="number">224</span>)</span><br><span class="line">    ax.imshow(img_gray, cmap=<span class="string">"gray"</span>)<span class="comment">#以灰度图显示图片</span></span><br><span class="line">    ax.set_title(<span class="string">"hei,i'am the third"</span>)<span class="comment">#给图片加titile</span></span><br><span class="line">    <span class="comment">#plt.axis("off")#不显示刻度</span></span><br><span class="line">    plt.show()<span class="comment">#显示刚才所画的所有操作</span></span><br></pre></td></tr></table></figure>
<p>效果：</p>
<p><img src="10.png" alt=""></p>
<h3 id="框出部分区域"><a href="#框出部分区域" class="headerlink" title="框出部分区域"></a>框出部分区域</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding=UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    img = Image.open(<span class="string">"./Alex.jpg"</span>)</span><br><span class="line">    img_gray = img.convert(<span class="string">"L"</span>)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">121</span>)</span><br><span class="line">    ax.imshow(img)</span><br><span class="line">    ax.set_title(<span class="string">"hei,i'am the first"</span>)</span><br><span class="line">    pointx = [<span class="number">20</span>, <span class="number">120</span>, <span class="number">120</span>, <span class="number">20</span>, <span class="number">20</span>]</span><br><span class="line">    pointy = [<span class="number">20</span>, <span class="number">20</span>, <span class="number">120</span>, <span class="number">120</span>, <span class="number">20</span>]</span><br><span class="line">    ax.plot(pointx, pointy, <span class="string">'r'</span>)<span class="comment">#画一个矩形，黑色；'r'红色</span></span><br></pre></td></tr></table></figure>
<p>效果：</p>
<p><img src="11.png" alt=""></p>
<h3 id="画点"><a href="#画点" class="headerlink" title="画点"></a>画点</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding=UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    img = Image.open(<span class="string">"./Alex.jpg"</span>)</span><br><span class="line">    img_gray = img.convert(<span class="string">"L"</span>)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">121</span>)</span><br><span class="line">    ax.imshow(img)</span><br><span class="line">    ax.set_title(<span class="string">"hei,i'am the first"</span>)</span><br><span class="line">    pointx = [<span class="number">20</span>, <span class="number">120</span>, <span class="number">120</span>, <span class="number">20</span>, <span class="number">20</span>]</span><br><span class="line">    pointy = [<span class="number">20</span>, <span class="number">20</span>, <span class="number">120</span>, <span class="number">120</span>, <span class="number">20</span>]</span><br><span class="line">    ax.plot(pointx, pointy, <span class="string">'r'</span>)<span class="comment">#画一个矩形，黑色；'r'红色</span></span><br><span class="line">    ax.scatter(<span class="number">65</span>, <span class="number">70</span>)<span class="comment">#画点</span></span><br><span class="line">    ax.scatter(<span class="number">90</span>, <span class="number">70</span>)<span class="comment">#画点</span></span><br><span class="line">    plt.axis(<span class="string">"off"</span>)<span class="comment">#不显示刻度</span></span><br></pre></td></tr></table></figure>
<p>效果：</p>
<p><img src="12.png" alt=""></p>
<p>（额，我怎么把我男神搞成这个样子了。。。。）  </p>
]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>图片处理</tag>
      </tags>
  </entry>
  <entry>
    <title>初级cnn研究辅助：python的matplotlib显示图片 之 按钮和触发事件</title>
    <url>/2018/04/08/%E5%88%9D%E7%BA%A7cnn%E7%A0%94%E7%A9%B6%E8%BE%85%E5%8A%A9%EF%BC%9Apython%E7%9A%84matplotlib%E6%98%BE%E7%A4%BA%E5%9B%BE%E7%89%87%20%E4%B9%8B%20%E6%8C%89%E9%92%AE%E5%92%8C%E8%A7%A6%E5%8F%91%E4%BA%8B%E4%BB%B6/</url>
    <content><![CDATA[<h3 id="图片点击触发事件"><a href="#图片点击触发事件" class="headerlink" title="图片点击触发事件"></a>图片点击触发事件</h3><p>点击左侧图片，显示右侧图片，并在你点击的位置画点。  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> py</span><br><span class="line"><span class="keyword">from</span> matplotlib.widgets <span class="keyword">import</span> Button,RadioButtons</span><br><span class="line"><span class="keyword">import</span> Image</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_press</span><span class="params">(event)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> event.inaxes == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"none"</span></span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">    fig = event.inaxes.figure</span><br><span class="line">    ax = fig.add_subplot(<span class="number">122</span>)</span><br><span class="line">    img_gray = Image.open(<span class="string">"./Alex.jpg"</span>).convert(<span class="string">"L"</span>)</span><br><span class="line">    ax.imshow(img_gray, cmap=<span class="string">"gray"</span>)</span><br><span class="line">    <span class="keyword">print</span> event.x</span><br><span class="line">    ax1.scatter(event.xdata, event.ydata)</span><br><span class="line">    py.axis(<span class="string">"off"</span>)</span><br><span class="line">    fig.canvas.draw()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    img = Image.open(<span class="string">"./Alex.jpg"</span>)</span><br><span class="line">    fig = py.figure()</span><br><span class="line">    fig.canvas.mpl_connect(<span class="string">"button_press_event"</span>, on_press) </span><br><span class="line">    ax1 = fig.add_subplot(<span class="number">121</span>)   </span><br><span class="line">    ax1.imshow(img)</span><br><span class="line">    py.axis(<span class="string">"off"</span>)</span><br><span class="line">    py.show()</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="13.png" alt="">  </p>
<p>解释：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig.canvas.mpl_connect(<span class="string">"button_press_event"</span>, on_press)<span class="comment">#在这个figure上加点击事件，点击后的情况在自己写的on_press()方法里</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_press</span><span class="params">(event)</span>:</span></span><br><span class="line">       event.inaxes.figure.canvas.draw()<span class="comment">#用于图片刷新</span></span><br><span class="line">       event.x<span class="comment">#事件的坐标用于其他按钮点击和figure点击发生冲突时判断返回</span></span><br><span class="line">       event.xdata,event.ydata<span class="comment">#鼠标点击的位置，与上面那个坐标表示形式不同</span></span><br></pre></td></tr></table></figure>
<h3 id="普通按钮触发事件"><a href="#普通按钮触发事件" class="headerlink" title="普通按钮触发事件"></a>普通按钮触发事件</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> py</span><br><span class="line"><span class="keyword">from</span> matplotlib.widgets <span class="keyword">import</span> Button,RadioButtons</span><br><span class="line"><span class="keyword">import</span> Image</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_press</span><span class="params">(event)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> event.inaxes == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"none"</span></span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">    fig = event.inaxes.figure</span><br><span class="line">    ax = fig.add_subplot(<span class="number">122</span>)</span><br><span class="line">    img_gray = Image.open(<span class="string">"./Alex.jpg"</span>).convert(<span class="string">"L"</span>)</span><br><span class="line">    ax.imshow(img_gray, cmap=<span class="string">"gray"</span>)</span><br><span class="line">    <span class="keyword">print</span> event.x</span><br><span class="line">    ax1.scatter(event.xdata, event.ydata)</span><br><span class="line">    py.axis(<span class="string">"off"</span>)</span><br><span class="line">    fig.canvas.draw()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">button_press</span><span class="params">(event)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'button is pressed!'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_button</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> button<span class="comment">#must global</span></span><br><span class="line">    point = py.axes([<span class="number">0.3</span>,<span class="number">0.03</span>,<span class="number">0.1</span>,<span class="number">0.03</span>])</span><br><span class="line">    button = Button(point, <span class="string">"click me"</span>)</span><br><span class="line">    button.on_clicked(button_press)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    img = Image.open(<span class="string">"./Alex.jpg"</span>)</span><br><span class="line">    fig = py.figure()</span><br><span class="line">    draw_button()</span><br><span class="line">    fig.canvas.mpl_connect(<span class="string">"button_press_event"</span>, on_press) </span><br><span class="line">    ax1 = fig.add_subplot(<span class="number">121</span>)   </span><br><span class="line">    ax1.imshow(img)</span><br><span class="line">    py.axis(<span class="string">"off"</span>)</span><br><span class="line">    py.show()</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="14.png" alt=""></p>
<p>点击按钮，控制台输出“button is pressed!”</p>
<p>注解：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">buttonax = plt.axes([<span class="number">0.8</span>,<span class="number">0.03</span>,<span class="number">0.05</span>,<span class="number">0.03</span>])<span class="comment">#按钮的位置大小</span></span><br><span class="line">button = Button(buttonax, <span class="string">"i'am a button"</span>)<span class="comment">#按钮</span></span><br><span class="line">button.on_clicked(save)<span class="comment">#按钮的点击，save()是自己定议的点击后的事件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(event)</span>:</span></span><br><span class="line">    <span class="keyword">print</span>  <span class="string">'i am press'</span></span><br></pre></td></tr></table></figure>
<h3 id="RadioButton"><a href="#RadioButton" class="headerlink" title="RadioButton"></a>RadioButton</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">button_press</span><span class="params">(event)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'button is pressed!'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">radio_press</span><span class="params">(label)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'select: '</span>,label</span><br><span class="line">    radiobutton.set_active(<span class="number">0</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_button</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> button<span class="comment">#must global</span></span><br><span class="line">    <span class="keyword">global</span> radiobutton</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'button'</span></span><br><span class="line">    point = py.axes([<span class="number">0.2</span>,<span class="number">0.03</span>,<span class="number">0.1</span>,<span class="number">0.03</span>])</span><br><span class="line">    button = Button(point, <span class="string">"click me"</span>)</span><br><span class="line">    button.on_clicked(button_press)</span><br><span class="line">    point_two = py.axes([<span class="number">0.6</span>, <span class="number">0.03</span>, <span class="number">0.2</span>, <span class="number">0.05</span>])</span><br><span class="line">    radiobutton = RadioButtons(point_two, (<span class="string">"select me"</span>, <span class="string">"or me"</span>))</span><br><span class="line">    radiobutton.on_clicked(radio_press)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    img = Image.open(<span class="string">"./Alex.jpg"</span>)</span><br><span class="line">    fig = py.figure()</span><br><span class="line">    draw_button()</span><br><span class="line">    fig.canvas.mpl_connect(<span class="string">"button_press_event"</span>, on_press) </span><br><span class="line">    ax1 = fig.add_subplot(<span class="number">121</span>)   </span><br><span class="line">    ax1.imshow(img)</span><br><span class="line">    py.axis(<span class="string">"off"</span>)</span><br><span class="line">    py.show()</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="15.png" alt=""></p>
<p>注解：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">radiobutton.set_active(<span class="number">0</span>)<span class="comment">#"DIFFERENT"被选择</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span><span class="params">(label)</span>:</span></span><br><span class="line">      <span class="keyword">print</span> <span class="string">"label"</span><span class="comment">#label is "DIFFERENT" or "SAME"</span></span><br></pre></td></tr></table></figure>
<p>然后你可能会发现点击右侧按钮时也会触发右侧出现图片，这该怎么办呢？</p>
<p>在一的例子中有event.x就是干这个用的，通过判断这个值判断鼠标点击的位置。但是每人的电脑不同，结果不同。  </p>
]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>图片处理</tag>
      </tags>
  </entry>
  <entry>
    <title>最大似然</title>
    <url>/2018/04/08/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6/</url>
    <content><![CDATA[<p>作为逻辑回归的loss函数考虑。  最小二乘法是可以从最大似然估计推导出来的，所以它俩的本质应当是一样的。</p>
<p>最小二乘法 思想是 求欧式距离最小值。 即求出一条线，样本距离这条线的和最小。</p>
<p>最大似然符和高斯分布时，和最小二乘法的结果一样。</p>
<p>最大似然是站在概率上考虑的，推导出一个概率函数表示目标函数，它希望这个概率函数 越大越好。</p>
<p>最大似然中心思想，假设拿出来的样本数据有很大的参考性，用这个样本数据反推“导致”这个结果的参数。</p>
<p>举一下最常见的例子，一个麻袋里有白球与黑球，但是我不知道它们之间的比例，那我就有放回的抽取10次，结果我发现我抽到了8次黑球2次白球，至此我想知道有多少个黑<br>球。</p>
<p>用最大似然的方式解决： 假设黑球概率是p，因此白球是（1-p）。  L=p^8 * (1-p)^2  求max(L).  先对L求<br>ln，因为L作为目标函数乘法太多 求导结果复杂，因此先求ln，不破坏单调性。</p>
<p>因此就是求 max( ln ( L ) )  , 由微积分概念，对ln( L )求导 因为ln是单调增函数，当导数为0时即可得到最大值 。推导：</p>
<p>ln<code>（p^8 * (1-p)^2 ）= （8 * ln（p）+ 2 * ln (1-p)）</code> = 8 / p - 2 / ( 1 - p ) = 0 求得<br>p=0.8</p>
<p>用最大似然解题过程：</p>
<p>（1）每个样本属于某个类别发生概率的乘积L（比如上面的例子，10个有8个黑球2个白球，因此是8个p 2个(1-p) 相乘）</p>
<p>（2）ln( L )    加ln方法，乘法求导太复杂，不破坏单调性增加ln方法。式子最大值时p的值就是黑球的概率。</p>
<p>（3）求导ln( L )    求导结果为0时 p的值就是黑球的概率。</p>
<p>应用于逻辑回归ln(L)的推导：</p>
<p><img src="1.png" alt=""></p>
<p>求导：</p>
<p><img src="2.png" alt=""></p>
<p>可以对比和最小二乘法的结果一样。</p>
<p>sigmoid： y = 1 / ( 1 + exp(-z ) ) , z = wx+b</p>
<p>由上式推导出： ln( y / ( 1-y) )  = z = ln ( p( y = 1 | x ) / p( y = 0 | x ) ) ,   p(<br>y = 0 | x )  = (1 - p( y = 1 | x ) )</p>
<p>推导得：p( y = 1 | x )  = exp(z) / ( 1 + exp(z) )                  p( y = 0 | x )<br>= 1 / ( 1 + exp(z) )</p>
<p>因此总的概率函数是 L =  yi * p( y = 1 | x )  + (1 - yi) * p( y = 0 | x )<br>(很巧妙，y=0/1 都符和)</p>
<p>ln(L)的和的最大值，就是我们的目标；因为ln的图像是横坐标越大纵坐标越大，但是斜率越小，因此对它求导，求导数的最小值就可以。</p>
<p>参考： [ <a href="http://www.cnblogs.com/sparkwen/p/3441197.html?utm_source=tuicool&amp;utm_me" target="_blank" rel="noopener">http://www.cnblogs.com/sparkwen/p/3441197.html?utm_source=tuicool&amp;utm_me</a><br>dium=referral ](<a href="http://www.cnblogs.com/sparkwen/p/3441197.html?utm_source=tuic" target="_blank" rel="noopener">http://www.cnblogs.com/sparkwen/p/3441197.html?utm_source=tuic</a><br>ool&amp;utm_medium=referral)</p>
<p><a href="http://blog.csdn.net/zouxy09/article/details/20319673" target="_blank" rel="noopener"> http://blog.csdn.net/zouxy09/article/details/20319673
</a></p>
<p><a href="https://www.cnblogs.com/monoSLAM/p/5257589.html" target="_blank" rel="noopener"> https://www.cnblogs.com/monoSLAM/p/5257589.html
</a></p>
<p><a href="https://www.zhihu.com/question/20447622" target="_blank" rel="noopener"> https://www.zhihu.com/question/20447622
</a></p>
<p>《机器学习》</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>最大似然</tag>
        <tag>最小二乘</tag>
      </tags>
  </entry>
  <entry>
    <title>朴素贝叶斯（一）</title>
    <url>/2018/04/08/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>朴素贝叶斯（Naive Bayes）是一种简单的分类算法，它的经典应用案例为人所熟知：文本分类（如垃圾邮件过滤）。</p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>1、朴素贝叶斯有个前提的假设：每个条件（属性）互相之间是独立的。<br>2、最初公式的分母是一个常数，忽略不计。<br>3、在做词分类时，考虑到词很多需要做大量的乘法会影响效率，再者小数的乘法会越乘越小导致数据很小丢失数据，因此对最终的公式做ln处理，不影响单调性，把乘法转换成加法。<br>4、为了防止在计算时出现概率为0的情况，做一些平滑处理。<br>（1）先验概率，分子加α，分母加kα k是类别总数（感觉不用平滑，分子为某个类别样本数）<br>（2）每个条件的概率，分子加α，分母加nα n是特征维度<br>当α=1时，称作Laplace平滑;当0&lt;α&lt;1时，称作Lidstone平滑;α=0时不做平滑。</p>
<p>上面的四点会在下面的详细讲解点出。</p>
<h5 id="公式及其推导"><a href="#公式及其推导" class="headerlink" title="公式及其推导"></a>公式及其推导</h5><p>最初公式：</p>
<p><img src="34.png" alt=""></p>
<p>给定训练数据集(X, Y)，其中每个样本x都包括n维特征，即x=(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>,. . . ,  x<sub>n</sub>)，类标记集合含有k种类别，即y=(y<sub>1</sub>, y<sub>2</sub>,. . . , y<sub>k</sub>)。<br>如果现在来了一个新样本x，求其类别。<br>那么问题就转化为求解P(y<sub>1</sub>|x),P(y<sub>2</sub>|x ) , . . . , P(y<sub>k</sub>|x)中最大的那个，即求后验概率最大的输出：argmaxykP(y<sub>k</sub>|x)</p>
<p>那P(y<sub>k</sub>|x)就通过贝叶斯定理求得：<br><img src="35.png" alt=""></p>
<p>分子中的P(y<sub>k</sub>) 是先验概率，根据训练集样本数就可以简单地计算出来。<br>分母P(x)可以根据全概率公式算，但是对于任何输入的数据都是一个常数，所以可以忽略不计。<br>而条件概率P(x|y<sub>k</sub>)=P(x<sub>1</sub>,x<sub>2</sub>, . . . , x<sub>n</sub>|y<sub>k</sub>)，它的参数规模是指数数量级别的，假设第i维特征x<sub>i</sub>可取值的个数有S<sub>i</sub>个，类别取值个数为k个，那么参数个数为：<br>k∏n<sub>i</sub>=1  S<sub>i</sub><br>这显然不可行。针对这个问题，  ** 朴素贝叶斯算法对条件概率分布作出了独立性的假设 ** ，通俗地讲就是说假设各个维度的特征x<sub>1</sub>, x<sub>2</sub>,  .  .  .  ,  x<sub>n</sub>互相独立  ，在这个假设的前提上，条件概率可以转化为：</p>
<p><img src="36.png" alt=""></p>
<p>公式最终转化成：</p>
<p><img src="37.png" alt=""></p>
<h5 id="平滑处理"><a href="#平滑处理" class="headerlink" title="平滑处理"></a>平滑处理</h5><p>_ ** 某些属性在某些类别上不存在 ** _ ，因此会导致p(x<sub>i</sub>|y<sub>k</sub>)为0，因此需要一下平滑处理（ _ *<em>我一般不会对先验概率公式（下面第一个公式）做平滑，因为某个类别的样本数一般不会为0 *</em> _ ）：<br>N是样本总数，k是类别总数，N<sub>y<sub>k</sub></sub>是类别为y<sub>k</sub>的样本个数，α是平滑值。<br><img src="38.png" alt=""><br>N<sub>y<sub>k</sub></sub>是类别为y<sub>k</sub>的样本个数，n是特征的维数，N<sub>y<sub>k</sub>,x<sub>i</sub></sub>是类别为y<sub>k</sub>的样本中，特征的值是x<sub>i</sub>的样本个数，α是平滑值。<br><img src="39.png" alt=""><br>当α=1时，称作Laplace平滑，当0&lt;α &lt; 1 时，称作Lidstone平滑， α=0时不做平滑。</p>
<h5 id="例"><a href="#例" class="headerlink" title="例"></a>例</h5><p>（来自下面的链接）：<br><img src="40.png" alt=""><br><img src="41.png" alt=""><br><img src="42.png" alt=""><br>由此可以判定y=-1。</p>
<p>更详细可参考 ： <a href="/2018/04/08/朴素贝叶斯（二）实现NBCorpus分类（附代码和数据）"> 朴素贝叶斯（二）实现NBCorpus分类（附代码和数据） </a><br>参考： <a href="http://blog.csdn.net/u013007900/article/details/78049587" target="_blank" rel="noopener"> http://blog.csdn.net/u013007900/article/details/78049587
</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title>用图形画出caffe输出数据的python程序 git基础命令</title>
    <url>/2018/04/08/%E7%94%A8%E5%9B%BE%E5%BD%A2%E7%94%BB%E5%87%BAcaffe%E8%BE%93%E5%87%BA%E6%95%B0%E6%8D%AE%E7%9A%84python%E7%A8%8B%E5%BA%8Fgit%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>caffe的训练过程输出的数据用图形显示出来。先上效果图：</p>
<p><img src="16.png" alt=""></p>
<p>图形说明：</p>
<pre><code>x是迭代次数，y左是train loss；y右是test accuracy。绿色是左边的线，红色是右边的线。</code></pre><p>提醒：slover的格式必须是标准格式，如，冒号后边要有空格。。。。好吧，是我懒得做处理了，反正，就酱~</p>
<p>下载：</p>
<pre><code>git clone https://github.com/meihuakaile/caffe_tool.git</code></pre><p>ubuntu中caffe的输出在/tmp/中，以caffe*命名。</p>
<p>我的代码基本就是把日志里的一些需要的文本匹配了出来。</p>
<p>代码用python编写，简单易懂易修改。</p>
<p>使用方法：</p>
<pre><code>python draw_loss.py --log log地址</code></pre><p>git基础命令：</p>
<p>1.cd mkdir<br>2.pwd显示当前目录<br>3.git init初始化当前目录成git可以管理的目录<br>4.git add 文件名  把文件提交到缓存区（.git文件夹中）<br>5.git commit -m ‘提交描述’ 把缓存区内容推到文件中<br>6.git status 显示当前仓库状态<br>7.git diff 文件名 查看这个文件修改了什么<br>8.git log 显示历史记录<br>9.git log –pretty=oneline 以行的形式显示历史记录</p>
<p>10.git reset –hard HEAD^  返回上一个版本<br>git reset –hard HEAD~n 返回第n个版本（是返回，而不是撤销哦，而且再看status也看不到n之后的了）<br>11.git reflog 显示连带返回版本的log<br>12.git reset –hard 版本号 返回这个版本<br>13.cat 文件名  显示文件的内容<br>14.git checkout – 文件名  可以在提交之前就撤销文件的修改<br>15.rm 文件名  删除该文件<br>16.git remote add origin 仓库（<a href="https://github.com/tugenhua0707/testgit.git）" target="_blank" rel="noopener">https://github.com/tugenhua0707/testgit.git）</a><br>把本地仓库和远程仓库连起来<br>git push -u origin master<br>由于远程库是空的，我们第一次推送master分支时，加上了 –u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的m<br>aster分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。<br>git push origin master 之后可以直接推送<br>17. git clone -q <a href="https://github.com/meihuakaile/hello_world" target="_blank" rel="noopener">https://github.com/meihuakaile/hello_world</a> 从远程克隆仓库到本地</p>
<p>git学习遇到的问题：  </p>
<p>1.fatal: I don’t handle protocol ‘https’<br>解决： git clone -q <a href="https://github.com/meihuakaile/hello_world（加上" target="_blank" rel="noopener">https://github.com/meihuakaile/hello_world（加上</a><br>-q，网上说是windows的git版本问题）  </p>
]]></content>
      <categories>
        <category>cnn图片数据处理、显示</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>caffe</tag>
        <tag>图片处理</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习经典之PCA的数学原理（很值得读）</title>
    <url>/2018/04/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E4%B9%8BPCA%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%EF%BC%88%E5%BE%88%E5%80%BC%E5%BE%97%E8%AF%BB%EF%BC%89/</url>
    <content><![CDATA[<p>PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用<br>于提取数据的主要特征分量，常用于高维数据的降维。网上关于PCA的文章有很多，但是大多数只描述了PCA的分析过程，而没有讲述其中的原理。这篇文章的目的是介绍P<br>CA的基本数学原理，帮助读者了解PCA的工作机制是什么。</p>
<p>当然我并不打算把文章写成纯数学文章，而是希望用直观和易懂的方式叙述PCA的数学原理，所以整个文章不会引入严格的数学推导。希望读者在看完这篇文章后能更好的明白<br>PCA的工作原理。</p>
<h1 id="数据的向量表示及降维问题"><a href="#数据的向量表示及降维问题" class="headerlink" title="数据的向量表示及降维问题"></a>数据的向量表示及降维问题</h1><p>一般情况下，在数据挖掘和机器学习中，数据被表示为向量。例如某个淘宝店2012年全年的流量及交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，格式如<br>下：</p>
<p>(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额)</p>
<p>其中“日期”是一个记录标志而非度量值，而数据挖掘关心的大多是度量值，因此如果我们忽略日期这个字段后，我们得到一组记录，每条记录可以被表示为一个五维向量，其中<br>一条看起来大约是这个样子：</p>
<p><img src="" alt=""> <img src="" alt="">  </p>
<p>(  500  ,  240  ,  25  ,  13  ,  2312.15  )  T</p>
<p>注意这里我用了转置，因为习惯上使用列向量表示一条记录（后面会看到原因），本文后面也会遵循这个准则。不过为了方便有时我会省略转置符号，但我们说到向量默认都是指<br>列向量。</p>
<p>我们当然可以对这一组五维向量进行分析和挖掘，不过我们知道，很多机器学习算法的复杂度和数据的维数有着密切关系，甚至与维数呈指数级关联。当然，这里区区五维的数据<br>，也许还无所谓，但是实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。</p>
<p>降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。</p>
<p>举个例子，假如某学籍数据有两列M和F，其中M列的取值是如何此学生为男性取值1，为女性取值0；而F列是学生为女性取值1，男性取值0。此时如果我们统计全部学籍数<br>据，会发现对于任何一条记录来说，当M为1时F必定为0，反之当M为0时F必定为1。在这种情况下，我们将M或F去掉实际上没有任何信息的损失，因为只要保留一列就可<br>以完全还原另一列。</p>
<p>当然上面是一个极端的情况，在现实中也许不会出现，不过类似的情况还是很常见的。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的<br>相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我<br>们应该很大程度上认为这天的访客数也较高（或较低）”。后面的章节中我们会给出相关性的严格数学定义。</p>
<p>这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。</p>
<p>上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除<br>几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？</p>
<p>要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。而PCA是一种具有严格数学基础并且已被广泛采用的降维方法。下面我不会直接描述PCA，而是通过逐步分<br>析问题，让我们一起重新“发明”一遍PCA。</p>
<h1 id="向量的表示及基变换"><a href="#向量的表示及基变换" class="headerlink" title="向量的表示及基变换"></a>向量的表示及基变换</h1><p>既然我们面对的数据被抽象为一组向量，那么下面有必要研究一些向量的数学性质。而这些数学性质将成为后续导出PCA的理论基础。</p>
<h2 id="内积与投影"><a href="#内积与投影" class="headerlink" title="内积与投影"></a>内积与投影</h2><p>下面先来看一个高中就学过的向量运算：内积。两个维数相同的向量的内积被定义为：</p>
<p>(  a  1  ,  a  2  ,  ⋯  ,  a  n  )  T  ⋅  (  b  1  ,  b  2  ,  ⋯  ,  b  n  )<br>T  =  a  1  b  1  +  a  2  b  2  +  ⋯  +  a  n  b  n  <img src="" alt=""></p>
<p>内积运算将两个向量映射为一个实数。其计算方式非常容易理解，但是其意义并不明显。下面我们分析内积的几何意义。假设A和B是两个n维向量，我们知道n维向量可以等价<br>表示为n维空间中的一条从原点发射的有向线段，为了简单起见我们假设A和B均为二维向量，则  A  =  (  x  1  ,  y  1  )<br>A=(x1,y1)  ，  B  =  (  x  2  ,  y  2  )  B=(x2,y2)<br>。则在二维平面上A和B可以用两条发自原点的有向线段表示，见下图：</p>
<p>![](<a href="https://img-blog.csdn.net/20160828165719150?watermark/2/text/aHR0cDovL2Jsb" target="_blank" rel="noopener">https://img-blog.csdn.net/20160828165719150?watermark/2/text/aHR0cDovL2Jsb</a><br>2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravi<br>ty/Center)  </p>
<p>好，现在我们从A点向B所在直线引一条垂线。我们知道垂线与B的交点叫做A在B上的投影，再设A与B的夹角是a，则投影的矢量长度为  |  A  |  c  o<br>s  (  a  )  |A|cos(a)  ，其中  |  A  |  =  x  2  1  +  y  2  1  −  −  −  −  −  −<br>√  |A|=x12+y12  是向量A的模，也就是A线段的标量长度。</p>
<p>注意这里我们专门区分了矢量长度和标量长度，标量长度总是大于等于0，值就是线段的长度；而矢量长度可能为负，其绝对值是线段长度，而符号取决于其方向与标准方向相同<br>或相反。</p>
<p>到这里还是看不出内积和这东西有什么关系，不过如果我们将内积表示为另一种我们熟悉的形式：</p>
<p><img src="" alt=""> A  ⋅  B  =  |  A  |  |  B  |  c  o  s  (  a  )</p>
<p>现在事情似乎是有点眉目了：A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，即让  |  B  |  =  1</p>
<p>|B|=1  ，那么就变成了：</p>
<p>A  ⋅  B  =  |  A  |  c  o  s  (  a  )  <img src="" alt=""></p>
<p>也就是说，  设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度<br>！这就是内积的一种几何解释，也是我们得到的第一个重要结论。在后面的推导中，将反复使用这个结论。  </p>
<p>基</p>
<p>下面我们继续在二维空间内讨论向量。上文说过，一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。例如下面这个向量：</p>
<p>![](<a href="https://img-blog.csdn.net/20160828165813932?watermark/2/text/aHR0cDovL2Jsb" target="_blank" rel="noopener">https://img-blog.csdn.net/20160828165813932?watermark/2/text/aHR0cDovL2Jsb</a><br>2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravi<br>ty/Center)  </p>
<p>在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的向量可以表示为(3,2)，这是我们再熟悉不过的向量表示。</p>
<p>不过我们常常忽略， ** 只有一个(3,2)本身是不能够精确表示一个向量的 ** 。我们仔细看一下，这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的<br>投影值是2。也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2。注意<br>投影是一个矢量，所以可以为负。</p>
<p>更正式的说，向量(x,y)实际上表示线性组合：</p>
<p><img src="" alt=""> x  (  1  ,  0  )  T  +  y  (  0  ,  1  )  T  </p>
<p>不难证明所有二维向量都可以表示为这样的线性组合。此处(1,0)和(0,1)叫做二维空间中的一组基。</p>
<p>![](<a href="https://img-blog.csdn.net/20160828170012604?watermark/2/text/aHR0cDovL2Jsb" target="_blank" rel="noopener">https://img-blog.csdn.net/20160828170012604?watermark/2/text/aHR0cDovL2Jsb</a><br>2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravi<br>ty/Center)  </p>
<p>所以， ** 要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了 **<br>。只不过我们经常省略第一步，而默认以(1,0)和(0,1)为基。</p>
<p>我们之所以默认选择(1,0)和(0,1)为基，当然是比较方便，因为它们分别是x和y轴正方向上的单位向量，因此就使得二维平面上点坐标和向量一一对应，非常方便。<br>但实际上任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量。</p>
<p>例如，(1,1)和(-1,1)也可以成为一组基。一般来说，我们希望基的模是1，因为从内积的意义可以看到，如果基的模是1，那么就可以方便的用向量点乘基而直接获<br>得其在新基上的坐标了！实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。例如，上面的基可以变为  (  1  2<br>√  ,  1  2  √  )  (12,12)  和  (  −  1  2  √  ,  1  2  √  )  (−12,12)  。</p>
<p>现在，我们想获得(3,2)在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算(3,2)和两个基的内积，不难得到新的坐标为<br>(  5  2  √  ,  −  1  2  √  )  (52,−12)  。下图给出了新的基以及(3,2)在新基上坐标值的示意图：</p>
<p>![](<a href="https://img-blog.csdn.net/20160828170131714?watermark/2/text/aHR0cDovL2Jsb" target="_blank" rel="noopener">https://img-blog.csdn.net/20160828170131714?watermark/2/text/aHR0cDovL2Jsb</a><br>2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravi<br>ty/Center)  </p>
<p>另外这里要注意的是，我们列举的例子中基是正交的（即内积为0，或直观说相互垂直），但可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的。不过因为正交基<br>有较好的性质，所以一般使用的基都是正交的。</p>
<h2 id="基变换的矩阵表示"><a href="#基变换的矩阵表示" class="headerlink" title="基变换的矩阵表示"></a>基变换的矩阵表示</h2><p>下面我们找一种简便的方式来表示基变换。还是拿上面的例子，想一下，将(3,2)变换为新基上的坐标，就是用(3,2)与第一个基做内积运算，作为第一个新的坐标分量<br>，然后用(3,2)与第二个基做内积运算，作为第二个新坐标的分量。实际上，我们可以用矩阵相乘的形式简洁的表示这个变换：</p>
<p><img src="" alt=""> (  1  /  2  √  −  1  /  2  √  1  /  2  √  1  /  2  √  )  (  3  2  )  =<br>(  5  /  2  √  −  1  /  2  √  )  </p>
<p>太漂亮了！其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。可以稍微推广一下，如果我们有m个二维向量，只要将二维向量按列排成一个两行m列矩阵，<br>然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。例如(1,1)，(2,2)，(3,3)，想变换到刚才那组基上，则可以这样表示：  </p>
<p><img src="" alt=""> (  1  /  2  √  −  1  /  2  √  1  /  2  √  1  /  2  √  )  (  1  1  2  2<br>3  3  )  =  (  2  /  2  √  0  4  /  2  √  0  6  /  2  √  0  )</p>
<p>于是一组向量的基变换被干净的表示为矩阵的相乘。</p>
<p>** 一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果 ** 。 </p>
<p>数学表示为：</p>
<p><img src="" alt="">  </p>
<p>其中  p  i  pi  是一个行向量，表示第i个基，  a  j  aj  是一个列向量，表示第j个原始数据记录。</p>
<p>特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。因此这种矩<br>阵相乘的表示也可以表示降维变换。</p>
<p>最后，上述分析同时给矩阵相乘找到了一种物理解释： ** 两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去 **<br>。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学线性代数时对矩阵相乘的方法感到奇怪，但是如果明白了矩阵相乘的物理意义，其合理性就一目了然了。</p>
<h1 id="协方差矩阵及优化目标"><a href="#协方差矩阵及优化目标" class="headerlink" title="协方差矩阵及优化目标"></a>协方差矩阵及优化目标</h1><p>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。但是我们还没有回答一个最最关键的问题：<br>如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？</p>
<p>要完全数学化这个问题非常繁杂，这里我们用一种非形式化的直观方法来看这个问题。</p>
<p>为了避免过于抽象的讨论，我们仍以一个具体的例子展开。假设我们的数据由五条记录组成，将它们表示成矩阵形式：</p>
<p><img src="" alt=""> (  1  1  1  3  2  3  4  4  2  4  )</p>
<p>其中每一列为一条数据记录，而一行为一个字段。为了后续处理方便，我们首先将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0（这样做的道理和好处<br>后面会看到）。</p>
<p>我们看上面的数据，第一个字段均值为2，第二个字段均值为3，所以变换后：</p>
<p><img src="" alt=""> (  −  1  −  2  −  1  0  0  0  2  1  0  1  )</p>
<p>我们可以看下五条数据在平面直角坐标系内的样子：<br>![](<a href="https://img-blog.csdn.net/20160828170458497?watermark/2/text/aHR0cDovL2Jsb" target="_blank" rel="noopener">https://img-blog.csdn.net/20160828170458497?watermark/2/text/aHR0cDovL2Jsb</a><br>2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravi<br>ty/Center)  </p>
<p>现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？</p>
<p>通过上一节对基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维<br>降到一维的问题。</p>
<p>那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。</p>
<p>以上图为例，可以看出如果向x轴投影，那么最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身四个各不相同的二维点投影后只剩下两个不同的值了，这是<br>一种严重的信息丢失，同理，如果向y轴投影最上面的两个点和分布在x轴上的两个点也会重叠。所以看来x和y轴都不是最好的投影选择。我们直观目测，如果向通过第一象限<br>和第三象限的斜线投影，则五个点在投影后还是可以区分的。</p>
<p>下面，我们用数学方法表述这个问题。</p>
<h2 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h2><p>上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即：</p>
<p><img src="" alt=""> V  a  r  (  a  )  =  1  m  ∑  i  =  1  m  (  a  i  −  μ  )  2</p>
<p>由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示：</p>
<p><img src="" alt=""> V  a  r  (  a  )  =  1  m  ∑  i  =  1  m  a  2  i</p>
<p>于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</p>
<h2 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h2><p>对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到<br>一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。</p>
<p>如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，<br>让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。</p>
<p>数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则：</p>
<p><img src="" alt=""> C  o  v  (  a  ,  b  )  =  1  m  ∑  i  =  1  m  a  i  b  i</p>
<p>可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。</p>
<p>当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。</p>
<p>至此，我们得到了降维问题的优化目标： ** 将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，<br>各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差） ** 。</p>
<h2 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h2><p>上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。</p>
<p>我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密<br>切相关。于是我们来了灵感：</p>
<p>假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X：</p>
<p><img src="" alt=""> X  =  (  a  1  b  1  a  2  b  2  ⋯  ⋯  a  m  b  m  )</p>
<p>然后我们用X乘以X的转置，并乘上系数1/m：</p>
<p><img src="" alt=""> 1  m  X  X  T  =  (  1  m  ∑  m  i  =  1  a  2  i  1  m  ∑  m  i  =  1<br>a  i  b  i  1  m  ∑  m  i  =  1  a  i  b  i  1  m  ∑  m  i  =  1  b  2  i  )</p>
<p>奇迹出现了！这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。</p>
<p>根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：</p>
<p>设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设  C  =  1  m  X  X  T<br>，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差  。</p>
<h2 id="协方差矩阵对角化"><a href="#协方差矩阵对角化" class="headerlink" title="协方差矩阵对角化"></a>协方差矩阵对角化</h2><p>根据上述推导，我们发现要达到优化目前，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目<br>的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系：</p>
<p>设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：</p>
<p>D  =  =  =  =  =  1  m  Y  Y  T  1  m  (  P  X  )  (  P  X  )  T  1  m  P  X<br>X  T  P  T  P  (  1  m  X  X  T  )  P  T  P  C  P  T</p>
<p><img src="" alt="">  </p>
<p>现在事情很明白了！我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了  寻找一个矩阵P，满足  P  C  P  T<br>PCPT<br>是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件  。</p>
<p>至此，我们离“发明”PCA还有仅一步之遥！</p>
<p>现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问<br>题。</p>
<p>由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：</p>
<p>1）实对称矩阵不同特征值对应的特征向量必然正交。</p>
<p>2）设特征向量  λ  λ  重数为r，则必然存在r个线性无关的特征向量对应于  λ  λ  ，因此可以将这r个特征向量单位正交化。</p>
<p>由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为  e  1  ,  e  2  ,  ⋯  ,  e  n<br>e1,e2,⋯,en  ，我们将其按列组成矩阵：</p>
<p>E  =  (  e  1  e  2  ⋯  e  n  )  </p>
<p>则对协方差矩阵C有如下结论：</p>
<p>E  T  C  E  =  Λ  =  ⎛  ⎝  ⎜  ⎜  ⎜  ⎜  ⎜  λ  1  λ  2  ⋱  λ  n  ⎞  ⎠  ⎟  ⎟  ⎟<br>⎟  ⎟  </p>
<p>其中  Λ  Λ  为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。</p>
<p>以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。</p>
<p>到这里，我们发现我们已经找到了需要的矩阵P：</p>
<p>P  =  E  T  </p>
<p>P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照  Λ  Λ<br>中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p>
<p>至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。</p>
<h1 id="算法及实例"><a href="#算法及实例" class="headerlink" title="算法及实例"></a>算法及实例</h1><p>为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。</p>
<h2 id="PCA算法"><a href="#PCA算法" class="headerlink" title="PCA算法"></a>PCA算法</h2><p>总结一下PCA的算法步骤：</p>
<p>设有m条n维数据。</p>
<p>1）将原始数据按列组成n行m列矩阵X</p>
<p>2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</p>
<p>3）求出协方差矩阵  C  =  1  m  X  X  T</p>
<p>4）求出协方差矩阵的特征值及对应的特征向量</p>
<p>5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</p>
<p>6）  Y  =  P  X  Y=PX  即为降维到k维后的数据</p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>这里以上文提到的</p>
<p>(  −  1  −  2  −  1  0  0  0  2  1  0  1  )  </p>
<p>为例，我们用PCA方法将这组二维数据其降到一维。</p>
<p>因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵：</p>
<p>C  =  1  5  (  −  1  −  2  −  1  0  0  0  2  1  0  1  )  ⎛  ⎝  ⎜  ⎜  ⎜  ⎜  ⎜<br>⎜  −  1  −  1  0  2  0  −  2  0  0  1  1  ⎞  ⎠  ⎟  ⎟  ⎟  ⎟  ⎟  ⎟  =  (  6  5<br>4  5  4  5  6  5  )  </p>
<p>然后求其特征值和特征向量，具体求解方法不再详述，可以参考相关资料。求解后特征值为：</p>
<p>λ  1  =  2  ,  λ  2  =  2  /  5  </p>
<p>其对应的特征向量分别是：</p>
<p>c  1  (  1  1  )  ,  c  2  (  −  1  1  )  </p>
<p>其中对应的特征向量分别是一个通解，  c  1  c1  和  c  2  c2  可取任意实数。那么标准化后的特征向量为：</p>
<p>(  1  /  2  √  1  /  2  √  )  ,  (  −  1  /  2  √  1  /  2  √  )  </p>
<p>因此我们的矩阵P是：</p>
<p>P  =  (  1  /  2  √  −  1  /  2  √  1  /  2  √  1  /  2  √  )  </p>
<p>可以验证协方差矩阵C的对角化：</p>
<p>P  C  P  T  =  (  1  /  2  √  −  1  /  2  √  1  /  2  √  1  /  2  √  )  (  6<br>/  5  4  /  5  4  /  5  6  /  5  )  (  1  /  2  √  1  /  2  √  −  1  /  2  √<br>1  /  2  √  )  =  (  2  0  0  2  /  5  )</p>
<p>最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示：</p>
<p>Y  =  (  1  /  2  √  1  /  2  √  )  (  −  1  −  2  −  1  0  0  0  2  1  0  1<br>)  =  (  −  3  /  2  √  −  1  /  2  √  0  3  /  2  √  −  1  /  2  √  )</p>
<p>降维投影结果如下图：</p>
<p>![](<a href="https://img-blog.csdn.net/20160828170825670?watermark/2/text/aHR0cDovL2Jsb" target="_blank" rel="noopener">https://img-blog.csdn.net/20160828170825670?watermark/2/text/aHR0cDovL2Jsb</a><br>2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravi<br>ty/Center)  </p>
<h1 id="进一步讨论"><a href="#进一步讨论" class="headerlink" title="进一步讨论"></a>进一步讨论</h1><p>根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也<br>就是让它们在不同正交方向上没有相关性。</p>
<p>因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Ker<br>nel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA<br>的效果就大打折扣了。</p>
<p>最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法<br>个性化的优化。</p>
<p>希望这篇文章能帮助朋友们了解PCA的数学理论基础和实现原理，借此了解PCA的适用场景和限制，从而更好的使用这个算法。</p>
<p>原文：<a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener">http://blog.codinglabs.org/articles/pca-tutorial.html</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>pca</tag>
      </tags>
  </entry>
  <entry>
    <title>朴素贝叶斯（二）实现NBCorpus分类（附代码和数据）</title>
    <url>/2018/04/08/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88%E4%BA%8C%EF%BC%89%E5%AE%9E%E7%8E%B0NBCorpus%E5%88%86%E7%B1%BB%EF%BC%88%E9%99%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%EF%BC%89/</url>
    <content><![CDATA[<h3 id="公式复习"><a href="#公式复习" class="headerlink" title="公式复习"></a>公式复习</h3><p>理论可参考 ： <a href="/2018/04/08/朴素贝叶斯（一）"> 朴素贝叶斯（一） </a><br>公式： <img src="43.png" alt=""><br>（P(x)为常数，可忽略不考虑）<br>平滑： <img src="44.png" alt=""><br>N<sub>y<sub>k</sub></sub>是类别为y<sub>k</sub>的样本个数，n是特征的维数，N<sub>y<sub>k</sub>,x<sub>i</sub></sub>是类别为y<sub>k</sub>的样本中，特征的值是x<sub>i</sub>的样本个数，α是平滑值。</p>
<h3 id="分词公式整理"><a href="#分词公式整理" class="headerlink" title="分词公式整理"></a>分词公式整理</h3><p>在对NBCorpus词分类时，带入上面的公式可得：<br>某词属于某类别的概率 = （该类别该词的个数  + 1/ 该类别词的总数 + 所有类别所有不重复单词总数） ×（该类别样本个数 / 所有类别总样本个数） /（所有类别该词个数 / 所有类别所有词个数）</p>
<ul>
<li>所有类别该词个数 / 所有类别所有词个数   就是上面公式的P(x) ，它的值与类别无关，值相同，所有把它去掉，公式变成：<br>_ ** （该类别该词的个数  + 1/ 该类别词的总数 + 所有类别所有不重复单词总数） ×（该类别样本个数 / 所有类别总样本个数） ** _</li>
<li>（该类别样本个数 / 所有类别总样本个数） 就是上面公式的P(y<sub>k</sub>)，即先验概率。没有使用平滑。</li>
<li>（该类别该词的个数  + 1/ 该类别词的总数 + 所有类别所有不重复单词总数） 就是上面公式的P(x|y<sub>k</sub>)，就是可能性。使用了平滑，分子加的1，分母加的所有类别所有不重复单词总数  都是平滑值。</li>
</ul>
<p>如果按照上面的公式，就和上面的例子一样，对于测试的样本里的每个属性计算出属于每个类别的概率，对于每个类别每个属性的概率相乘。但是，小数越乘越小，最终可能会导致数据会丢失成0，无法比较。解决办法是对公式加ln操作，把乘法变成加法，且不破坏单调性。</p>
<h3 id="分词简单例子"><a href="#分词简单例子" class="headerlink" title="分词简单例子"></a>分词简单例子</h3><p><img src="45.png" alt=""><br><img src="46.png" alt=""><br><img src="47.png" alt=""></p>
<p>上面的词量比较少，没有进行ln操作。但是如果词量太大，举例最后的计算转变成<br><img src="48.png" alt=""></p>
<h3 id="数据及代码"><a href="#数据及代码" class="headerlink" title="数据及代码"></a>数据及代码</h3><p>数据下载地址：<a href="http://download.csdn.net/download/u010668907/10263175" target="_blank" rel="noopener">http://download.csdn.net/download/u010668907/10263175</a><br>。。。。无语了，想上传全部的NBCorpus库，csdn一直说已经存在，整体的库后续再说吧，上面的链接里的是做下面算法计算的那部分，够用了。<br>下面的代码里加了ln操作，否则计算之后每个类别都是0，无法比较</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># __author__='chenliclchen'</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计每个类别每个单词出现的次数</span></span><br><span class="line"><span class="comment"># （该类别文档数） / （所有类别文档数）</span></span><br><span class="line"><span class="comment"># （该类别该单词个数 + 1）/（该类别单词个数 + 所有类别无重复单词个数）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># step1，统计每个类别每个单词出现的次数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_every_sort_every_word</span><span class="params">(sort_path)</span>:</span></span><br><span class="line">    count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> os.listdir(sort_path):</span><br><span class="line">        file_path = os.path.join(sort_path, item)</span><br><span class="line">        <span class="keyword">with</span> open(file_path) <span class="keyword">as</span> file:</span><br><span class="line">            words = file.readlines()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            word = word.replace(<span class="string">'\r\n'</span>, <span class="string">''</span>)</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> count.keys():</span><br><span class="line">                count[word] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                count[word] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存step1的结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_every_sort_every_word</span><span class="params">(count_dict, sort_path)</span>:</span></span><br><span class="line">    file = open(sort_path, <span class="string">'wb'</span>)</span><br><span class="line">    pickle.dump(count_dict, file)</span><br><span class="line">    file.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有类别不重复单词总数， 总词数包括重复</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_all_words</span><span class="params">(count_path)</span>:</span></span><br><span class="line">    all_words_no_repeat = set()</span><br><span class="line">    all_words_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> os.listdir(count_path):</span><br><span class="line">        file = open(os.path.join(count_path, item), <span class="string">'rb'</span>)</span><br><span class="line">        one_sort = pickle.load(file)</span><br><span class="line">        all_words_no_repeat.update(one_sort.keys())</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> one_sort.keys():</span><br><span class="line">            all_words_count += one_sort[key]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> len(all_words_no_repeat), all_words_count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个类别样本数量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_every_sort_docu</span><span class="params">(sort_path)</span>:</span></span><br><span class="line">    count_docu = &#123;&#125;</span><br><span class="line">    all_docu_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> os.listdir(sort_path):</span><br><span class="line">        one_sort_path = os.path.join(sort_path, item)</span><br><span class="line">        count_docu[item] = len(os.listdir(one_sort_path))</span><br><span class="line">        all_docu_num += count_docu[item]</span><br><span class="line">    count_docu[<span class="string">'all'</span>] = all_docu_num</span><br><span class="line">    <span class="keyword">return</span> count_docu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回某个类别所有词的数据量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_one_sort</span><span class="params">(sort_path)</span>:</span></span><br><span class="line">    all_words_count = <span class="number">0</span></span><br><span class="line">    file = open(sort_path, <span class="string">'rb'</span>)</span><br><span class="line">    one_sort = pickle.load(file)</span><br><span class="line">    file.close()</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> one_sort.keys():</span><br><span class="line">        all_words_count += one_sort[key]</span><br><span class="line">    <span class="keyword">return</span> all_words_count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_words</span><span class="params">(test_path)</span>:</span></span><br><span class="line">    file = open(test_path)</span><br><span class="line">    words = file.readlines()</span><br><span class="line">    file.close()</span><br><span class="line">    <span class="keyword">for</span> ind, word <span class="keyword">in</span> enumerate(words):</span><br><span class="line">        words[ind] = word.replace(<span class="string">'\r\n'</span>, <span class="string">''</span>)</span><br><span class="line">    <span class="keyword">return</span> words</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    base_path = <span class="string">'./NBCorpus'</span></span><br><span class="line">    sort_path = os.path.join(base_path, <span class="string">'train'</span>)</span><br><span class="line">    count_path = os.path.join(base_path, <span class="string">'count'</span>)</span><br><span class="line">    <span class="comment"># 计算并保存step1的数据</span></span><br><span class="line">    <span class="keyword">for</span> one_sort <span class="keyword">in</span> os.listdir(sort_path):</span><br><span class="line">        one_sort_path = os.path.join(sort_path, one_sort)</span><br><span class="line">        one_sort_count = count_every_sort_every_word(one_sort_path)</span><br><span class="line">        sort_name = os.path.split(one_sort_path)</span><br><span class="line">        one_count_path = os.path.join(count_path, sort_name[<span class="number">-1</span>])</span><br><span class="line">        save_every_sort_every_word(one_sort_count, one_count_path)</span><br><span class="line"></span><br><span class="line">    sort_prob = &#123;&#125;</span><br><span class="line">    test_path = os.path.join(base_path, <span class="string">'test'</span>, <span class="string">'487141newsML.txt'</span>)</span><br><span class="line">    test_words = get_test_words(test_path)</span><br><span class="line">    all_words_num, all_words_num_repeat = count_all_words(count_path)</span><br><span class="line">    every_docu = count_every_sort_docu(sort_path)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> os.listdir(count_path):</span><br><span class="line">        one_sort_count_path = os.path.join(count_path, item)</span><br><span class="line">        one_sort_num = count_one_sort(one_sort_count_path)</span><br><span class="line">        file = open(one_sort_count_path, <span class="string">'rb'</span>)</span><br><span class="line">        one_sort_count = pickle.load(file)</span><br><span class="line">        prior = every_docu[item] / every_docu[<span class="string">'all'</span>]</span><br><span class="line">        <span class="keyword">import</span> math</span><br><span class="line">        <span class="comment"># 加入ln操作</span></span><br><span class="line">        all_words_prob = math.log(prior)</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> test_words:</span><br><span class="line">            word_num = one_sort_count.get(word, <span class="number">0</span>)</span><br><span class="line">            word_prob = (word_num + <span class="number">1</span>) / (one_sort_num + all_words_num)</span><br><span class="line">            <span class="comment"># all_words_prob *= word_prob</span></span><br><span class="line">            <span class="comment"># 加入ln操作</span></span><br><span class="line">            all_words_prob += math.log(word_prob)</span><br><span class="line">        sort_prob[item] = all_words_prob</span><br><span class="line">        file.close()</span><br><span class="line">    <span class="keyword">print</span> filter(<span class="keyword">lambda</span> x:max(sort_prob.values()) == sort_prob[x], sort_prob)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>最终的输出结果就是 ：AFRICA  </p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title>Mybatis.xml配置</title>
    <url>/2018/03/29/Mybatis-xml%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h3 id="mapUnderscoreToCamelCase"><a href="#mapUnderscoreToCamelCase" class="headerlink" title="mapUnderscoreToCamelCase"></a>mapUnderscoreToCamelCase</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">"mapUnderscoreToCamelCase"</span> <span class="attr">value</span>=<span class="string">"true"</span>/&gt;</span></span><br></pre></td></tr></table></figure>
<p>设为true时，代码把数据库中的带“_”的字段名和java model中的驼峰相对应。（其实这样会有很多的不好的地方）</p>
<h3 id="useGeneratedKeys"><a href="#useGeneratedKeys" class="headerlink" title="useGeneratedKeys"></a>useGeneratedKeys</h3><p>强制允许jdbc支持生成key。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">"useGeneratedKeys"</span> <span class="attr">value</span>=<span class="string">"true"</span>/&gt;</span></span><br></pre></td></tr></table></figure>
<p>Allows JDBC support for generated keys. A compatible driver is required.<br>This setting forces generated keys to be used if set to true,<br>as some drivers deny compatibility but still work</p>
<h3 id="typeAliases"><a href="#typeAliases" class="headerlink" title="typeAliases"></a>typeAliases</h3><p>xml里有<strong>parameterType</strong>  如果不在mybatis.xml中指定：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">typeAliases</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">package</span> <span class="attr">name</span>=<span class="string">"com.data.model"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">typeAliases</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>parameterType就要把实体类的包路径完全写下来。写了上面的package后会自动去这个包下面找，只需要写类名不需要包路径。<br>总结自：<a href="http://blog.csdn.net/lelewenzibin/article/details/42713585" target="_blank" rel="noopener">http://blog.csdn.net/lelewenzibin/article/details/42713585</a></p>
<h3 id="最简单的xml配置例子"><a href="#最简单的xml配置例子" class="headerlink" title="最简单的xml配置例子"></a>最简单的xml配置例子</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">settings</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">"mapUnderscoreToCamelCase"</span> <span class="attr">value</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">"useGeneratedKeys"</span> <span class="attr">value</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">settings</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">typeAliases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">package</span> <span class="attr">name</span>=<span class="string">"com.atp.model"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">typeAliases</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring配置Mybatis</title>
    <url>/2018/02/01/Spring%E9%85%8D%E7%BD%AEMybatis/</url>
    <content><![CDATA[<p>　　mybatis的使用有多种办法。如果使用了spring，可以使用最简单的办法，在spring的配置文件中引入mapper.xml和mybatis.xml（本文也只讲解了这种方法）。</p>
<h1 id="使用spring-mybatis依赖包"><a href="#使用spring-mybatis依赖包" class="headerlink" title="使用spring+mybatis依赖包"></a>使用spring+mybatis依赖包</h1><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mybatisspring.version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">mybatisspring.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mybatis.version</span>&gt;</span>3.4.4<span class="tag">&lt;/<span class="name">mybatis.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mysql.version</span>&gt;</span>5.1.30<span class="tag">&lt;/<span class="name">mysql.version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;mybatisspring.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;mybatis.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;mysql.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h1 id="mybatis-xml"><a href="#mybatis-xml" class="headerlink" title="mybatis.xml"></a>mybatis.xml</h1><p>　　首先，一个mybatis.xml文件做部分配置，如<a href="/2018/03/29/Mybatis-xml配置/">MyBatis.xml配置</a></p>
<h1 id="mapper-xml"><a href="#mapper-xml" class="headerlink" title="mapper.xml"></a>mapper.xml</h1><p>　　之后还需要mapper.xml，可能需要多个，一般和dao层类对应。<br>mapper.xml中需要注意的是namespace，目前见到比较常用的两种办法：</p>
<ul>
<li>在外面写一个接口类，namespace写成这个接口类的名字，接口类的方法写成mapper里的sql id，在service层直接通过接口名+方法名(namespace+sql id)使用。<strong>有多个参数时，用@Param(“name”) 标记多个参数，name是mapper中使用这个参数时的名字。</strong></li>
<li>第一种方法比较局限，dao层是一个接口，主要实现在mapper中，如果想在dao层中加别的代码（一般也不建议加别的逻辑）就无法实现。</li>
</ul>
<p>第二种方法是得到spring配置中的sqlSession，通过它的selectOne等方法执行mapper，这时候mapper的namespace可以是自己取得名字，selectOne的第一个参数是namespace+sql id，第二个参数是sql语句需要的参数。<br>但是某人说在网上看的不建议使用第二种方法，原因他也忘记了，可能是不方便。如果使用方法1，在idea上装一些插件可以通过dao接口的接口方法点到mapper里，而且ctrl+F6修改接口名时mapper里也会自动修改，说起来确实方便了很多。</p>
<h1 id="spring配置mybatis"><a href="#spring配置mybatis" class="headerlink" title="spring配置mybatis"></a>spring配置mybatis</h1><p>1、上面的sqlSession是下面代码中的sqlSession的实例。<br>2、下面代码中的</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"configLocation"</span> <span class="attr">value</span>=<span class="string">"classpath:mybatis.xml"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"mapperLocations"</span> <span class="attr">value</span>=<span class="string">"classpath*:mapper/*.xml"</span>/&gt;</span></span><br></pre></td></tr></table></figure>
<p>配置了第一二步的mybatis.xml和mapper.xml文件。</p>
<p>jdbc.properties：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">driverClassName&#x3D;com.mysql.jdbc.Driver</span><br><span class="line">mysql.url&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;test?autoReconnect&#x3D;true&amp;connectTimeout&#x3D;10000&amp;socketTimeout&#x3D;60000&amp;useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8</span><br><span class="line">mysql.uname&#x3D;cl</span><br><span class="line">mysql.passWord&#x3D;1234</span><br></pre></td></tr></table></figure>
<p>spring配置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">context:property-placeholder</span> <span class="attr">location</span>=<span class="string">"classpath:properties/jdbc.properties"</span> <span class="attr">ignore-unresolvable</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"dataSource"</span> <span class="attr">class</span>=<span class="string">"com.alibaba.druid.pool.DruidDataSource"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">init-method</span>=<span class="string">"init"</span> <span class="attr">destroy-method</span>=<span class="string">"close"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"driverClassName"</span> <span class="attr">value</span>=<span class="string">"$&#123;driverClassName&#125;"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"url"</span> <span class="attr">value</span>=<span class="string">"$&#123;mysql.url&#125;"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"username"</span> <span class="attr">value</span>=<span class="string">"$&#123;mysql.uname&#125;"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"password"</span> <span class="attr">value</span>=<span class="string">"$&#123;mysql.passWord&#125;"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"maxActive"</span> <span class="attr">value</span>=<span class="string">"30"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"initialSize"</span> <span class="attr">value</span>=<span class="string">"10"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"minIdle"</span> <span class="attr">value</span>=<span class="string">"10"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"maxWait"</span> <span class="attr">value</span>=<span class="string">"200"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"timeBetweenEvictionRunsMillis"</span> <span class="attr">value</span>=<span class="string">"60000"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"minEvictableIdleTimeMillis"</span> <span class="attr">value</span>=<span class="string">"300000"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"validationQuery"</span> <span class="attr">value</span>=<span class="string">"SELECT 'x'"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"testWhileIdle"</span> <span class="attr">value</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"testOnBorrow"</span> <span class="attr">value</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"testOnReturn"</span> <span class="attr">value</span>=<span class="string">"false"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"poolPreparedStatements"</span> <span class="attr">value</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"maxPoolPreparedStatementPerConnectionSize"</span> <span class="attr">value</span>=<span class="string">"20"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"sqlSessionFactory"</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.SqlSessionFactoryBean"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"dataSource"</span> <span class="attr">ref</span>=<span class="string">"dataSource"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"configLocation"</span> <span class="attr">value</span>=<span class="string">"classpath:mybatis.xml"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"mapperLocations"</span> <span class="attr">value</span>=<span class="string">"classpath*:mapper/*.xml"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.mapper.MapperScannerConfigurer"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"basePackage"</span> <span class="attr">value</span>=<span class="string">"com.data.dao"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"sqlSessionFactoryBeanName"</span> <span class="attr">value</span>=<span class="string">"sqlSessionFactory"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"sqlSession"</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.SqlSessionTemplate"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">constructor-arg</span> <span class="attr">index</span>=<span class="string">"0"</span> <span class="attr">ref</span>=<span class="string">"sqlSessionFactory"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><p>SqlSessionFactoryBean是一个工厂bean，它的作用就是解析配置（数据源、别名等）。<br>有些项目有很多的数据源，然后可能会把所有的mapper定义在同一个文件（配置mybatis的文件里用mappers参数定义，就是上面的mybatis.xml文件）里，mapperLocations不用。<br>然后这种情况spring怎么知道你的sqlSession是为谁服务的呢？那就看MapperScannerConfigurer的配置参数markerInterface使用接口过滤/annotationClass使用注解过滤了。</p>
<p>MapperScannerConfigurer：<br><code>basePackage</code>：扫描器开始扫描的基础包名，支持嵌套扫描；<br><code>sqlSessionTemplateBeanName</code>：前文提到的模板bean的名称；<br><code>markerInterface</code>：基于接口的过滤器，实现了该接口的dao才会被扫描器扫描，与basePackage是与的作用。<br><code>annotationClass</code>：基于注解的过滤器，配置了该注解的dao才会被扫描器扫描，与basePackage是与的作用。</p>
<h2 id="markerInterface接口过滤："><a href="#markerInterface接口过滤：" class="headerlink" title="markerInterface接口过滤："></a>markerInterface接口过滤：</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"sqlSessionFactory"</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.SqlSessionFactoryBean"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"dataSource"</span> <span class="attr">ref</span>=<span class="string">"dataSource"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"configLocation"</span> <span class="attr">value</span>=<span class="string">"classpath:mybatis.xml"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.mapper.MapperScannerConfigurer"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"basePackage"</span> <span class="attr">value</span>=<span class="string">"com.data.dao"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"sqlSessionFactoryBeanName"</span> <span class="attr">value</span>=<span class="string">"sqlSessionFactory"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"markerInterface"</span> <span class="attr">value</span>=<span class="string">"com.data.dao.UserDao"</span> /&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"sqlSession"</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.SqlSessionTemplate"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">constructor-arg</span> <span class="attr">index</span>=<span class="string">"0"</span> <span class="attr">ref</span>=<span class="string">"sqlSessionFactory"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>com.data.dao.UserDao</code>是一个接口，实现这个接口，且在com.data.dao下 的类会使用这个sqlSession</p>
<h2 id="annotationClass注解过滤："><a href="#annotationClass注解过滤：" class="headerlink" title="annotationClass注解过滤："></a>annotationClass注解过滤：</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"sqlSessionFactory"</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.SqlSessionFactoryBean"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"dataSource"</span> <span class="attr">ref</span>=<span class="string">"dataSource"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"configLocation"</span> <span class="attr">value</span>=<span class="string">"classpath:mybatis.xml"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.mapper.MapperScannerConfigurer"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"basePackage"</span> <span class="attr">value</span>=<span class="string">"com.data.dao"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"sqlSessionFactoryBeanName"</span> <span class="attr">value</span>=<span class="string">"sqlSessionFactory"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"annotationClass"</span> <span class="attr">value</span>=<span class="string">"com.data.dao.UserDao"</span> /&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"sqlSession"</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.SqlSessionTemplate"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">constructor-arg</span> <span class="attr">index</span>=<span class="string">"0"</span> <span class="attr">ref</span>=<span class="string">"sqlSessionFactory"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>com.data.dao.UserDao</code>是一个自定义注解，被这个注解标注，且在com.data.dao下 的类会用这个sqlSession。<br>自定义注解例子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Target</span>(&#123;ElementType.TYPE&#125;)</span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> UserDao &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The value may indicate a suggestion for a logical component name,</span></span><br><span class="line"><span class="comment">     * to be turned into a Spring bean in case of an autodetected component.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> the suggested component name, if any</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">String <span class="title">value</span><span class="params">()</span> <span class="keyword">default</span> ""</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>进阶部分参考：<a href="https://blog.csdn.net/hupanfeng/article/details/21454847" target="_blank" rel="noopener">https://blog.csdn.net/hupanfeng/article/details/21454847</a></p>
]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title>jackson</title>
    <url>/2017/10/19/Jackson/</url>
    <content><![CDATA[<p>参考：<a href="http://www.cnblogs.com/kakag/p/5054772.html" target="_blank" rel="noopener">http://www.cnblogs.com/kakag/p/5054772.html</a><br><a href="http://blog.csdn.net/java_huashan/article/details/46375857" target="_blank" rel="noopener">http://blog.csdn.net/java_huashan/article/details/46375857</a></p>
<p>依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.fasterxml.jackson.core<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jackson-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Jackson 主要有三部分组成，除了三个模块之间存在依赖，不依赖任何外部 jar 包。<br>三个模块及artifactId：jackson-core、jackson-annotations、jackson-databind。</p>
<p>提供了三种使用方式：</p>
<p><code>Streaming API</code> : 其他两种方式都依赖于它而实现，如果要从底层细粒度控制 json 的解析生成，可以使用这种方式;<br><code>Tree Model</code> : 通过基于内存的树形结构来描述 json 数据。json 结构树由 JsonNode 组成。不需要绑定任何类和实体，可以方便的对 JsonNode 来进行操作。<br><code>Data Binding</code> : 最常用的方式，基于属性的 get 和 set方法以及注解来实现 JavaBean 和 json 的互转，底层实现还是 Streaming API.</p>
<p>我目前实际遇到的：<br>第一种不长使用。<br>第二种在返回的json很大层次很深但是你只需要某几层的某几个数据时比较适合使用。<br>第三种就是常规的使用，经常使用。可以把得到的json直接转成相应的类别。</p>
<p>json有很多的属性可以设置，具体可以看上面参考链接里的第一个链接。</p>
<p>比如下面第三种时例子里的<code>mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)</code>;<br>jsonString和自定义类转换时，当自定义类中有比jsonString少的属性时，设置了上面的值就不会报错，可以继续自动转换；可是没有设置时就会报错。<br>自定义类属性比jsonString多时jackson本身就不会报错。</p>
<p>第一种不再细说，因为不长使用。<br>第二种简单例子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">#Jackson提供一个树节点被称为"JsonNode",ObjectMapper提供方法来读json作为树的JsonNode根节点</span><br><span class="line">JsonNode jsonNode = JsonUtils.getMapper().readTree(jsonString);</span><br><span class="line"></span><br><span class="line">#查看节点的类型</span><br><span class="line">jsonNode.getNodeType()</span><br><span class="line">#是不是一个容器</span><br><span class="line"></span><br><span class="line">jsonNode.isContainerNode()</span><br><span class="line">#得到 节点 下属性名</span><br><span class="line"></span><br><span class="line">Iterator&lt;String&gt; fieldNames = jsonNode.fieldNames();</span><br><span class="line"><span class="keyword">while</span>(fieldNames.hasNext())&#123;</span><br><span class="line"> log.info(fieldNames.next());</span><br><span class="line">&#125;</span><br><span class="line">#asText的作用是有值返回值，无值返回空字符串 对值是基本类型时有用</span><br><span class="line">jsonNode.asText()</span><br><span class="line">#get() type是array时可以用来取第N个</span><br><span class="line">node.get(<span class="number">0</span>)</span><br><span class="line">#得到node下的str节点</span><br><span class="line">node.get(“str”)</span><br><span class="line">#得到node下的str的节点，与get不同的是str不存在时返回的不是null而是missing node不妨碍后面的as...</span><br><span class="line">node.path(<span class="string">"str"</span>)</span><br><span class="line">#findPath可以直接找到json中的某个节点</span><br><span class="line">jsonNode.findPath(<span class="string">"str"</span>)</span><br></pre></td></tr></table></figure>

<p>第三种：<br>jsonString和对应类可直接转换。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> com.facebook.presto.jdbc.internal.guava.base.Preconditions;</span><br><span class="line"><span class="keyword">import</span> com.fasterxml.jackson.core.JsonParser;</span><br><span class="line"><span class="keyword">import</span> com.fasterxml.jackson.core.JsonProcessingException;</span><br><span class="line"><span class="keyword">import</span> com.fasterxml.jackson.databind.DeserializationFeature;</span><br><span class="line"><span class="keyword">import</span> com.fasterxml.jackson.databind.ObjectMapper;</span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.validation.constraints.NotNull;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonUtils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> ObjectMapper mapper = <span class="keyword">new</span> ObjectMapper();</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">private</span> <span class="title">JsonUtils</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">static</span> &#123;</span><br><span class="line"> 	<span class="comment">//设置忽略未声明的字段</span></span><br><span class="line"> 	mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, <span class="keyword">false</span>);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 把json数据转成类</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> json 需要转成对象的字符串</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> object 对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt; 泛型</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> 转成的对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">T <span class="title">toObject</span><span class="params">(String json, Class&lt;T&gt; object)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"> 	Preconditions.checkArgument(StringUtils.isNotBlank(json), <span class="string">"toObject方法的参数异常"</span>, json);</span><br><span class="line"></span><br><span class="line"> 	<span class="keyword">try</span> &#123;</span><br><span class="line"> 		<span class="keyword">return</span> mapper.readValue(json, object);</span><br><span class="line"> 	&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line"> 		log.error(<span class="string">"&#123;&#125; json转成object时失败！"</span>, json);</span><br><span class="line"> 		<span class="keyword">throw</span> e;</span><br><span class="line"> 	&#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong><em>json中出现很多\时，是因为多层序列化，有对象被多次序列化。一个对象A序列化后作为string给另一个对象B，B再序列化，之后A就会出现\，如果再多一层，引号前面会有三个\\。<br>json反序列化时需要转化成对象或者map。</em></strong></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>map</title>
    <url>/2017/10/19/map/</url>
    <content><![CDATA[<p>1.并不是所有对象都是可以做map的key的。如，我们平常自己实现的类，如果不重写hashCode和equals方法，是没有办法<br>使用map的。hashCode一般使用自己实现类的<strong><em>不可变属性计算而得</em></strong>，equals则由自己的实现定义计算而得。在比较的过程<br>中<strong><em>先比较hashCode再比较equals方法</em></strong>。<br><strong><em>如果hashCode一样，equals也是true，数据会被冲掉（丢失）</em></strong>。</p>
<p>详细举例：<br>譬如把一个自己定义的class Foo{…}作为key放到HashMap。实际上HashMap也是把数据存在一个数组里面，所以在put函数<br>里面，HashMap会调Foo.hashCode()算出作为这个元素在数组里面的下标，然后把key和value封装成一个对象放到数组。<br>等一下，万一2个对象算出来的hash code一样怎么办？会不会冲掉？先回答第2个问题，会不会冲掉就要看Foo.equals()了，<br>如果equals()也是true那就要冲掉了。万一是false,就是所谓的collision了。当2个元素hashCode一样但是equals为false的时候，<br>那个HashMap里面的数组的这个元素就变成了链表。也就是hash code一样的元素在一个链表里面，链表的头在那个数组里面。<br>get的时候，HashMap也先调key.hashCode()算出数组下标，然后看equals如果是true就是找到了，所以就涉及了equals。</p>
<p>2.<strong><em>hashMap可以做key</em></strong>。map是一个接口，AbstractMap是一个实现了map接口的抽象类，这个抽象类实现了其他map通用的方法，如equals、hashCode、contains等。<br>而其他map类继承了AbstractMap，只需要实现部分方法。因此继承了AbstractMap的类都可以作为key。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>接口comparable和comparator的区别</title>
    <url>/2017/10/19/%E6%8E%A5%E5%8F%A3comparable%E5%92%8Ccomparator%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>comparable只提供了compareTo方法，可以当做是一个内比较器。<br>comparator提供了compare和equals方法，外比较器。<br>如果开发者add进入一个Collection的对象想要Collections的sort方法帮你自动进行排序的话，那么这个对象必须实现Comparable接口。<br>下面通过讲解最常见的sort：<br>对一个自己编写的对象ChatLog实现sort的步骤（以前都是套用的这个流程并没有完全理解）：<br>（1）ChatLog实现Comparable并Override compareTo方法，在compareTo中具体实现比较过程。</p>
<p>（2）为ChatLog实现接口Comparator，在compare方法简单比较大小。</p>
<p>（3）执行Collections.sort(List<ChatLog>, comparator)完成排序。</p>
<p>这样的一个过程，我们还可以去掉第（3）中的第二个参数，进而不要第二步。<br>实现Comparator接口后，如果要修改排序的实现就只需要改Comparator不用该ChatLog对象了，如需要升序改成降序，把Comparator的compare中实现的前后对象换一下位置就可以实现了。<br>其实实现Comparator接口只是为了在外面包一层排序的外衣，可以省略。<br>第一步：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ChatLog</span> <span class="keyword">implements</span> <span class="title">Comparable</span></span>&#123;</span><br><span class="line">    String name = <span class="keyword">null</span>;</span><br><span class="line">    String time = <span class="keyword">null</span>;</span><br><span class="line">    String chat = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getTime</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> time;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTime</span><span class="params">(String time)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.time = time;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getChat</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> chat;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setChat</span><span class="params">(String chat)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.chat = chat;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Object o)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="keyword">this</span> == o)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(o!=<span class="keyword">null</span> &amp;&amp; o <span class="keyword">instanceof</span> ChatLog)&#123;</span><br><span class="line">            ChatLog chatLog = (ChatLog) o;</span><br><span class="line">            <span class="keyword">if</span>((time + name).compareTo(chatLog.time+chatLog.name) &lt; <span class="number">0</span>)&#123;</span><br><span class="line">                <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第二步：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Comparator&lt;ChatLog&gt; COMPARATOR = <span class="keyword">new</span> Comparator&lt;ChatLog&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(ChatLog o1, ChatLog o2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> o1.compareTo(o2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>第三步：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//对chatlog列表进行排序</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;ChatLog&gt; <span class="title">sortMsg</span><span class="params">(List&lt;ChatLog&gt; allChat)</span></span>&#123;</span><br><span class="line">    Collections.sort(allChat, COMPARATOR);</span><br><span class="line">    <span class="keyword">return</span> allChat;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>catalina.base和catalina.home区别</title>
    <url>/2017/05/25/catalina.base%E5%92%8Ccatalina.home%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>参考：<a href="http://desert3.iteye.com/blog/1356006" target="_blank" rel="noopener">http://desert3.iteye.com/blog/1356006</a></p>
<p>tomcat的目录：<br><code>bin</code> (运行脚本）<br><code>conf</code> (配置文件）<br><code>lib</code> (核心库文件）<br><code>logs</code> (日志目录)<br><code>temp</code> (临时目录)<br><code>webapps</code> (自动装载的应用程序的目录）<br><code>work</code> (JVM临时文件目录[java.io.tmpdir]) </p>
<p>上述目录中lib和bin是tomcat独有的，其他的目录有多少个实例就必须有多少个这样的文件夹。因此引出了题目catalina.base和catalina.home的区别。<br>当你想要使用多个tomcat实例（运行多个项目），又不想多次安装tomcat时。会把除了lib和bin目录外的其他目录复制多份，每个项目一份。<br><strong><em>而这时原来的有bin和lib的目录就是catalina.home；每个项目独有的其他几个目录所在层就是catalina.base。</em></strong></p>
<p><strong><em>总的说，CATALINA_HOME 是Tomcat 的安装目录，CATALINA_BASE 是Tomcat 的工作目录。</em></strong><br>不知道时，可以在代码里通过String property = System.getProperty(“catalina.base”); 得知它们的默认值（例如，你用mvn的tomcat插件执行项目时）。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>catalina.base</tag>
        <tag>catalina.home</tag>
      </tags>
  </entry>
  <entry>
    <title>log-appender</title>
    <url>/2017/05/25/log-appender/</url>
    <content><![CDATA[<p><code>&lt;appender&gt;</code>是<code>&lt;configuration&gt;</code>的子节点。有name、class属性。</p>
<p>注意点：appender下的file节点和rollingPolicy下fileNamePattern的区别。前者是活动文件，即当前日志存储的文件；后者是归档文件，即每天压缩的文件之后起的名字（假设policy是每天都把日志压缩一下的策略）。</p>
<h3 id="ConsoleAppender"><a href="#ConsoleAppender" class="headerlink" title="ConsoleAppender"></a>ConsoleAppender</h3><p>把日志输出到控制台。子节点：<br>（1）<code>&lt;encoder&gt;</code>对日志格式化。<br>（2）<code>&lt;target&gt;</code>字符串System.out或System.err,默认是System.out。（不太懂，没用过）</p>
<p>例：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 控制台输出日志 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">layout</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.PatternLayout"</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">layout</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="FileAppender"><a href="#FileAppender" class="headerlink" title="FileAppender"></a>FileAppender</h3><p>把日志输出到文件，子节点有：<br>（1）<code>&lt;file&gt;</code> 日志被写入的文件位置，可以相对也可以绝对。上层目录不存在会自动创建，没有默认值。<br>（2） <del><code>&lt;rollingPolicy&gt;</code></del><br>（3）<code>&lt;encoder&gt;</code> 将记录格式化。<br>（4）<code>&lt;append&gt;</code> true，追加到现有文件中；false，清空重新写到文件中。 默认true。<br>（5）<code>&lt;prudent&gt;</code> true，安全的写入到文件，即使其他的FileAppender也在写这个文件，效率低。 默认是false。</p>
<h3 id="RollingFileAppender"><a href="#RollingFileAppender" class="headerlink" title="RollingFileAppender"></a>RollingFileAppender</h3><p>滚动记录日志。满足某一条件时，文件会被移动别的文件。子节点：<br>（1）<code>&lt;file&gt;</code> 日志被写入的文件位置，可以相对也可以绝对。上层目录不存在会自动创建，没有默认值。<br>（2）<code>&lt;rollingPolicy&gt;</code> 发生滚动时，RollingFileAppender的行为，重命名或者移动位置。<br>（3）<code>&lt;encoder&gt;</code> 将记录格式化。<br>（4）<code>&lt;append&gt;</code> true，追加到现有文件中；false，清空重新写到文件中。 默认true。<br>（5）<code>&lt;prudent&gt;</code> true，不支持FixedWindowRollingPolicy；支持TimeBasedRollingPolicy，但是有两个约束，其一，不支持不允许文件压缩，其二，不能设置file属性。（建议不要设为true）<br>（6）<code>&lt;triggeringPolicy&gt;</code> 告知 RollingFileAppender 合适激活滚动。（？？？？？？）</p>
<h3 id="常用rollingPolicy"><a href="#常用rollingPolicy" class="headerlink" title="常用rollingPolicy"></a>常用rollingPolicy</h3><h4 id="TimeBasedRollingPolicy"><a href="#TimeBasedRollingPolicy" class="headerlink" title="TimeBasedRollingPolicy"></a>TimeBasedRollingPolicy</h4><p>最常用的策略。用时间指定滚动策略。有以下节点：<br>（1）<code>&lt;fileNamePattern&gt;</code>  必要节点。可以是文件和%d的组合。%d可以包含时间格式，可以直接用%d。和RollingFileAppender下的file节点不同的是，此处是文件归档的名字，即当是当前策略时即是每天日志文件压缩时的名字；而file下的是日志实时记录的位置。<br>（2）<code>&lt;maxHistory&gt;</code> 可选节点，保留文档的最大数量，超出数量就删除。 注意，删除旧文档时，为了建立文档而创建的目录也会被删除。如果设置是每天一滚动，maxHistory设置是3，那样就只会保存最近3填的日志，其他的删除。</p>
<h4 id="FixedWindowRollingPolicy"><a href="#FixedWindowRollingPolicy" class="headerlink" title="FixedWindowRollingPolicy"></a>FixedWindowRollingPolicy</h4><p>根据固定窗口算法重命名文件的滚动策略（？？？？？）。有以下子节点：<br>（1）<code>&lt;minIndex&gt;</code> 窗口索引最小值<br>（2）<code>&lt;maxIndex&gt;</code> 窗口索引最大值，当用户指定的窗口过大时，会自动将窗口设置为12。<br>（3）<code>&lt;fileNamePattern &gt;</code>  必须包含“%i”例如，假设最小值和最大值分别为1和2，命名模式为 mylog%i.log,会产生归档文件mylog1.log和mylog2.log。还可以指定文件压缩选项，例如，mylog%i.log.gz 或者 没有log.%i.log.zip</p>
<h3 id="triggeringPolicy"><a href="#triggeringPolicy" class="headerlink" title="triggeringPolicy"></a>triggeringPolicy</h3><h4 id="SizeBasedTriggeringPolicy"><a href="#SizeBasedTriggeringPolicy" class="headerlink" title="SizeBasedTriggeringPolicy"></a>SizeBasedTriggeringPolicy</h4><p>看当前活动文件的大小，如果超过指定大小会告知 RollingFileAppender 触发当前活动文件滚动。只有一个节点：<br>（1）<code>&lt;maxFileSize&gt;</code>:这是活动文件的大小，默认值是10MB.</p>
<h3 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h3><p>负责两件事，一是把日志信息转换成字节数组，二是把字节数组写入到输出流。<br>有一个<code>&lt;pattern&gt;</code>节点，用来设置日志的输入格式。使用“%”加“转换符”方式，如果要输出“%”，则必须用“\”对“%”进行转义。<br>常用转换符，可参看 <a href="http://aub.iteye.com/blog/1103685" target="_blank" rel="noopener">http://aub.iteye.com/blog/1103685</a>  </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>保存最近30天日志</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;catalina.base&#125;/logs/data-atpqmq.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名。--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;catalina.base&#125;/logs/data-atpqmq.%d&#123;yyyy-MM-dd&#125;.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">maxHistory</span>&gt;</span>30<span class="tag">&lt;/<span class="name">maxHistory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--对记录事件进行格式化。--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--如果是 true，日志被追加到文件结尾，如果是 false，清空现存文件，默认是true。--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">append</span>&gt;</span>true<span class="tag">&lt;/<span class="name">append</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--如果是 true，日志会被安全的写入文件，即使其他的FileAppender也在向此文件做写入操作，效率低，默认是 false--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">prudent</span>&gt;</span>false<span class="tag">&lt;/<span class="name">prudent</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>按照固定窗口模式生成日志文件，当文件大于20MB时，生成新的日志文件。窗口大小是1到3，当保存了3个归档文件后，将覆盖最早的日志。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span>   </span><br><span class="line">  <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">file</span>&gt;</span>test.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span>   </span><br><span class="line">   </span><br><span class="line">    <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.FixedWindowRollingPolicy"</span>&gt;</span>   </span><br><span class="line">      <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>tests.%i.log.zip<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span>   </span><br><span class="line">      <span class="tag">&lt;<span class="name">minIndex</span>&gt;</span>1<span class="tag">&lt;/<span class="name">minIndex</span>&gt;</span>   </span><br><span class="line">      <span class="tag">&lt;<span class="name">maxIndex</span>&gt;</span>3<span class="tag">&lt;/<span class="name">maxIndex</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span>   </span><br><span class="line">   </span><br><span class="line">    <span class="tag">&lt;<span class="name">triggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"</span>&gt;</span>   </span><br><span class="line">      <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>20MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;/<span class="name">triggeringPolicy</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>   </span><br><span class="line">      <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger&#123;35&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span>   </span><br><span class="line">  <span class="tag">&lt;/<span class="name">appender</span>&gt;</span>   </span><br><span class="line">           </span><br><span class="line">  <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"FILE"</span> /&gt;</span>   </span><br><span class="line">  <span class="tag">&lt;/<span class="name">root</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>参看：<a href="http://aub.iteye.com/blog/1103685" target="_blank" rel="noopener">http://aub.iteye.com/blog/1103685</a></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>appender</tag>
      </tags>
  </entry>
  <entry>
    <title>flask练习</title>
    <url>/2017/05/11/flask%E7%BB%83%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="template"><a href="#template" class="headerlink" title="template"></a>template</h1><p>html默认必须放在项目目录下的templates文件夹下。flask会默认自动去这个文件夹下找定义的html，使用html时不需要加templates这层目录。</p>
<h1 id="前后端传递参数："><a href="#前后端传递参数：" class="headerlink" title="前后端传递参数："></a>前后端传递参数：</h1><p>  参考：<a href="https://www.jianshu.com/p/4350065bdffe" target="_blank" rel="noopener">https://www.jianshu.com/p/4350065bdffe</a><br>（1）直接取放在链接里的数据，如 <a href="http://localhost:5000/home/lili/" target="_blank" rel="noopener">http://localhost:5000/home/lili/</a>  想直接取到字符串’lili’。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.route("/home/&lt;name&gt;", methods=['GET', 'POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">home</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">"home.html"</span>, name=name)  <span class="comment">#  直接用name就可以了。</span></span><br><span class="line">  （<span class="number">2</span>）get请求，通过request.args.get</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route("/test", methods=['GET'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_strap</span><span class="params">()</span>:</span></span><br><span class="line">    name = request.args.get(<span class="string">'name'</span>)     <span class="comment">#    http://localhost:5000/test?name=lili</span></span><br></pre></td></tr></table></figure>
<p>（3）post请求</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.route("/login", methods=['POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">()</span>:</span></span><br><span class="line">    name = request.form[<span class="string">'name'</span>]</span><br><span class="line">    password = request.form[<span class="string">'password'</span>]</span><br><span class="line">    <span class="keyword">if</span> password.strip():</span><br><span class="line">        <span class="keyword">return</span> render_template(<span class="string">"home.html"</span>, name=name)</span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">"login.html"</span>, message=<span class="string">"wrong password"</span>, name=name)</span><br></pre></td></tr></table></figure>
<p>‘name’和’password’是form表单里input的name值。</p>
<h1 id="bootstrap"><a href="#bootstrap" class="headerlink" title="bootstrap"></a>bootstrap</h1><p>flask  template使用bootstrap和普通使用bootstrap的方式不同。<br>参考：<a href="http://flask-bootstrap-zh.readthedocs.io/zh/latest/basic-usage.html" target="_blank" rel="noopener">http://flask-bootstrap-zh.readthedocs.io/zh/latest/basic-usage.html</a><br>（1）安装<br><code>pip install flask-bootstrap</code><br>（2）使用教程可以参考上面的链接。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request, render_template</span><br><span class="line"><span class="keyword">from</span> flask_bootstrap <span class="keyword">import</span> Bootstrap</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line">app.config[<span class="string">'BOOTSTRAP_SERVER_LOCAL'</span>] = <span class="literal">True</span>   <span class="comment"># 使用本地</span></span><br><span class="line">bootstrap = Bootstrap(app)</span><br></pre></td></tr></table></figure>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>例子：<a href="https://github.com/meihuakaile/template" target="_blank" rel="noopener">template</a><br>执行 template_test.py 后，在浏览器输入 <a href="http://127.0.0.1:5000/" target="_blank" rel="noopener">http://127.0.0.1:5000/</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title>python简单小记</title>
    <url>/2017/05/11/python%E7%AE%80%E5%8D%95%E5%B0%8F%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h1><p>{} 字典、对象<br>[ ]列表、数组<br>()元组<br>增加元素： list用append(obj)、insert(index, obj)，元组不可变，字典通过key直接添加，多次添加会覆盖上次数据<br>减少： list用pop([index])，元组不可变，字典pop(key)<br>是否存在：字典key in dict<br>长度：list用len(list)</p>
<p># 元组和列表变成数组 字典变成对象<br>json.dumps(obj) obj转json<br>json.loads(json) json转obj</p>
<h1 id="引号"><a href="#引号" class="headerlink" title="引号"></a>引号</h1><p>python有时会出现三个双引号/单引号， 原因是可以输出字符串的原有样式。<br>python中单引号的存在是“let‘s go”这样的内容可以直接打印出来。（外单内双/内双外单  都可以）</p>
<p>1.单引号里可以有双引号。内部内容（包括双引号）可以当做字符串。<br>2.双引号里可以有单引号。内部内容（包括单引号）可以当做字符串。<br>3.三个单引号/三个双引号。内部内容可以按照内部所有格式输出，结果不变。</p>
<p>eg ：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="string">'''hello,'''</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="string">'''hello,</span></span><br><span class="line"><span class="string"><span class="meta">... </span>    lili'''</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> a</span><br><span class="line">hello,</span><br><span class="line">    lili</span><br></pre></td></tr></table></figure>

<h1 id="None"><a href="#None" class="headerlink" title="None"></a>None</h1><p>python中的None是空对象的意思；可以将None赋给任意值，也可以将任意值给值是None的对象。</p>
<h1 id="enumerate"><a href="#enumerate" class="headerlink" title="enumerate"></a>enumerate</h1><p>迭代 。 可同时得到下标和数据，有时比较方便：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>l = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> ind, item <span class="keyword">in</span> enumerate(l):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">print</span> ind, item</span><br><span class="line">...</span><br><span class="line"><span class="number">0</span> <span class="number">1</span></span><br><span class="line"><span class="number">1</span> <span class="number">3</span></span><br><span class="line"><span class="number">2</span> <span class="number">4</span></span><br><span class="line"><span class="number">3</span> <span class="number">6</span></span><br></pre></td></tr></table></figure>
<p>enumerate还可以有第二个参数start_ind，表示从第几个下标开始。</p>
<h1 id="pass"><a href="#pass" class="headerlink" title="pass"></a>pass</h1><p>和java里标注的 “do something” 功能一样。 先把位置站住，为之后再填代码。<br>也有人为了代码块里没有东西  防止不出错。</p>
<h1 id="mysql"><a href="#mysql" class="headerlink" title="mysql"></a>mysql</h1><p>安装：<code>sudo apt-get install python-mysqldb</code><br><strong><em>注意sql语句中的字符串一定要加上引号，如下面（2）/（3）的%s外面的引号。！！！！！否则无法识别成字符串</em></strong></p>
<h2 id="sql语句"><a href="#sql语句" class="headerlink" title="sql语句"></a>sql语句</h2><p>（1）<code>sql = &quot;&quot;&quot;insert into user(email, name, password) VALUES (&quot;&quot;&quot;+str(email)+&quot;,&quot; + str(name) + &quot;,&quot; + str(password) + &quot;)&quot;</code><br>（2） <code>sql = &#39;insert into user(email, name, password) VALUES (&quot;%s&quot;, &quot;%s&quot;, &quot;%s&quot;)&#39;
    cursor.execute(sql % (email, name, password))</code><br>（3）<code>sql = &quot;insert into user(email, name, password) VALUES (&#39;%s&#39;, &#39;%s&#39;, &#39;%s&#39;)&quot; % (email, name, password)</code></p>
<h2 id="数据库连接"><a href="#数据库连接" class="headerlink" title="数据库连接"></a>数据库连接</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self._connecter = MySQLdb.Connect(<span class="string">"localhost"</span>, <span class="string">"chenliclchen"</span>, <span class="string">"024"</span>, <span class="string">"test"</span>)</span><br><span class="line">self._sql_cursor = self._connecter.cursor()</span><br></pre></td></tr></table></figure>
<p>connect的参数是 主机名  数据库用户名  数据库用户密码  数据库名</p>
<h2 id="简单操作"><a href="#简单操作" class="headerlink" title="简单操作"></a>简单操作</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    self._sql_cursor.execute(sql)</span><br><span class="line">    self._connecter.commit()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    logger.exception(<span class="string">'exception'</span>)</span><br><span class="line">    self._connecter.rollback()</span><br><span class="line"></span><br><span class="line">self._connecter.close()</span><br></pre></td></tr></table></figure>
<p>增删改都可用上面的语法，查：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    self._sql_cursor.execute(sql)</span><br><span class="line">    result = self._sql_cursor.fetchall()</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    logger.exception(<span class="string">"exception"</span>)</span><br><span class="line">    self._connecter.rollback()</span><br><span class="line"></span><br><span class="line">self._connecter.close()</span><br></pre></td></tr></table></figure>
<p>fetchone(): 该方法获取下一个查询结果集。结果集是一个对象<br>fetchall():接收全部的返回结果行.</p>
<h1 id="logging"><a href="#logging" class="headerlink" title="logging"></a>logging</h1><p>python的logging默认是warn级别。</p>
<h4 id="终端配置"><a href="#终端配置" class="headerlink" title="终端配置"></a>终端配置</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logger = logging.getLogger(<span class="string">"register"</span>)</span><br><span class="line">console = logging.StreamHandler()</span><br><span class="line">console.setLevel(logging.INFO)</span><br><span class="line">formatter = logging.Formatter(<span class="string">'%(name)-12s: %(levelname)-8s %(message)s'</span>)</span><br><span class="line">console.setFormatter(formatter)</span><br><span class="line">logger.addHandler(console)</span><br></pre></td></tr></table></figure>
<p>logging.getLogger(name) name为空时默认是root。为name为register的logger配置之后，其他的代码也用register的logger时配置一样。</p>
<h4 id="打印到文件配置"><a href="#打印到文件配置" class="headerlink" title="打印到文件配置"></a>打印到文件配置</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logging.basicConfig(level=logging.INFO,</span><br><span class="line">format=<span class="string">'%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s'</span>,</span><br><span class="line">datefmt=<span class="string">'%a, %d %b %Y %H:%M:%S'</span>,</span><br><span class="line">filename=<span class="string">'register.log'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="logging-basicConfig参数"><a href="#logging-basicConfig参数" class="headerlink" title="logging.basicConfig参数"></a>logging.basicConfig参数</h4><p><code>filename</code>: 指定日志文件名<br><code>filemode</code>: 和file函数意义相同，指定日志文件的打开模式，’w’或’a’<br><code>format</code>: 指定输出的格式和内容，format可以输出很多有用信息，如上例所示:<br><code>%(levelno)s</code> : 打印日志级别的数值<br><code>%(levelname)s</code>: 打印日志级别名称<br><code>%(pathname)s</code>: 打印当前执行程序的路径，其实就是sys.argv[0]<br><code>%(filename)s</code>: 打印当前执行程序名<br><code>%(funcName)s</code>: 打印日志的当前函数<br><code>%(lineno)d</code>: 打印日志的当前行号<br><code>%(asctime)s</code>: 打印日志的时间<br><code>%(thread)d</code>: 打印线程ID<br><code>%(threadName)s</code>: 打印线程名称<br><code>%(process)d</code>: 打印进程ID<br><code>%(message)s</code>: 打印日志信息<br><code>datefmt</code>: 指定时间格式，同time.strftime()<br><code>level</code>: 设置日志级别，默认为logging.WARNING<br><code>stream</code>: 指定将日志的输出流，可以指定输出到sys.stderr,sys.stdout或者文件，默认输出到sys.stderr，当stream和filename同时指定时，stream被忽略</p>
<h4 id="logging带参数输出"><a href="#logging带参数输出" class="headerlink" title="logging带参数输出"></a>logging带参数输出</h4><p>（1）<code>logger.info(&quot;%(sql)s %(email)s %(name)s %(password)s&quot;, {&#39;sql&#39;: sql, &#39;email&#39;: email, &#39;name&#39;: name, &#39;password&#39;: password})
         logger.debug(msg [ ,*args [, **kwargs]])
        logger.info(&quot;%s %s %s %s&quot;, *(sql, email, name, password))</code><br>（2）<code>logger.log(lvl, msg[ , *args[ , **kwargs]] )</code> 级别作为参数<br>（3）<code>logger.exception(&#39;exception&#39;)</code> 会自动把异常写到logger的日志中去。</p>
<h1 id="with"><a href="#with" class="headerlink" title="with"></a>with</h1><h4 id="优雅关闭文件"><a href="#优雅关闭文件" class="headerlink" title="优雅关闭文件"></a>优雅关闭文件</h4><p>在打开文件时，如果出现异常，文件可能会忘记关闭。如，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">file_object = open(html_file)</span><br><span class="line">body_data = file_object.read()</span><br></pre></td></tr></table></figure>
<p>一般的做法是用try包起来，然后在finally里关闭，因为finally是无论如何都会执行的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        file_object = open(html_file)</span><br><span class="line">        body_data = file_object.read()</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        log.write(<span class="string">'no data read\n'</span>)</span><br><span class="line"><span class="keyword">finally</span></span><br><span class="line">    file_object.close()</span><br></pre></td></tr></table></figure>
<p>但是这样的做法不够优雅。因此考虑用with：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(html_file) <span class="keyword">as</span> file_object:</span><br><span class="line">    body_data = file_object.read()</span><br></pre></td></tr></table></figure>
<p>这样就不用考虑忘记关闭文件了。如果with语句或语句块中发生异常，会调用默认的异常处理器处理，但文件还是会正常关闭。</p>
<h4 id="支持本协议的对象"><a href="#支持本协议的对象" class="headerlink" title="支持本协议的对象"></a>支持本协议的对象</h4><p>with语句仅仅能对支持上下文管理协议的对象使用。支持本协议的对象有：</p>
<p>file<br>decimal.Context<br>thread.LockType<br>threading.Lock<br>threading.RLock<br>threading.Condition<br>threading.Semaphore<br>threading.BoundedSemaphore</p>
<h4 id="原理，执行过程"><a href="#原理，执行过程" class="headerlink" title="原理，执行过程"></a>原理，执行过程</h4><p>（1）、当with语句执行时，便执行上下文表达式(一般为某个方法，例如上面的open方法)来获得一个上下文管理器对象，上下文管理器的职责是提供一个上下文对象，用于在with语句块中处理细节。<br>（2）、一旦获得了上下文对象，就会调用它的__enter__()方法，将完成with语句块执行前的所有准备工作，如果with语句后面跟了as语句，则用<strong>enter</strong>()方法的返回值来赋值。<br>（3）、当with语句块结束时，无论是正常结束，还是由于异常，都会调用上下文对象的__exit__()方法，__exit__()方法有3个参数，如果with语句正常结束，三个参数全部都是 None；如果发生异常，三个参数的值分别等于调用sys.exc_info()函数返回的三个值：类型（异常类）、值（异常实例）和跟踪记录（traceback），相应的跟踪记录对象。<br>（4）、因为上下文管理器主要作用于共享资源，__enter__()和__exit__()方法基本是完成的是分配和释放资源的低层次工作，比如：数据库连接、锁分配、信号量加/减、状态管理、文件打开/关闭、异常处理等。</p>
<h4 id="自己定义with"><a href="#自己定义with" class="headerlink" title="自己定义with"></a>自己定义with</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'__enter__() called'</span>)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_hello</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"hello world!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, e_t, e_v, t_b)</span>:</span></span><br><span class="line">        print(<span class="string">'__exit__() called'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先会执行__enter__方法</span></span><br><span class="line"><span class="keyword">with</span> A() <span class="keyword">as</span> a:  <span class="comment"># a为__enter__的返回对象</span></span><br><span class="line">    a.print_hello()</span><br><span class="line">    print(<span class="string">'got instance'</span>)</span><br><span class="line">    <span class="comment"># 结束会执行__exit__方法</span></span><br><span class="line"></span><br><span class="line">执行输出：</span><br><span class="line">__enter__() called</span><br><span class="line">hello world!</span><br><span class="line">got instance</span><br><span class="line">__exit__() called</span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://www.cnblogs.com/skiler/p/6958344.html" target="_blank" rel="noopener">https://www.cnblogs.com/skiler/p/6958344.html</a></p>
<h1 id="repr-和-str"><a href="#repr-和-str" class="headerlink" title="__repr__和__str__"></a>__repr__和__str__</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">Test</span><span class="params">(object)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, value=<span class="string">'hello, world'</span>)</span>:</span></span><br><span class="line"><span class="meta">... </span>        self.data = value</span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = Test()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">&lt;__main__.Test object at <span class="number">0x7f136896e250</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> t</span><br><span class="line">&lt;__main__.Test object at <span class="number">0x7f136896e250</span>&gt;</span><br></pre></td></tr></table></figure>
<p># 看到了么？上面打印类对象并不是很友好，显示的是对象的内存地址<br># 下面我们重构下该类的<strong>repr</strong>以及<strong>str</strong>，看看它们俩有啥区别</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重构__repr__</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">TestRepr</span><span class="params">(Test)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="meta">... </span>        <span class="keyword">return</span> <span class="string">"repr"</span></span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tr = TestRepr()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tr</span><br><span class="line">repr</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> tr</span><br><span class="line">repr</span><br></pre></td></tr></table></figure>
<p># 重构<strong>repr</strong>方法后，不管直接输出对象还是通过print打印的信息都按我们<strong>repr</strong>方法中定义的格式进行显示了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重构__str__</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">TestStr</span><span class="params">(Test)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="meta">... </span>        <span class="keyword">return</span> <span class="string">"str"</span></span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ts = TestStr()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ts</span><br><span class="line">&lt;__main__.TestStr object at <span class="number">0x7f136896e590</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> ts</span><br><span class="line">str</span><br></pre></td></tr></table></figure>
<p># 你会发现，直接输出对象ts时并没有按我们<strong>str</strong>方法中定义的格式进行输出，而用print输出的信息却改变了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">TestStr</span><span class="params">(Test)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="meta">... </span>        <span class="keyword">return</span> <span class="string">"str"</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="meta">... </span>        <span class="keyword">return</span> <span class="string">"repr"</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ts = TestStr()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ts</span><br><span class="line">repr</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> ts</span><br><span class="line">str</span><br><span class="line"><span class="comment">#  __repr__用于交互模式下提示回应；print输出会先找str</span></span><br></pre></td></tr></table></figure>
<p>明显的区别是，在终端里repr可以使响应输出和print输出的结果一样；str在响应输出仍然是类的地址，print可以按照<strong>str</strong>的定义输出。<br><strong><em>print情况下，先找str函数，没有实现再使用repr函数；终端交互响应时，会先找repr函数，没有再找str函数。</em></strong></p>
<h1 id="datetime"><a href="#datetime" class="headerlink" title="datetime"></a>datetime</h1><p><code>def strftime(format, p_tuple=None)/strftime(format）</code>  time转string<br><code>datetime.strptime(date_string, format)</code> string转time<br><code>timedelta()</code>  时间计算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># __author__='chenliclchen'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime, timedelta</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入当前时间，正数部分是秒，小数部分是毫秒。（java是正数部分是毫秒，python×1000 = java）</span></span><br><span class="line"><span class="keyword">print</span> time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment"># time.struct_time(tm_year=2017, tm_mon=10, tm_mday=17, tm_hour=10, tm_min=22, tm_sec=54, tm_wday=1, tm_yday=290, tm_isdst=0)</span></span><br><span class="line"><span class="comment"># 年 月 日 小时 分钟 秒 一周第几天（0-6） 一年第几天 夏时令</span></span><br><span class="line">nowtime = time.localtime(time.time())</span><br><span class="line"><span class="keyword">print</span> nowtime</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tue Oct 17 10:34:13 2017</span></span><br><span class="line">retime = time.asctime(nowtime)</span><br><span class="line"><span class="keyword">print</span> retime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 时间转字符串</span></span><br><span class="line"><span class="comment"># 2017-10-17 10:38:09</span></span><br><span class="line">time_format1 = <span class="string">"%Y-%m-%d %H:%M:%S"</span></span><br><span class="line"><span class="keyword">print</span> time.strftime(time_format1, nowtime)</span><br><span class="line"></span><br><span class="line"><span class="comment"># time.strftime(format)</span></span><br><span class="line"><span class="comment"># Tue Oct 17 10:40:56 2017</span></span><br><span class="line">time_format2 = <span class="string">"%a %b %d %H:%M:%S %Y"</span></span><br><span class="line">a = time.strftime(time_format2, nowtime)</span><br><span class="line"><span class="keyword">print</span> a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将字符串转成时间戳</span></span><br><span class="line"><span class="comment"># time.struct_time(tm_year=2017, tm_mon=10, tm_mday=17, tm_hour=10, tm_min=43, tm_sec=5, tm_wday=1, tm_yday=290, tm_isdst=-1)</span></span><br><span class="line"><span class="keyword">print</span> time.strptime(a, time_format2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1508208272.0(1970~2038)</span></span><br><span class="line"><span class="keyword">print</span> time.mktime(time.strptime(a, time_format2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># datetime.now(): 2017-10-17 10:50:05.383724 2017</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"datetime.now(): "</span>, datetime.now(), datetime.now().year</span><br><span class="line"><span class="comment"># datetime.strftime(format)</span></span><br><span class="line"><span class="comment"># datetime format 2017-10-17 10:57:44 &lt;type 'str'&gt;</span></span><br><span class="line">to_string = datetime.strftime(datetime.now(), time_format1)</span><br><span class="line">to_string = datetime.now().strftime(time_format1)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"datetime to string: "</span>, to_string, type(to_string)</span><br><span class="line"><span class="comment"># datetime.strptime(date_string, format)</span></span><br><span class="line"><span class="comment"># datetime totime: 2017-10-17 10:57:44 &lt;type 'datetime.datetime'&gt;</span></span><br><span class="line">to_date = datetime.strptime(<span class="string">"2017-10-17 10:57:44"</span>, time_format1)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"datetime_string to datetime: "</span>, to_date, type(to_date)</span><br><span class="line"></span><br><span class="line">tmp = datetime.strptime(<span class="string">"2017-10-16"</span>, <span class="string">"%Y-%m-%d"</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"tmp, tmp.isoweekday(): "</span>, tmp, tmp.isoweekday()</span><br><span class="line">last_time = tmp + timedelta(days=<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"last_time"</span>, last_time</span><br><span class="line"><span class="keyword">print</span> datetime.strftime(last_time, <span class="string">"%Y-%m-%d"</span>)</span><br></pre></td></tr></table></figure>
<p><a href="https://docs.python.org/3/library/datetime.html" target="_blank" rel="noopener">https://docs.python.org/3/library/datetime.html</a></p>
<h1 id="unicode-escape、string-escape"><a href="#unicode-escape、string-escape" class="headerlink" title="unicode-escape、string-escape"></a>unicode-escape、string-escape</h1><p>unicode-escape编码集，它是将unicode内存编码值直接存储。<br>如果输出中出现‘\u’，如‘\u9152\u5e97’ 这样的就需要先用它进行解码，在用utf-8编码。如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l = [<span class="string">'12'</span>, <span class="string">u'\u9152\u5e97\u53d1\u5e03\u6545\u969c'</span>]</span><br><span class="line"><span class="keyword">print</span> str(l).decode(<span class="string">'unicode-escape'</span>).encode(<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure>
<p>下面的两种方法效果一样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ll = <span class="string">u'\u9152\u5e97\u53d1\u5e03\u6545\u969c'</span></span><br><span class="line"><span class="keyword">print</span> ll.encode(<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ll = <span class="string">'\u9152\u5e97\u53d1\u5e03\u6545\u969c'</span></span><br><span class="line"><span class="keyword">print</span> ll.decode(<span class="string">'unicode-escape'</span>).encode(<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure>
<p>string-escape编码集，可以对字节流用string-escape进行编码。<br>目前理解的是出现‘\x’时，如’\xe9\x85\x92\xe5\xba\x97’这样的就需要用此数据集解码就可以直接显示。如：<br><code>print &#39;\xe9\x85\x92\xe5\xba\x97&#39;.decode(&#39;string-escape&#39;)</code></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>一个奇特的脚本输出日志方式-----记录时间方式</title>
    <url>/2017/05/11/%E4%B8%80%E4%B8%AA%E5%A5%87%E7%89%B9%E7%9A%84%E8%84%9A%E6%9C%AC%E8%BE%93%E5%87%BA%E6%97%A5%E5%BF%97%E6%96%B9%E5%BC%8F-----%E8%AE%B0%E5%BD%95%E6%97%B6%E9%97%B4%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<p>我的python是野路子出身，平常会写一些脚本处理少量数据。当然处理数据的过程中，都要有一些输出记录脚本是否是在执行或者是程序中间出错卡死了，一般的办法是在<br>处理完一个文件就做一个输出，但是当要处理的文件很小数量很多，这时每个文件的处理很快脚本的输出会占用很多时间（当输出时间大于处理时间时，嘻嘻嘻～）。我看过的源<br>码一种解决方式是，记录处理个数，每1000个做一次输出。而今天说的是我前几日又看的一段开源脚本的处理方式——-通过记录时间。如下伪代码：</p>
<pre><code>时间开始

for ...

   for...

       if 时间结束 - 时间开始 &gt; 某值：

            输出日志

            时间开始

       ...</code></pre><p>伪代码很好理解，第二个“时间开始”和“输出日志”在一层。</p>
<p>但是这样的方式有一个缺点，这也是我去看源码的原因。如果你的输出日志记录的是“当前处理到的文件”，那么如果长时间不看代码或者编码者之外的人运行这个脚本就会奇怪<br>“这个日志输出文件名不是有序的也没有规律可循，是不是我理解错了还是脚本写的有问题？”，毕竟每个文件的处理时间有差别，输出就不是规律的了。解决方法是在脚本的执<br>行初始就对这样的输出方式解释一遍，或者readme。</p>
<p>至于它的优点，First，不用一直输出，节省时间，输出少一些输出日志也会干净一些；Second，不用记录处理个数，没有做过尝试，但是记录个数在有多个循环时要<br>不停的计算，浪费时间。</p>
<p>另外很多输出日志也利用循环的下标，当下标可以整除1000时输出日志。和时间记录的方式相比，缺点很明显，当循环有多层时，使用下标方法无法再使用。而且以C语言中<br>一直强调的“能用加减就不要用乘除”的原则，记录时间方法是更加节省时间的。</p>
<p>总之，输出日志是必须的，但是不能影响时间效率。  </p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
  </entry>
  <entry>
    <title>程序员修炼之道----从小工到大家读书笔记（一）</title>
    <url>/2017/05/11/%E7%A8%8B%E5%BA%8F%E5%91%98%E4%BF%AE%E7%82%BC%E4%B9%8B%E9%81%93----%E4%BB%8E%E5%B0%8F%E5%B7%A5%E5%88%B0%E5%A4%A7%E5%AE%B6%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>以前一直不敢写读后感，担心会误人子弟。后来突然觉得，权当分享吧，大家还可以探讨自己的不同想法，读书就当如此吧。  </p>
<p>5月6</p>
<p>偶然看到这本书，以前去公司实习，上司就说起过这本书“你把这本书看完，工作就不用看其他书了”。一直感叹它的神奇，却未曾观看，今日有幸见到，真想对它说“我来晚了<br>，幸会幸会”。只是如今我仍身在学校，做的项目也不多，实用性项目估计也就实习时那个了，这样的大家之书我不知是否能领悟一二。</p>
<p>但其实可能是我想多了，读这样的书，学到何止是编程之道。今天，我先谈谈编程的哲学之道。</p>
<p>1.如果在项目中，你有一个新奇想法，但是直接提出，会遇到各个部门的问题，经费和人力的“刁难”。我们可以采取“引诱”策略，即自己先小做一点东西出来，勾起其他人<br>的兴趣，一点一点的告诉别人需要添加的东西，整个的点子慢慢就成型了。这点我觉得就算是在别的工作生活上也是很有帮助的，通俗点讲就是，如果你有一个事情，希望其他人<br>也可以加入进来，但是如果直接让他们来参加你的工作，可能大家会觉得没有兴趣或者“有这点时间我还不如做点别的”，这时候我们就需要采取迂回策略，自己先做出一些成就<br>来吸引他人加入。</p>
<p>2.做产品要把握好度，不能不顾时间的精益求精。我曾经参加一个培训，最后要求在不到几天的时间里用所教技术做出一个网站。我本人是一个完美主义者，在做那个网站工作<br>时，当然又是什么都要求做到完美，什么功能都相加，什么地方都想做到完美，需求被设计的很精细，然而，到最后时间当然不够，连最后的提前演练都没来得及。还好那个项目<br>只是个培训，如果是要给客户，我想我死定了。所以呢，我们其实都明白软件是不可能完美的，时间很紧迫，你应当选出重要的完成，让你的客户参与你的工作，也许有些需求是<br>不必要的。</p>
<p>3.你的资产。行业内一直流传“程序员是吃青春饭的”。我还是个年轻的家伙，还不知道老了以后的情况。额。。。青春饭这个书中并没有提到，只是我自己突然想到了。好吧<br>，我是常常容易跑神。回到正题，我身边有个姑娘跟我抱怨多次“我怎么选了这么个行业啊，这么多知识和技术，学了又过时了，讨厌死了”，我的回答挺欠揍“这才是计算机的<br>迷人所在啊”。你的知识代表了你的价值，它们就是你的资产。计算机的技术日新月异的变化，可能你今天才决定要学一下这门语言，明天媒体就大肆宣扬此语言已无出路可循将<br>被互联网淘汰。新语言层出不穷，如果你拒绝接受新的解决方法，固步自封，这是否也是“青春饭”的一个原因呢。</p>
<p>书中建议，一年学一门新的语言，一月读一本新书。也不要只读技术书，人之所以强大，是因为有自己的思想。用批判的眼光看待一切，媒体宣传的不一定是好的，网页搜到的前<br>几也不一定是好的，书店里显眼地方摆放的书也不一定是最好的，一切都有可能是故意的。所以，要有自己的想法。多参加一些技术分享，找到与自己至趣相投的，与自己项目相<br>关的人士聊聊技术。另外还可以了解别人所用的技术，不要把自己封起来，要多尝试。</p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
  </entry>
  <entry>
    <title>计算机视觉与图像新手的心酸胡扯</title>
    <url>/2017/05/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%8E%E5%9B%BE%E5%83%8F%E6%96%B0%E6%89%8B%E7%9A%84%E5%BF%83%E9%85%B8%E8%83%A1%E6%89%AF/</url>
    <content><![CDATA[<p>我做计算机视觉有半年左右，也是现在现在很火的深度学习。有人说这个是“技术大爆炸”，突然冒出大批人来搞深度学习。但是我现在不想说深度学习，只想说说它的辅助工作-图片处理基础。</p>
<p>我是因为读研才开始基础计算机视觉，之前什么都不懂，是真的什么都不懂。然后实验室还是新开设的这个项目，也就是说，实验室也不会有人来指导我如何做，那时只有老师说要什么东西，我就得自己各种的查找来完成，现在想想还是很苦逼。</p>
<p>开始时连图片是数字组成的都不知道。所以真的是新新手啊。图片是由一个矩阵组成的，这个矩阵可以是一维，三维，四维（我所知道的）。四维图片时RGBA，即red，green，blue，阿尔法值。三维是RGB，但其实如果你用的开源框架caffe，caffe的内部用opencv处理图片，opencv读出来的三维图是BGR。<br>四维和三维都是彩色图。一维的一般为灰度图，mode是L，但是mode是P时也是一维的，我前几天刚刚接触model为P的图片，它是一维的，但是它却可以有颜色，貌似是因为调色板什么的原因，我一直未有时间着手去搞清楚。如果只是看一个图片的属性是看不出他的mode类型的。用python的image去读一个图片输出它的返回值就可以看到一张图片的组成参数，包括它的mode。</p>
<p>我用python处理图片，它的Image，skimage都可以读一张图片，不过读出来的形式不同，Image读出来的仍然是图片形式，skimage读出来的是矩阵，我的其他文章中有提到它们的不同。image读出来的图片可以转成矩阵，这时就要提到另一个python做数据处理的常用模块—numpy，numpy是一个多维数组【矩阵】。深度学习处理的其实就是图片的真正组成—–矩阵，numpy有很多的方法，所以会常常用到它，它的花式索引，广<br>播计算等等使用时都非常方便。numpy和image之间可以互相转化。image自己本身就有很多便利的可以对图片进行转化灰度图【一维】和切割旋转等的方法，但是如果需要逐像素对图片进行处理就可以选择转成numpy后处理。</p>
<p>图片的展示以及训练结果数据的展示如果可以通过图片显示会更加的明显，这时我们就需要python的另一个数据处理常用的模块—matplotlib，它有很多办法可以把你的图片或数据以你想要的方式展示出来，具体可参看官网或者我的另一篇文章有稍微讲了一些常用的办法。</p>
<p>所以如果你是新手，而且是没有人带什么都不懂的新手，你用python处理图片时请参看，Image，skimage，numpy，matplotlib。</p>
]]></content>
      <categories>
        <category>生活杂记</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>python简单爬虫（二）</title>
    <url>/2017/04/08/python%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<p>首先必须要会用python中处理异常的语法。看了一个很好，字体感觉很舒服（<br><img src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/knock.gif" alt="敲打"><br>原谅我的挑剔）且很详细，python异常： <a href="http://www.cnblogs.com/dkblog/archive/2011/06/24/2089026.html" target="_blank" rel="noopener"> 点击打开链接
</a></p>
<p>二者环境仍和上篇一样，版本2.X</p>
<p>最后虽然上面那个异常处理讲的很清楚，但是我们这里主要用的两个异常没有具体说，我这里再提一下：URLError、HTTPError。URLError是HTTP<br>Error的基类，HTTPError的错误中会返回相应的code，如我们最常见的404，其他的可以自己去查。  </p>
<p>1.原本平时访问网站时都会出现错误，请求不到等等，更遑论我们这是爬虫了（且可能是漏洞百出）。且网站是被人的，说不定人家什么时候就把文件的存放位置改了呢，是不<br>是，仔细想想，状况百出呀，然你总不能做电脑旁边一直看着你的程序吧。。。这个时候就需要try来解决了。</p>
<p>首先来个小栗子：</p>
<pre><code>import urllib2

url = &apos;http://bbs.csdn.net/WhereAreYou&apos;
req = urllib2.Request(url)
try:
    response = urllib2.urlopen(req)
except urllib2.URLError, e:
    print e.reason
    print e.code
#print response.read()</code></pre><p>你执行这个栗子，结果是：</p>
<p>-——— Python ———-<br>Forbidden<br>403  </p>
<p>输出完成 (耗时 0 秒) - 正常终止</p>
<p>然大家千万不要被他骗了，你把那个url地址在浏览器中尝试，你会发现csdn给你的是404错误。403错误是csdn服务器禁止了你的请求，为毛咧？因为他发现了<br>你不是正常访问。解决方法是让它以为你是正常访问，这里假装我们是chrome浏览器，在req里添加头，也就是它：</p>
<pre><code>req.add_header(&apos;User-Agent&apos;,&apos;Chrome&apos;)</code></pre><p>现在再执行就是404（找不到你要的文件）</p>
<p>这是个HTTPError的栗子，如果把url换成“<a href="http://wwwbalibali.com/”就成了URLError（那就不能有code）里的了。我的理" target="_blank" rel="noopener">http://wwwbalibali.com/”就成了URLError（那就不能有code）里的了。我的理</a><br>解是如果服务器存在一般都是HTTPError，否则就是除了HTTPError的URLError了。  </p>
<p>2.下面是一个可以体现URLError是HTTPError父类的栗子，以及异常处理的语法，一旦捕捉到异常即跳出：</p>
<pre><code>#coding=utf-8
import urllib
import urllib2

#url = &apos;http://www.balabala.com/&apos;
url = &apos;http://bbs.csdn.net/WhereAreYou&apos;
req = urllib2.Request(url)
req.add_header(&apos;User-Agent&apos;,&apos;Chrome&apos;)
try:
    request = urllib2.urlopen(req)
except urllib2.HTTPError,e:
    print &apos;the server can not fullfill our request&apos;
    print &apos;the return code is: {0}&apos;.format(e.code)
except urllib2.URLError,e:
    print &apos;we can not catch the server&apos;
    print &apos;the reason is: {0}&apos;.format(e.reason)
else :
    print &apos;success!&apos;</code></pre><p>3.这是一个和上面处理结果一样的栗子，方式略不同</p>
<pre><code>import urllib2

url = &apos;http://www.baibai.com/&apos;
#url = &apos;http://bbs.csdn.net/WhereAreYou&apos;
req = urllib2.Request(url)
req.add_header(&apos;User-Agent&apos;,&apos;Chrome&apos;)
try:
    request = urllib2.urlopen(req)
except urllib2.URLError, e:
    if hasattr(e,&apos;code&apos;):
        print &apos;the server can not fullfill our request&apos;
        print &apos;the return code is: {0}&apos;.format(e.code)
    elif hasattr(e,&apos;reason&apos;):
        print &apos;we can not catch the server&apos;
        print &apos;the reason is: {0}&apos;.format(e.reason)
    else :
        print &apos;success!&apos;</code></pre><p>其实就我自身小经验觉得，很多地方解决可能出现的且你知道什么样的异常选择if-elif处理也不失为一个好办法。举个栗子吧，如果你在分析一个页面的成分时，可能将<br>来你分析的那个节点没有了，这就可以轻易的停止你的程序，而这时如果用try解决明显很累赘。所以，你懂得。  </p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>python简单爬虫例子（一）</title>
    <url>/2017/04/08/python%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB%E4%BE%8B%E5%AD%90%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>环境与上一篇一样windows，editplus，python-2.7.6（且我前面文章有介绍过配置过程）</p>
<p>另外介绍一个抓包工具fiddler，超级好用的，特别是在以后你需要爬一些很复杂网站时。（不要它是英文就接受不了，上手很快的）  </p>
<p>以前都是用beautifulsoup，现在想从头尝试用urllib2.</p>
<p>urllib2是python提供的抓取网页的组件。</p>
<p>1.最简单例子：</p>
<pre><code>import urllib2
response = urllib2.urlopen(&quot;http://www.baidu.com/&quot;)
html = response.read()
print html</code></pre><p>输出就是百度首页的编码。</p>
<p>2.下面是一个需要发送数据的爬虫简单例子。发送方式时get。（其实我自己也不知道为什么，在浏览器的网站栏里，网站的url中的中文是正常显示的，但是我把url<br>拷到editplus里之后就变了，好吧，拷到其他地方也是这样。。。不知道是为什么，开始还担心请求会不成功的，后来还是有数据的。看来是我的web开发学的不到位<br>，如果有知道原因的，请留言告诉我一声，虽然这件事和这个例子没什么关系。。。）</p>
<pre><code>#coding=utf-8
import urllib
import urllib2

#http://dujia.com/pq/list_%E5%AE%9C%E6%98%8C?searchfrom=around&amp;arounddep=%E6%AD%A6%E6%B1%89&amp;tf=Ihot_01
data = {}
data[&apos;searchfrom&apos;] = &apos;around&apos;
data[&apos;arounddep&apos;] = &apos;%E6%AD%A6%E6%B1%89&apos;
data[&apos;tf&apos;] = &apos;Ihot_01&apos;

value = urllib.urlencode(data)
print value
url = &apos;http://dujia.com/pq/list_%E5%AE%9C%E6%98%8C&apos; + &apos;?&apos; + value

response = urllib2.urlopen(url)
print response.read()</code></pre><p>3.也是需要发送数据的爬虫例子。这个是post方式的。</p>
<pre><code>import urllib
import urllib2

#http://dujia.com/pq/list_%E5%AE%9C%E6%98%8C?searchfrom=around&amp;arounddep=%E6%AD%A6%E6%B1%89&amp;tf=Ihot_01
data = {}
data[&apos;searchfrom&apos;] = &apos;around&apos;
data[&apos;arounddep&apos;] = &apos;%E6%AD%A6%E6%B1%89&apos;
data[&apos;tf&apos;] = &apos;Ihot_01&apos;

value = urllib.urlencode(data)
print value

url = &apos;http://dujia.com/pq/list_%E5%AE%9C%E6%98%8C&apos;
response = urllib2.urlopen(url,value)
print response.read()</code></pre><p>貌似两个也没大差哈~  </p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
